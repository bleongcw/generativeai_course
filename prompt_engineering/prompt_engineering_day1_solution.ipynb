{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95161edb-938c-47a2-9b7a-834323213506",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "panel-layout": {
     "height": 77.734375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Prompt engineering** is the practice of designing and optimizing inputs (prompts) to guide AI models \n",
    "in generating more accurate, useful, and relevant responses. It plays a crucial role in improving \n",
    "interactions with large language models (LLMs) such as OpenAI's GPT-4. For this notebook, we require you to have both Claude or OpenAI API access. \n",
    "\n",
    "## Basic Principles of Prompt Engineering\n",
    "\n",
    "1. **Clarity and Specificity**  \n",
    "   - Clearly define the task and avoid vague or ambiguous instructions.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     Bad: \"Tell me about history.\"\n",
    "     Good: \"Provide a summary of the Renaissance period and its impact on European art.\"\n",
    "     ```\n",
    "\n",
    "2. **Role Assignment**  \n",
    "   - Assign a persona or role to the AI model to get responses in a specific tone or expertise.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"You are an AI financial advisor. Explain the benefits of index funds to a beginner investor.\"\n",
    "     ```\n",
    "\n",
    "3. **Context and Constraints**  \n",
    "   - Provide relevant background information and define constraints such as format, length, or tone.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"Write a 100-word product description for a new AI-powered smartphone.\"\n",
    "     ```\n",
    "\n",
    "4. **Step-by-Step Breakdown**  \n",
    "   - Ask the model to explain its reasoning in steps for complex tasks.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"Explain the process of machine learning model training in a step-by-step manner.\"\n",
    "     ```\n",
    "\n",
    "5. **Examples and Formatting Guidance**  \n",
    "   - Show examples to guide the model on expected output formats.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"Translate the following English sentences to French:\\n1. Hello, how are you?\\n2. The weather is nice today.\"\n",
    "     ```\n",
    "\n",
    "6. **Iterative Refinement**  \n",
    "   - Experiment with different prompts and refine them based on the model's responses.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"List 5 ways AI is used in healthcare. If possible, provide real-world examples.\"\n",
    "     ```\n",
    "\n",
    "By following these principles, developers can craft effective prompts that enhance the performance of AI models.\n",
    "\"\"\"\n",
    "\n",
    "Python code can follow below if needed\n",
    "\n",
    "For this prompt engineering, we follow [Anthropic API fundamentals](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals) and have a similar track with OpenAI API.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a6b3f-9798-47b2-b1cc-08320654d6c4",
   "metadata": {
    "panel-layout": {
     "height": 112.015625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Step 1: Load Environment Variables and API Keys\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "In this step, we will ensure that the notebook can load the global `.env` file and that the required API keys are in place.\n",
    "\n",
    "## Objectives:\n",
    "1. Load environment variables using the `dotenv` package.\n",
    "2. Verify that API keys are set up correctly.\n",
    "3. Provide options to check API keys with and without displaying them.\n",
    "\n",
    "## Instructions:\n",
    "1. Ensure you have the `python-dotenv` package installed:\n",
    "   ```bash\n",
    "   pip install python-dotenv\n",
    "\n",
    "2. Create a .env file in the root directory (if not already created) and add your API keys:\n",
    "\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "ANTHROPIC_API_KEY=your_anthropic_api_key\n",
    "\n",
    "3. Run the following Python code to load and verify the API keys.</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932f451c-0860-44d0-9c56-9df17823357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys are successfully loaded.\n",
      "üîí API keys are loaded but hidden for security.\n"
     ]
    }
   ],
   "source": [
    "### Python Code:\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check if API keys are loaded\n",
    "if openai_api_key and anthropic_api_key:\n",
    "    print(\"‚úÖ API keys are successfully loaded.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: One or more API keys are missing.\")\n",
    "\n",
    "# Optionally, display API keys (for debugging purposes only)\n",
    "display_keys = False  # Change to True if you want to see the keys\n",
    "\n",
    "if display_keys:\n",
    "    print(f\"OpenAI API Key: {openai_api_key}\")\n",
    "    print(f\"Anthropic API Key: {anthropic_api_key}\")\n",
    "else:\n",
    "    print(\"üîí API keys are loaded but hidden for security.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aeaa12-7edf-4f8a-b761-16420d6954ad",
   "metadata": {
    "panel-layout": {
     "height": 105.671875,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Messages format and Understanding the concept of **role**, **user** and **content**\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "\n",
    "### Message Format     \n",
    "As we saw in the previous lesson, we can use `client.messages.create()` (Claude) and `client.chat.completions.create` (Open AI) to send a message to Claude & OpenAI and get their respective responses.\n",
    "\n",
    "The messages format allows us to structure our API calls to Claude or GPT in the form of a conversation, allowing for **context preservation**: The messages format allows for maintaining an entire conversation history, including both user and assistant messages. This ensures that Claude or GPT has access to the full context of the conversation when generating responses, leading to more coherent and relevant outputs.  \n",
    "\n",
    "**Note: many use-cases don't require a conversation history, and there's nothing wrong with providing a list of messages that only contains a single message!** \n",
    "\n",
    "In addition to `content`, the `Message` object contains some other pieces of information:\n",
    "\n",
    "* `id` - a unique object identifier\n",
    "* `type` - The object type, which will always be \"message\"\n",
    "* `role` - The conversational role of the generated message. This will always be \"assistant\".\n",
    "* `model` - The model that handled the request and generated the response\n",
    "* `stop_reason` - The reason the model stopped generating.  We'll learn more about this later.\n",
    "* `stop_sequence` - We'll learn more about this shortly.\n",
    "* `usage` - information on billing and rate-limit usage. Contains information on:\n",
    "    * `input_tokens` - The number of input tokens that were used.\n",
    "    * `output_tokens` - The number of output tokens that were used.\n",
    "\n",
    "It's important to know that we have access to these pieces of information, but if you only remember one thing, make it this: `content` contains the actual model-generated content\n",
    "\n",
    "### How does **role**, **user** and **content** work? \n",
    "Let's take a closer look at this bit (whether it's Anthropic or OpenAI): \n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ]\n",
    "```\n",
    "\n",
    "The messages parameter is a crucial part of interacting with the Claude and OpenAI API. It allows you to provide the conversation history and context for **Claude** or **OpenAI** to generate a relevant response. \n",
    "\n",
    "The messages parameter expects a list of message dictionaries, where each dictionary represents a single message in the conversation.\n",
    "\n",
    "Each message dictionary should have the following keys:\n",
    "\n",
    "* `role`: A string indicating the role of the message sender. It can be either \"user\" (for messages sent by the user) or \"assistant\" (for messages sent by Claude or OpenAI).\n",
    "* `content`: A string or list of content dictionaries representing the actual content of the message. If a string is provided, it will be treated as a single text content block. If a list of content dictionaries is provided, each dictionary should have a \"type\" (e.g., \"text\" or \"image\") and the corresponding content.  For now, we'll leave `content` as a single string.\n",
    "\n",
    "Here's an example of a messages list with a single user message:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude/OpenAI! How are you today?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "And here's an example with multiple messages representing a conversation:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude/OpenAI! How are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! I'm doing well, thank you. How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a fun fact about ferrets?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Did you know that excited ferrets make a clucking vocalization known as 'dooking'?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Remember that messages always alternate between user and assistant messages.</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc04083-fbde-4063-aa42-64e6d9268c9b",
   "metadata": {
    "panel-layout": {
     "height": 0,
     "visible": true,
     "width": 100
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_017uWCGskMV8cffGzgCskTS5', content=[TextBlock(text='The exact flavor blend in Dr Pepper is a closely guarded trade secret, but the generally accepted list of key flavors includes:\\n\\n- Cherry\\n- Vanilla\\n- Prune\\n- Amaretto (almond)\\n- Wintergreen\\n- Molasses\\n- Plum\\n- Prune\\n- Spices like cinnamon and nutmeg\\n\\nThe flavor is often described as a unique blend of cherry, vanilla, and other fruit and spice notes. Dr Pepper has a distinctive taste that is unlike other cola or soda flavors. The exact recipe has remained a mystery since the drink was first created in the late 1800s.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=18, output_tokens=146))\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "# Call the Claude API\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87db25c7-3f65-4e79-8f92-9e812efc43f0",
   "metadata": {
    "panel-layout": {
     "height": 0,
     "visible": true,
     "width": 100
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Pepper is known for its unique blend of 23 different flavors, which create its distinct taste. While the specific recipe is a closely guarded secret, some of the commonly speculated flavors include:\n",
      "\n",
      "1. Cola\n",
      "2. Cherry\n",
      "3. Licorice\n",
      "4. Almond\n",
      "5. Vanilla\n",
      "6. Blackberry\n",
      "7. Apricot\n",
      "8. Caramel\n",
      "9. Pepper\n",
      "10. Anise\n",
      "11. Sarsaparilla\n",
      "12. Ginger\n",
      "13. Molasses\n",
      "14. Lemon\n",
      "15. Plum\n",
      "16. Orange\n",
      "17. Nutmeg\n",
      "18. Cardamon\n",
      "19. All Spice\n",
      "20. Coriander\n",
      "21. Juniper\n",
      "22. Birch\n",
      "23. Prune\n",
      "\n",
      "These are not official, and the true mix of flavors that gives Dr. Pepper its characteristic taste remains proprietary.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Check if API key is loaded\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OpenAI API key is missing. Please check your .env file.\")\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",  # You can also use \"gpt-3.5-turbo\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63678bde-2086-4809-9b40-e8964aa8f868",
   "metadata": {
    "panel-layout": {
     "height": 77.734375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "<details><summary>Click to expand/collapse</summary>\n",
    "\n",
    "### Lesson Goals\n",
    "* Understand the role of the `max_tokens` parameter.\n",
    "* Use the `temperature` parameter to control model responses.\n",
    "* Explain the purpose of `stop_sequence`.\n",
    "\n",
    "## Required Parameters\n",
    "\n",
    "When making a request to a Large Language Model (LLM) such as **Claude** (Anthropic) or **GPT** (OpenAI), there are three required parameters:\n",
    "\n",
    "* `model`\n",
    "* `max_tokens`\n",
    "* `messages`\n",
    "\n",
    "So far, we have been using the `max_tokens` parameter in every single request, but we have not stopped to discuss what it is.\n",
    "\n",
    "---\n",
    "\n",
    "### Using `max_tokens` in Claude\n",
    "\n",
    "Here is an example request to Claude:\n",
    "\n",
    "```python\n",
    "our_first_message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Here is an equivalent request to OpenAI: \n",
    "\n",
    "```python\n",
    "our_first_message = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### So What is the Purpose of max_tokens?\n",
    "In short, **max_tokens** controls the maximum number of tokens that the model should generate in its response. \n",
    "\n",
    "Before we go any further, let us pause for a moment to discuss tokens.\n",
    "\n",
    "Most Large Language Models don't think in full words but instead process and generate responses using tokens, which are small building blocks of a text sequence.\n",
    "\n",
    "When we provide a prompt to an LLM, the model:\n",
    "\n",
    "Converts the input into tokens.\n",
    "Processes the tokens and generates the output one token at a time.\n",
    "\n",
    "### Token Differences Between Claude and OpenAI\n",
    "\n",
    "|**Model**                | **Approximate Token Size**         |\n",
    "|--------------------------|-----------------------------------|\n",
    "|**Claude (Anthropic)**   | 1 token ‚âà 3.5 English characters |\n",
    "|**OpenAI (GPT-4/GPT-3.5)** | 1 token ‚âà 4 English characters |\n",
    "\n",
    "*The exact token count may vary depending on the language and structure of the text.*\n",
    "\n",
    "### Why alter max tokens?\n",
    "Understanding tokens is crucial when working with Claude, particularly for the following reasons:\n",
    "\n",
    "* **API limits**: The number of tokens in your input text and the generated response count towards the API usage limits. Each API request has a maximum limit on the number of tokens it can process. Being aware of tokens helps you stay within the API limits and manage your usage efficiently.\n",
    "* **Performance**: The number of tokens Claude generates directly impacts the processing time and memory usage of the API. Longer input texts and higher max_tokens values require more computational resources. Understanding tokens helps you optimize your API requests for better performance.\n",
    "* **Response quality**: Setting an appropriate max_tokens value ensures that the generated response is of sufficient length and contains the necessary information. If the max_tokens value is too low, the response may be truncated or incomplete. Experimenting with different max_tokens values can help you find the optimal balance for your specific use case.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857db08c-bbfc-4e36-a7ac-a24c7b3583a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a poem for you:\n",
      "\n",
      "The Dance of the Pen\n",
      "\n",
      "Across the page, the pen takes flight,\n",
      "Weaving words with rhythmic might.\n",
      "A canvas blank, a mind in flight,\n",
      "Together they create the dance of light.\n",
      "\n",
      "Each stroke a step, each phrase a beat,\n",
      "The poem unfolds, a work so sweet.\n",
      "Emotions flow, thoughts take shape,\n",
      "As the pen and mind their magic drape.\n",
      "\n",
      "A symphony of ink and thought,\n",
      "A tapestry of beauty, newly wrought.\n",
      "The dance continues, ever-changing,\n",
      "Capturing moments, forever rearranging.\n",
      "\n",
      "In this dance, the heart finds its voice,\n",
      "Expressing dreams, making a choice.\n",
      "The pen, the poet, a harmonious pair,\n",
      "Crafting poems that fill the air.\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client (not OpenAI)\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "# Claude API with max_tokens and you can try max_tokens from 10 to 1000 and 500 is usually where the poem ends\n",
    "\n",
    "truncated_response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n",
    "    ]\n",
    ")\n",
    "print(truncated_response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a8a8d6-d41b-41cf-b2d8-72ea3fd14720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a meadow, soft and wide, under the cloak of a starlit sky,\n",
      "Where whispers of the wind collude, and the nightingales shyly cry.\n",
      "\n",
      "Beneath the silver brushstrokes, of Luna‚Äôs gentle gleam,\n",
      "Lies a tranquil, silent promise, a canvas born from dream.\n",
      "\n",
      "The grass, a verdant ocean, waves in the moon‚Äôs soft light,\n",
      "Each blade a silent sentry, in the quiet arms of night.\n",
      "\n",
      "Crickets sing in choruses, a symphony so sweet,\n",
      "A serenade of nature‚Äôs heart, where sky and earth discreetly meet.\n",
      "\n",
      "Dewdrops, like scattered diamonds, upon the meadow lie,\n",
      "Capturing moonbeams, daintily, as if they shyly vie.\n",
      "\n",
      "A fox, with ember eyes aglow, through shadows deftly weaves,\n",
      "Rustles 'neath the old oak‚Äôs boughs, brushed by autumn leaves.\n",
      "\n",
      "The river, a silent wanderer, carves its path with grace,\n",
      "Its waters whisper secrets, in this serene and hallowed place.\n",
      "\n",
      "And above, the constellations spin, tales in celestial weave,\n",
      "Orion with his belt so bold, the Bear‚Äôs drape none deceive.\n",
      "\n",
      "In this night, under stardust veil, where quiet souls do speak,\n",
      "The earth tells her ancient lore to those who dare to seek.\n",
      "\n",
      "So lie here in the meadow, let the night embrace your dreams,\n",
      "For under the vast, eternal skies, the world is more than it seems.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with API key from environment\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Check if API key is loaded correctly\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OpenAI API key is missing. Please check your .env file.\")\n",
    "\n",
    "# Make API call with max_tokens set to 10 and try 100 or 500\n",
    "truncated_response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print truncated response\n",
    "print(truncated_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08690297-d85a-4d73-a71d-f66fa648d882",
   "metadata": {},
   "source": [
    "# Stop Sequences \n",
    "\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "Another important parameter we haven't seen yet is `stop_sequence` which allows us to provide the model with a set of strings that, when encountered in the generated response, cause the generation to stop.  They are essentially a way of telling Claude or OpenAI, \"if you generate this sequence, stop generating anything else!\"\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6df5609-8979-4332-ab6d-cf421c680644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a JSON object representing a person with a name, email, and phone number:\n",
      "\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"email\": \"johndoe@example.com\",\n",
      "  \"phoneNumber\": \"123-456-7890\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Example of stop_sequence with Claude AI\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number .\"}],\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b6606c-e5ca-4b15-824a-57511c025c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example JSON object representing a person with a name, email, and phone number:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"email\": \"johndoe@example.com\",\n",
      "  \"phone_number\": \"+1234567890\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of stop_sequence with OpenAI API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client using environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Make an API call with a stop sequence\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",  # Use \"gpt-3.5-turbo\" if needed\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number.\"}\n",
    "    ],\n",
    "    stop=[\"}\"]  # Stop generation after the closing JSON brace\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4bc6f2-c6f4-44e3-89e1-6ec319658dce",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "The `temperature` parameter is used to control the \"randomness\" and \"creativity\" of the generated responses. It ranges from 0 to 1, with higher values resulting in more diverse and unpredictable responses with variations in phrasing.  Lower temperatures can result in more deterministic outputs that stick to the most probable phrasing and answers. **Temperature has a default value of 1**.\n",
    "\n",
    "When generating text, any LLM (Claude, GPT or DeepSeek) predicts the probability distribution of the next token (word or subword). The temperature parameter is used to manipulate this probability distribution before sampling the next token. If the temperature is low (close to 0.0), the probability distribution becomes more peaked, with high probabilities assigned to the most likely tokens. This makes the model more deterministic and focused on the most probable or \"safe\" choices. If the temperature is high (closer to 1.0), the probability distribution becomes more flattened, with the probabilities of less likely tokens increasing. This makes the model more random and exploratory, allowing for more diverse and creative outputs. \n",
    "\n",
    "See this diagram for a visual representation of the impact of temperature (Source: Anthropic):\n",
    "<img src=\"images/temperature.png\" alt=\"Chart description\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "Why would you change temperature?\n",
    "**Use temperature closer to 0.0 for analytical tasks, and closer to 1.0 for creative and generative tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a40f138c-af49-46ae-89a8-790efa095c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting Claude three times with temperature of 0\n",
      "================\n",
      "Response 1: Xendor.\n",
      "Response 2: Xendor.\n",
      "Response 3: Xendor.\n",
      "Prompting Claude three times with temperature of 0.3\n",
      "================\n",
      "Response 1: Zyloth.\n",
      "Response 2: Xendor.\n",
      "Response 3: Zyloth.\n",
      "Prompting Claude three times with temperature of 0.5\n",
      "================\n",
      "Response 1: Xendor.\n",
      "Response 2: Xendor.\n",
      "Response 3: Xyloth.\n",
      "Prompting Claude three times with temperature of 0.75\n",
      "================\n",
      "Response 1: Zyloth.\n",
      "Response 2: Xylion.\n",
      "Response 3: Xendor.\n",
      "Prompting Claude three times with temperature of 1\n",
      "================\n",
      "Response 1: Zoloth.\n",
      "Response 2: Xendor.\n",
      "Response 3: Xylion.\n"
     ]
    }
   ],
   "source": [
    "#Example of Temperature with Claude AI\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "def demonstrate_temperature():\n",
    "    temperatures = [0, 0.3, 0.5, 0.75, 1]\n",
    "    for temperature in temperatures:\n",
    "        print(f\"Prompting Claude three times with temperature of {temperature}\")\n",
    "        print(\"================\")\n",
    "        for i in range(3):\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-3-haiku-20240307\",\n",
    "                max_tokens=100,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Come up with a name for an alien planet. Respond with a single word.\"}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            print(f\"Response {i+1}: {response.content[0].text}\")\n",
    "\n",
    "demonstrate_temperature()\n",
    "\n",
    "#Notice that with a temperature of 0, all three responses are the same.  \n",
    "#Note that even with a temperature of 0.0, the results will not be fully deterministic.  \n",
    "#However, there is a clear difference when compared to the results with a temperature of 1.  \n",
    "#Each response was a completely different alien planet name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa033a3-cae7-49f5-96e6-f08aa35e73df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting OpenAI GPT three times with temperature of 0\n",
      "================\n",
      "Response 1: Sunny\n",
      "Response 2: Sunny\n",
      "Response 3: Sunny\n",
      "Prompting OpenAI GPT three times with temperature of 0.3\n",
      "================\n",
      "Response 1: Sunny\n",
      "Response 2: Sunny\n",
      "Response 3: Sunny\n",
      "Prompting OpenAI GPT three times with temperature of 0.5\n",
      "================\n",
      "Response 1: Sunny\n",
      "Response 2: Sunny\n",
      "Response 3: Sunny\n",
      "Prompting OpenAI GPT three times with temperature of 0.75\n",
      "================\n",
      "Response 1: Sunny\n",
      "Response 2: Sunny\n",
      "Response 3: Sunny\n",
      "Prompting OpenAI GPT three times with temperature of 1\n",
      "================\n",
      "Response 1: Sunny\n",
      "Response 2: Sunny\n",
      "Response 3: Sunny\n"
     ]
    }
   ],
   "source": [
    "# Example of Temperature with OpenAI API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client using environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def demonstrate_temperature():\n",
    "    temperatures = [0, 0.3, 0.5, 0.75, 1]\n",
    "    \n",
    "    for temperature in temperatures:\n",
    "        print(f\"Prompting OpenAI GPT three times with temperature of {temperature}\")\n",
    "        print(\"================\")\n",
    "        \n",
    "        for i in range(3):\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo\",  # Use \"gpt-3.5-turbo\" if needed\n",
    "                max_tokens=100,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Come up with a name for an golden retriever. Respond with a single word.\"}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            print(f\"Response {i+1}: {response.choices[0].message.content}\")\n",
    "\n",
    "# Run the function\n",
    "demonstrate_temperature()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ddfd7-a6ca-46f4-89f7-dc1b06d7c0c9",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "The `system_prompt` is an optional parameter that you can include when sending messages to **Claude (Anthropic)** and **GPT-4 (OpenAI)**. It sets the stage for the conversation by providing high-level instructions, defining the AI's role, or giving background information that should inform its responses.\n",
    "\n",
    "### Key Points About the `system_prompt`:\n",
    "- It's **optional** but can be useful for setting the **tone** and **context** of the conversation.\n",
    "- It is applied at the **conversation level**, affecting all responses within that exchange.\n",
    "- It helps **steer the model‚Äôs behavior** without needing to include instructions in every user message.\n",
    "\n",
    "### How Each Model Uses the `system_prompt`:\n",
    "| **Model**   | **System Prompt Usage** |\n",
    "|------------|------------------------|\n",
    "| **Claude (Anthropic)** | Uses `system_prompt` to set high-level guidance for the conversation. |\n",
    "| **GPT-4 (OpenAI)** | Uses a system message as the first entry in `messages` (e.g., `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}`). |\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices for Using System Prompts**\n",
    "‚úÖ **Use it for high-level guidance** (e.g., defining tone, behavior, role).  \n",
    "‚úÖ **Avoid detailed instructions or long documents** in the system prompt.  \n",
    "‚úÖ **Provide detailed instructions** inside the **first User message** for better results.  \n",
    "‚úÖ **No need to repeat it** for every subsequent user turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f886696a-916a-4f29-85e1-03e9cff55fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†!ÊàëÂæàÂ•Ω,Ë∞¢Ë∞¢‰Ω†ÁöÑÈóÆÂÄô„ÄÇ‰Ω†‰ªäÂ§©ËøáÂæóÊÄé‰πàÊ†∑?\n"
     ]
    }
   ],
   "source": [
    "#Example of System Prompts with Claude AI\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    system=\"You are a helpful foreign language tutor that always responds in Chinese.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hey there, how are you?!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1db9d08-ffa7-4cfc-a755-0476cef9e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰Ω†Â•ΩÔºÅÊàëÂæàÂ•ΩÔºåË∞¢Ë∞¢„ÄÇ‰Ω†ÊúÄËøëÊÄé‰πàÊ†∑ÔºüÈúÄË¶ÅÂ∏ÆÂä©Â≠¶‰π†‰∏≠ÊñáÂêóÔºü\n"
     ]
    }
   ],
   "source": [
    "# Example of System Prompts with OpenAI API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client using environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Create a chat completion with a system prompt\n",
    "message = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",  # Use \"gpt-3.5-turbo\" if needed\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful foreign language tutor that always responds in Chinese.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hey there, how are you?!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(message.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain_env)",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "panel-cell-order": [
   "95161edb-938c-47a2-9b7a-834323213506",
   "943a6b3f-9798-47b2-b1cc-08320654d6c4",
   "37aeaa12-7edf-4f8a-b761-16420d6954ad",
   "bbc04083-fbde-4063-aa42-64e6d9268c9b",
   "87db25c7-3f65-4e79-8f92-9e812efc43f0",
   "63678bde-2086-4809-9b40-e8964aa8f868"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
