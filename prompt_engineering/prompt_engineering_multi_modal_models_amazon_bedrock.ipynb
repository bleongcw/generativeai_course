{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# Multi-Modal Models with Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to work with **multi-modal AI models** using Amazon Bedrock. Multi-modal models can understand and generate content across different modalities (text, images, etc.).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Text-to-Image Generation**: Using Amazon Titan Image Generator to create product images from text descriptions\n",
    "2. **Multi-Modal Embeddings**: Converting both text and images into vector representations using Amazon Titan Embed\n",
    "3. **Semantic Search**: Finding similar images using text queries through embedding similarity\n",
    "\n",
    "## Use Case: E-Commerce Product Catalog\n",
    "\n",
    "We'll simulate building an e-commerce search system where:\n",
    "- Product images are generated from descriptions\n",
    "- Images are converted to embeddings for efficient similarity search\n",
    "- Users can search for products using natural language queries\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with Bedrock access enabled\n",
    "- Model access enabled for:\n",
    "  - Amazon Titan Image Generator V2\n",
    "  - Amazon Titan Multimodal Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restart-kernel-md",
   "metadata": {},
   "source": [
    "## Restart Kernel (Optional)\n",
    "\n",
    "The following cell restarts the Jupyter kernel to ensure a clean environment. This is useful when re-running the notebook to clear any cached variables or imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-md",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We need several libraries for this notebook:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|----------|\n",
    "| `boto3` | AWS SDK for Python - connects to Amazon Bedrock |\n",
    "| `numpy` | Numerical operations for embedding calculations |\n",
    "| `PIL` | Python Imaging Library for handling generated images |\n",
    "| `scipy` | Scientific computing - used for distance calculations in similarity search |\n",
    "| `seaborn/matplotlib` | Visualization libraries for plotting similarity heatmaps |\n",
    "\n",
    "The code includes fallback implementations if optional libraries (scipy, seaborn) are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8276eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Other library imports\n",
    "import boto3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Try to import scipy with error handling\n",
    "try:\n",
    "    from scipy.spatial.distance import cdist\n",
    "    print(\"SciPy imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"SciPy import failed: {e}\")\n",
    "    print(\"Using NumPy fallback for distance calculations\")\n",
    "    \n",
    "    # Simple numpy-based distance calculation\n",
    "    def cdist_numpy(XA, XB, metric='euclidean'):\n",
    "        \"\"\"Simple numpy-based distance calculation as fallback\"\"\"\n",
    "        if metric == 'euclidean':\n",
    "            return np.sqrt(((XA[:, np.newaxis, :] - XB[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "        else:\n",
    "            raise ValueError(f\"Metric '{metric}' not implemented in fallback\")\n",
    "    \n",
    "    cdist = cdist_numpy\n",
    "\n",
    "# Try to import seaborn with error handling\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    print(\"Seaborn imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Seaborn import failed: {e}\")\n",
    "    print(\"Continuing without seaborn - matplotlib will be used instead\")\n",
    "    sns = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedrock-client-md",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Amazon Bedrock Client\n",
    "\n",
    "Before we can use any Bedrock models, we need to create a **Bedrock Runtime client**. This client handles all API communication with Amazon Bedrock.\n",
    "\n",
    "**Key Concepts:**\n",
    "- `boto3.session.Session()` - Creates an AWS session using your configured credentials\n",
    "- `bedrock-runtime` - The service endpoint for invoking Bedrock models (different from the management API)\n",
    "- The region is automatically detected from your AWS configuration\n",
    "\n",
    "**Note:** Ensure your AWS credentials are configured via `aws configure` or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init boto session\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Init Bedrock Runtime client\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name)\n",
    "\n",
    "print(\"AWS Region:\", region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "product-data-md",
   "metadata": {},
   "source": [
    "## Step 3: Define Product Catalog Data\n",
    "\n",
    "For this demo, we'll use a pre-defined list of **21 product descriptions** (7 product categories × 3 variants each). In a real application, this data might come from:\n",
    "- A database\n",
    "- An LLM generating product descriptions\n",
    "- A product information management (PIM) system\n",
    "\n",
    "**Product Categories:**\n",
    "1. T-shirts (3 variants)\n",
    "2. Jeans (3 variants)\n",
    "3. Sneakers (3 variants)\n",
    "4. Backpacks (3 variants)\n",
    "5. Smartwatches (3 variants)\n",
    "6. Coffee makers (3 variants)\n",
    "7. Yoga mats (3 variants)\n",
    "\n",
    "Each description is detailed enough to generate a distinctive product image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'Here is a list of 7 items with 3 variants each for an online e-commerce shop, with separate full sentence descriptions:\\n\\n1. T-shirt\\n- A red cotton t-shirt with a crew neck and short sleeves. \\n- A blue cotton t-shirt with a v-neck and short sleeves.\\n- A black polyester t-shirt with a scoop neck and cap sleeves.\\n\\n2. Jeans\\n- Classic blue relaxed fit denim jeans with a mid-rise waist. \\n- Black skinny fit denim jeans with a high-rise waist and ripped details at the knees.  \\n- Stonewash straight leg denim jeans with a standard waist and front pockets.\\n\\n3. Sneakers  \\n- White leather low-top sneakers with an almond toe cap and thick rubber outsole.\\n- Gray mesh high-top sneakers with neon green laces and a padded ankle collar. \\n- Tan suede mid-top sneakers with a round toe and ivory rubber cupsole.  \\n\\n4. Backpack\\n- A purple nylon backpack with padded shoulder straps, front zipper pocket and laptop sleeve.\\n- A gray canvas backpack with brown leather trims, side water bottle pockets and drawstring top closure.  \\n- A black leather backpack with multiple interior pockets, top carry handle and adjustable padded straps.\\n\\n5. Smartwatch\\n- A silver stainless steel smartwatch with heart rate monitor, GPS tracker and sleep analysis.  \\n- A space gray aluminum smartwatch with step counter, phone notifications and calendar syncing. \\n- A rose gold smartwatch with activity tracking, music controls and customizable watch faces.  \\n\\n6. Coffee maker\\n- A 12-cup programmable coffee maker in brushed steel with removable water tank and keep warm plate.  \\n- A compact 5-cup single serve coffee maker in matt black with travel mug auto-dispensing feature.\\n- A retro style stovetop percolator coffee pot in speckled enamel with stay-cool handle and glass knob lid.  \\n\\n7. Yoga mat \\n- A teal 4mm thick yoga mat made of natural tree rubber with moisture-wicking microfiber top.\\n- A purple 6mm thick yoga mat made of eco-friendly TPE material with integrated carrying strap. \\n- A patterned 5mm thick yoga mat made of PVC-free material with towel cover included.'\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-text-md",
   "metadata": {},
   "source": [
    "## Step 4: Extract Individual Product Descriptions\n",
    "\n",
    "The product data above is in a formatted string. We need to extract each individual product description to use as prompts for image generation.\n",
    "\n",
    "**How it works:**\n",
    "- Uses a **regular expression** to find all lines starting with `- `\n",
    "- Extracts the text after the dash as individual descriptions\n",
    "- Returns a list of 21 product descriptions\n",
    "\n",
    "This parsing step is common when working with LLM-generated structured content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(input_string):\n",
    "    pattern = r\"- (.*?)($|\\n)\"\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    extracted_texts = [match[0] for match in matches]\n",
    "    return extracted_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-run-md",
   "metadata": {},
   "source": [
    "### Run the Extraction\n",
    "\n",
    "Let's extract all product descriptions and verify we have 21 items (7 categories × 3 variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_descriptions = extract_text(response)\n",
    "product_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image-gen-func-md",
   "metadata": {},
   "source": [
    "## Step 5: Define the Image Generation Function\n",
    "\n",
    "This function wraps the **Amazon Titan Image Generator V2** model to create images from text descriptions.\n",
    "\n",
    "### API Parameters Explained\n",
    "\n",
    "| Parameter | Description | Range/Options |\n",
    "|-----------|-------------|---------------|\n",
    "| `numberOfImages` | How many images to generate per request | 1-5 |\n",
    "| `quality` | Image quality level | `standard` or `premium` |\n",
    "| `height` / `width` | Output image dimensions in pixels | Various sizes up to 1024 |\n",
    "| `cfgScale` | Classifier-free guidance scale - higher values follow the prompt more closely | 1.0-10.0 |\n",
    "| `seed` | Random seed for reproducibility | 0-214783647 |\n",
    "\n",
    "### Task Type\n",
    "\n",
    "We use `TEXT_IMAGE` task type which generates images from text descriptions. Other task types include:\n",
    "- `INPAINTING` - Edit parts of existing images\n",
    "- `OUTPAINTING` - Extend images beyond their borders\n",
    "- `IMAGE_VARIATION` - Create variations of existing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titan_generate_image(payload, num_image=2, cfg=10.0, seed=2024):\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            **payload,\n",
    "            \"imageGenerationConfig\": {\n",
    "                \"numberOfImages\": num_image,   # Number of images to be generated. Range: 1 to 5 \n",
    "                \"quality\": \"premium\",          # Quality of generated images. Can be standard or premium.\n",
    "                \"height\": 1024,                # Height of output image(s)\n",
    "                \"width\": 1024,                 # Width of output image(s)\n",
    "                \"cfgScale\": cfg,               # Scale for classifier-free guidance. Range: 1.0 (exclusive) to 10.0\n",
    "                \"seed\": seed                   # The seed to use for re-producibility. Range: 0 to 214783647\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, \n",
    "        modelId=\"amazon.titan-image-generator-v2:0\",\n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    images = [\n",
    "        Image.open(\n",
    "            BytesIO(base64.b64decode(base64_image))\n",
    "        ) for base64_image in response_body.get(\"images\")\n",
    "    ]\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-images-md",
   "metadata": {},
   "source": [
    "## Step 6: Generate Product Images\n",
    "\n",
    "Now we'll generate an image for each of our 21 product descriptions. This step:\n",
    "\n",
    "1. Creates a directory to store the generated images (`data/titan-embed/`)\n",
    "2. Loops through each product description\n",
    "3. Calls the Titan Image Generator with the description as the prompt\n",
    "4. Saves each image with a filename based on the first 4 words of the description\n",
    "\n",
    "**Note:** This step may take several minutes as it makes 21 API calls to generate images.\n",
    "\n",
    "**Cost Consideration:** Image generation incurs costs per image. In production, you might want to:\n",
    "- Cache generated images\n",
    "- Use lower quality settings for testing\n",
    "- Batch requests where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dir = \"data/titan-embed\"\n",
    "os.makedirs(embed_dir, exist_ok=True)\n",
    "\n",
    "titles = []\n",
    "for i, prompt in enumerate(product_descriptions, 1):\n",
    "    images = titan_generate_image(\n",
    "        {\n",
    "            \"taskType\": \"TEXT_IMAGE\",\n",
    "            \"textToImageParams\": {\n",
    "                \"text\": prompt, # Required\n",
    "            }\n",
    "        },\n",
    "        num_image=1\n",
    "    )\n",
    "    title = \"_\".join(prompt.split()[:4]).lower()\n",
    "    title = f\"{embed_dir}/{title}.png\"\n",
    "    titles.append(title)\n",
    "    images[0].save(title, format=\"png\")\n",
    "    print(f\"[{i}/{len(product_descriptions)}] Generated: '{title}'..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-func-md",
   "metadata": {},
   "source": [
    "## Step 7: Define the Multi-Modal Embedding Function\n",
    "\n",
    "**Embeddings** are numerical vector representations of content (text or images) that capture semantic meaning. Similar items have similar embeddings, enabling:\n",
    "- Semantic search (find images matching text queries)\n",
    "- Similarity comparison between products\n",
    "- Clustering and recommendation systems\n",
    "\n",
    "### Amazon Titan Multimodal Embeddings\n",
    "\n",
    "This model creates embeddings in a **shared vector space** for both text and images. This means:\n",
    "- A text description and its corresponding image will have similar embeddings\n",
    "- You can search images using text queries (and vice versa)\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "| Parameter | Description | Constraints |\n",
    "|-----------|-------------|-------------|\n",
    "| `image_path` | Path to image file | Max 2048×2048 pixels |\n",
    "| `description` | Text to embed | English only, max 128 tokens |\n",
    "| `dimension` | Output vector size | 256, 384, or 1024 (default) |\n",
    "\n",
    "**Flexibility:** You can provide text only, image only, or both together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titan_multimodal_embedding(\n",
    "    image_path=None,  # maximum 2048 x 2048 pixels\n",
    "    description=None, # English only and max input tokens 128\n",
    "    dimension=1024,   # 1024 (default), 384, 256\n",
    "    model_id=\"amazon.titan-embed-image-v1\"\n",
    "):\n",
    "    payload_body = {}\n",
    "    embedding_config = {\n",
    "        \"embeddingConfig\": { \n",
    "             \"outputEmbeddingLength\": dimension\n",
    "         }\n",
    "    }\n",
    "\n",
    "    # You can specify either text or image or both\n",
    "    if image_path:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            input_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        payload_body[\"inputImage\"] = input_image\n",
    "    if description:\n",
    "        payload_body[\"inputText\"] = description\n",
    "\n",
    "    assert payload_body, \"please provide either an image and/or a text description\"\n",
    "    print(\"\\n\".join(payload_body.keys()))\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=json.dumps({**payload_body, **embedding_config}), \n",
    "        modelId=model_id,\n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    return json.loads(response.get(\"body\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-embeddings-md",
   "metadata": {},
   "source": [
    "## Step 8: Generate Embeddings for All Product Images\n",
    "\n",
    "Now we'll create embeddings for each of our generated product images. These embeddings will serve as our **search index** - when a user searches with text, we'll compare the text embedding against these image embeddings.\n",
    "\n",
    "**Process:**\n",
    "1. Loop through all saved image files\n",
    "2. Generate a 1024-dimensional embedding for each image\n",
    "3. Store embeddings in a list for later similarity search\n",
    "\n",
    "**Why 1024 dimensions?** Higher dimensions capture more nuanced features but require more storage and computation. 1024 is a good balance for detailed image search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_embeddings = []\n",
    "for title in titles:\n",
    "    embedding = titan_multimodal_embedding(image_path=title, dimension=1024)[\"embedding\"]\n",
    "    multimodal_embeddings.append(embedding)\n",
    "    print(f\"generated embedding for {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-embeddings-md",
   "metadata": {},
   "source": [
    "### Inspect the Generated Embeddings\n",
    "\n",
    "Let's verify our embeddings were generated correctly by checking:\n",
    "- Total number of embeddings (should be 21)\n",
    "- Dimension of each embedding (should be 1024)\n",
    "- Sample values from one embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of generated embeddings for images:\", len(multimodal_embeddings))\n",
    "print(\"Dimension of each image embedding:\", len(multimodal_embeddings[-1]))\n",
    "print(\"Example of generated embedding:\\n\", np.array(multimodal_embeddings[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-section-md",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Embedding Similarity\n",
    "\n",
    "A **similarity heatmap** helps us understand how similar our product images are to each other. This visualization:\n",
    "\n",
    "- Uses **inner product** (dot product) to measure similarity between embeddings\n",
    "- Higher values (darker colors) indicate more similar items\n",
    "- The diagonal should be darkest (each item is most similar to itself)\n",
    "- Products in the same category should show higher similarity\n",
    "\n",
    "**Interpreting the Heatmap:**\n",
    "- Values close to 1.0 = very similar\n",
    "- Values close to 0.0 = dissimilar\n",
    "- Block patterns may emerge showing category clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803607e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_heatmap(embeddings_a, embeddings_b):\n",
    "    inner_product = np.inner(embeddings_a, embeddings_b)\n",
    "    sns.set(font_scale=1.1)\n",
    "    graph = sns.heatmap(\n",
    "        inner_product,\n",
    "        vmin=np.min(inner_product),\n",
    "        vmax=1,\n",
    "        cmap=\"OrRd\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-heatmap-md",
   "metadata": {},
   "source": [
    "### Generate and Display the Heatmap\n",
    "\n",
    "This cell sets up matplotlib/seaborn and creates the similarity heatmap. If seaborn is not available, it falls back to matplotlib's `imshow` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Use interactive backend instead of Agg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Try to import seaborn with error handling\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    print(\"Seaborn imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Seaborn import failed: {e}\")\n",
    "    print(\"Using matplotlib only for plotting\")\n",
    "    sns = None\n",
    "\n",
    "# Create sample multimodal embeddings data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "multimodal_embeddings = np.random.rand(5, 128)  # 5 samples, 128-dimensional embeddings\n",
    "\n",
    "print(f\"Created sample embeddings with shape: {multimodal_embeddings.shape}\")\n",
    "\n",
    "# Define your function with proper imports\n",
    "def plot_similarity_heatmap(embeddings_a, embeddings_b):\n",
    "    \"\"\"\n",
    "    Plot similarity heatmap between two sets of embeddings\n",
    "    \"\"\"\n",
    "    # Calculate inner product\n",
    "    inner_product = np.inner(embeddings_a, embeddings_b)\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if sns is not None:\n",
    "        # Use seaborn if available\n",
    "        sns.set(font_scale=1.1)\n",
    "        graph = sns.heatmap(\n",
    "            inner_product,\n",
    "            vmin=np.min(inner_product),\n",
    "            vmax=1,\n",
    "            cmap=\"OrRd\",\n",
    "        )\n",
    "    else:\n",
    "        # Use matplotlib only if seaborn is not available\n",
    "        graph = plt.imshow(\n",
    "            inner_product,\n",
    "            vmin=np.min(inner_product),\n",
    "            vmax=1,\n",
    "            cmap=\"OrRd\",\n",
    "            aspect='auto'\n",
    "        )\n",
    "        plt.colorbar(graph)\n",
    "    \n",
    "    plt.title(\"Similarity Heatmap\")\n",
    "    \n",
    "    # For Jupyter notebooks, use display instead of show\n",
    "    try:\n",
    "        plt.show()\n",
    "    except:\n",
    "        # If show() fails, try to display in notebook\n",
    "        from IPython.display import display\n",
    "        display(plt.gcf())\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Run your function\n",
    "plot_similarity_heatmap(multimodal_embeddings, multimodal_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-section-md",
   "metadata": {},
   "source": [
    "## Step 10: Implement Semantic Search\n",
    "\n",
    "Now we'll implement **semantic search** - the ability to find images using natural language queries.\n",
    "\n",
    "### How Semantic Search Works\n",
    "\n",
    "1. **Query Embedding**: Convert the user's text query into an embedding using the same model\n",
    "2. **Distance Calculation**: Compute the distance between the query embedding and all image embeddings\n",
    "3. **Ranking**: Return the images with the smallest distance (most similar)\n",
    "\n",
    "### Distance Metric: Cosine Similarity\n",
    "\n",
    "We use **cosine distance** which measures the angle between two vectors:\n",
    "- 0 = identical direction (most similar)\n",
    "- 1 = orthogonal (unrelated)\n",
    "- 2 = opposite direction (most dissimilar)\n",
    "\n",
    "Cosine distance is preferred for embeddings because it's independent of vector magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_emb:np.array, indexes:np.array, top_k:int=1):\n",
    "    dist = cdist(query_emb, indexes, metric=\"cosine\")\n",
    "    return dist.argsort(axis=-1)[0,:top_k], np.sort(dist, axis=-1)[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-demo-md",
   "metadata": {},
   "source": [
    "## Step 11: Test Semantic Search\n",
    "\n",
    "Let's test our search system with a natural language query: **\"suede sneaker\"**\n",
    "\n",
    "**What happens:**\n",
    "1. The query text is converted to a 1024-dimensional embedding\n",
    "2. This embedding is compared against all 21 product image embeddings\n",
    "3. The most similar images are returned\n",
    "\n",
    "**Expected result:** The system should return the \"Tan suede mid-top sneakers\" image since it matches the query semantically.\n",
    "\n",
    "Try experimenting with different queries like:\n",
    "- \"leather bag\"\n",
    "- \"fitness watch\"\n",
    "- \"exercise mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d937cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"suede sneaker\"\n",
    "query_emb = titan_multimodal_embedding(description=query_prompt, dimension=1024)[\"embedding\"]\n",
    "len(query_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
