{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic Model List - Jupyter Notebook\n",
    "\n",
    "This notebook demonstrates how to retrieve and display a list of available models from the Anthropic AI platform.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this code, make sure you have:\n",
    "\n",
    "1. Installed the Anthropic Python client:\n",
    "   ```\n",
    "   !pip install anthropic\n",
    "   ```\n",
    "\n",
    "2. Set your Anthropic API key as an environment variable:\n",
    "   ```\n",
    "   import os\n",
    "   os.environ[\"ANTHROPIC_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "   ```\n",
    "   \n",
    "   Alternatively, you can create a `.env` file and load it:\n",
    "   ```\n",
    "   from dotenv import load_dotenv\n",
    "   load_dotenv()\n",
    "   ```\n",
    "\n",
    "## Retrieving and Displaying Anthropic Models\n",
    "\n",
    "The code below will:\n",
    "1. Initialize the Anthropic client\n",
    "2. Retrieve a list of available models (limited to 20)\n",
    "3. Display each model's ID and display name in a formatted way\n",
    "\n",
    "```python\n",
    "# Import the Anthropic library\n",
    "import anthropic\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Print model ID and display name\n",
    "print(\"Available Anthropic Models:\")\n",
    "print(\"--------------------------\")\n",
    "for model in models.data:\n",
    "    print(f\"{model.id:<35} | {model.display_name}\")\n",
    "```\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- The script uses the official Anthropic Python client to interact with their API\n",
    "- We limit the results to 20 models, although Anthropic typically offers fewer than that\n",
    "- For each model, we display:\n",
    "  - The model ID (e.g., \"claude-3-7-sonnet-20250219\") - this is what you use in API calls\n",
    "  - The human-readable display name (e.g., \"Claude 3.7 Sonnet\")\n",
    "- The `:<35` in the f-string is a formatting instruction that left-aligns the model ID text and pads it to 35 characters, creating a clean output\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "When run, this code should produce output similar to:\n",
    "\n",
    "```\n",
    "Available Anthropic Models:\n",
    "--------------------------\n",
    "claude-3-7-sonnet-20250219         | Claude 3.7 Sonnet\n",
    "claude-3-5-sonnet-20240620         | Claude 3.5 Sonnet\n",
    "claude-3-opus-20240229             | Claude 3 Opus\n",
    "claude-3-sonnet-20240229           | Claude 3 Sonnet\n",
    "claude-2.1                         | Claude 2.1\n",
    "```\n",
    "\n",
    "## Enhanced Display (Optional)\n",
    "\n",
    "If you want a more visually appealing display, you can use the `tabulate` library:\n",
    "\n",
    "```python\n",
    "import anthropic\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "model_data = []\n",
    "for model in models.data:\n",
    "    # Format the date to be more readable\n",
    "    created_date = model.created_at.strftime(\"%Y-%m-%d\")\n",
    "    model_data.append([model.id, model.display_name, created_date])\n",
    "\n",
    "# Print as a table\n",
    "print(tabulate(model_data, headers=[\"Model ID\", \"Display Name\", \"Created Date\"], tablefmt=\"grid\"))\n",
    "```\n",
    "\n",
    "Make sure to install tabulate first with `!pip install tabulate` if you choose this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic Models:\n",
      "--------------------------\n",
      "claude-opus-4-20250514              | Claude Opus 4\n",
      "claude-sonnet-4-20250514            | Claude Sonnet 4\n",
      "claude-3-7-sonnet-20250219          | Claude Sonnet 3.7\n",
      "claude-3-5-sonnet-20241022          | Claude Sonnet 3.5 (New)\n",
      "claude-3-5-haiku-20241022           | Claude Haiku 3.5\n",
      "claude-3-5-sonnet-20240620          | Claude Sonnet 3.5 (Old)\n",
      "claude-3-haiku-20240307             | Claude Haiku 3\n",
      "claude-3-opus-20240229              | Claude Opus 3\n",
      "claude-3-sonnet-20240229            | Claude Sonnet 3\n",
      "claude-2.1                          | Claude 2.1\n",
      "claude-2.0                          | Claude 2.0\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Print model ID and display name\n",
    "print(\"Available Anthropic Models:\")\n",
    "print(\"--------------------------\")\n",
    "for model in models.data:\n",
    "    print(f\"{model.id:<35} | {model.display_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API keys are successfully loaded.\n",
      "ðŸ”’ API keys are loaded but hidden for security.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check if API keys are loaded\n",
    "if anthropic_api_key:\n",
    "    print(\"âœ… API keys are successfully loaded.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: One or more API keys are missing.\")\n",
    "\n",
    "# Optionally, display API keys (for debugging purposes only)\n",
    "display_keys = False  # Change to True if you want to see the keys\n",
    "\n",
    "if display_keys:\n",
    "    print(f\"Anthropic API Key: {anthropic_api_key}\")\n",
    "else:\n",
    "    print(\"ðŸ”’ API keys are loaded but hidden for security.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"claude-3-5-haiku-20241022\"\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "def get_completion(prompt: str):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Hello, Claude!\"\n",
    "\n",
    "# Get Claude's response\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Model Comparison Guide\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "Anthropic offers several Claude models, each optimized for different use cases. Here's a comprehensive comparison to help you choose the right model for your needs. This is a rough guide to help you think about pricing. \n",
    "\n",
    "### Claude 4 Series (Latest)\n",
    "\n",
    "| Model | Context Window | Strengths | Best For | Pricing (per M tokens) |\n",
    "|-------|----------------|-----------|----------|------------------------|\n",
    "| **Claude Opus 4** | 200K tokens | Most capable, advanced reasoning, complex tasks | Research, analysis, coding, creative writing | Input: $15, Output: $75 |\n",
    "| **Claude Sonnet 4** | 200K tokens | Balanced performance and speed | General purpose, business applications | Input: $3, Output: $15 |\n",
    "\n",
    "### Claude 3 Series\n",
    "\n",
    "| Model | Context Window | Strengths | Best For | Pricing (per M tokens) |\n",
    "|-------|----------------|-----------|----------|------------------------|\n",
    "| **Claude 3.7 Sonnet** | 200K tokens | Enhanced reasoning, improved coding | Development, technical writing | Input: $3, Output: $15 |\n",
    "| **Claude 3.5 Sonnet (New)** | 200K tokens | Fast, versatile, good at reasoning | Most general use cases | Input: $3, Output: $15 |\n",
    "| **Claude 3.5 Haiku** | 200K tokens | Fastest, cost-effective | Simple tasks, high-volume processing | Input: $0.25, Output: $1.25 |\n",
    "| **Claude 3.5 Sonnet (Old)** | 200K tokens | Previous generation Sonnet | Legacy applications | Input: $3, Output: $15 |\n",
    "| **Claude 3 Opus** | 200K tokens | Most capable 3.x model | Complex reasoning, research | Input: $15, Output: $75 |\n",
    "| **Claude 3 Sonnet** | 200K tokens | Balanced 3.x model | General purpose | Input: $3, Output: $15 |\n",
    "| **Claude 3 Haiku** | 200K tokens | Fastest 3.x model | Simple, quick tasks | Input: $0.25, Output: $1.25 |\n",
    "\n",
    "## Model Selection Guide\n",
    "\n",
    "### ðŸŽ¯ **For Production Applications**\n",
    "- **Claude Sonnet 4**: Best balance of capability and cost\n",
    "- **Claude 3.5 Sonnet (New)**: Proven performance, widely adopted\n",
    "\n",
    "### âš¡ **For High-Volume/Speed-Critical Tasks**\n",
    "- **Claude 3.5 Haiku**: Fastest response times, most cost-effective\n",
    "- **Claude 3 Haiku**: Alternative for legacy systems\n",
    "\n",
    "### ðŸ§  **For Complex Reasoning & Analysis**\n",
    "- **Claude Opus 4**: Cutting-edge capabilities\n",
    "- **Claude 3 Opus**: Proven complex reasoning abilities\n",
    "\n",
    "### ðŸ’» **For Coding & Technical Tasks**\n",
    "- **Claude 3.7 Sonnet**: Enhanced for development workflows\n",
    "- **Claude Sonnet 4**: Advanced coding capabilities\n",
    "\n",
    "### ðŸ“ **For Creative Writing & Content**\n",
    "- **Claude Opus 4**: Most creative and nuanced\n",
    "- **Claude 3.5 Sonnet**: Good creative balance\n",
    "\n",
    "## Key Considerations\n",
    "\n",
    "### **Context Window**\n",
    "- All current models support 200K tokens (~150K words)\n",
    "- Ideal for processing long documents, codebases, or conversations\n",
    "\n",
    "### **Response Quality vs Speed**\n",
    "- **Opus models**: Highest quality, slower responses\n",
    "- **Sonnet models**: Balanced quality and speed\n",
    "- **Haiku models**: Fastest responses, good quality for simpler tasks\n",
    "\n",
    "### **Cost Optimization**\n",
    "- Start with **Claude 3.5 Haiku** for prototyping\n",
    "- Upgrade to **Sonnet** models for production\n",
    "- Use **Opus** models only when maximum capability is required\n",
    "\n",
    "### **Model Updates**\n",
    "- Claude 4 series represents the latest generation\n",
    "- Claude 3.5 models receive periodic updates\n",
    "- Always test new models before switching production systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages format\n",
    "\n",
    "As we saw in the previous lesson, we can use `client.messages.create()` to send a message to Claude and get a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_014WE4hqyZBF7Ynw3eTG39SF', content=[TextBlock(text='The exact recipe for Coca-Cola is a closely guarded trade secret, but the general flavors and ingredients are known:\\n\\n- Carbonated water - This provides the bubbly carbonation.\\n\\n- Caffeine - Coca-Cola contains around 34mg of caffeine per 12oz serving.\\n\\n- Sugar (or high fructose corn syrup) - This provides the sweetness.\\n\\n- Caramel coloring - This gives Coca-Cola its distinctive brown color.\\n\\n- Phosphoric acid - This adds tartness and a slightly acidic flavor.\\n\\nThe specific flavors used are a blend of citrus flavors (such as lemon, lime, and orange), spices (such as cinnamon and nutmeg), and other flavorings. The original Coca-Cola formula also contained coca leaf extract, which provided a small amount of cocaine, but this has been removed since the early 1900s.\\n\\nThe exact blend of these flavors is what gives Coca-Cola its unique taste, and the full recipe has been kept secret by the Coca-Cola company since its creation in 1886.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=17, output_tokens=247, service_tier='standard'))\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Coca Cola?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this bit: \n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The messages parameter is a crucial part of interacting with the Claude API. It allows you to provide the conversation history and context for Claude to generate a relevant response. \n",
    "\n",
    "The messages parameter expects a list of message dictionaries, where each dictionary represents a single message in the conversation.\n",
    "Each message dictionary should have the following keys:\n",
    "\n",
    "* `role`: A string indicating the role of the message sender. It can be either \"user\" (for messages sent by the user) or \"assistant\" (for messages sent by Claude).\n",
    "* `content`: A string or list of content dictionaries representing the actual content of the message. If a string is provided, it will be treated as a single text content block. If a list of content dictionaries is provided, each dictionary should have a \"type\" (e.g., \"text\" or \"image\") and the corresponding content.  For now, we'll leave `content` as a single string.\n",
    "\n",
    "Here's an example of a messages list with a single user message:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude! How are you today?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "And here's an example with multiple messages representing a conversation:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude! How are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! I'm doing well, thank you. How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a fun fact about ferrets?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Did you know that excited ferrets make a clucking vocalization known as 'dooking'?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Remember that messages always alternate between user and assistant messages (Source of Image: Anthropic Courses).\n",
    "\n",
    "![Alternating Messages](images/alternating_messages.png)\n",
    "\n",
    "The messages format allows us to structure our API calls to Claude in the form of a conversation, allowing for **context preservation**: The messages format allows for maintaining an entire conversation history, including both user and assistant messages. This ensures that Claude has access to the full context of the conversation when generating responses, leading to more coherent and relevant outputs.  \n",
    "\n",
    "**Note: many use-cases don't require a conversation history, and there's nothing wrong with providing a list of messages that only contains a single message!** "
   ]
  },
  {
   "cell_type": "code",
   "source": "# Example: Customer service bot with specific guidelines\nsystem_prompt = \"\"\"You are a friendly customer service representative for TechGadgets Inc.\n\nGuidelines:\n- Always be polite and professional\n- Address customers by name when provided\n- If you don't know something, offer to connect them with a specialist\n- Keep responses concise but helpful\n- Use emojis sparingly and professionally\"\"\"\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=300,\n    system=system_prompt,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hi, I'm Sarah. My laptop charger stopped working yesterday. What should I do?\"}\n    ]\n)\n\nprint(\"ðŸ¤– Customer Service Response:\")\nprint(response.content[0].text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## System Prompt Best Practices\n\n### âœ… Good System Prompts:\n- Clear and specific about role/behavior\n- Concise but complete\n- Focus on high-level guidelines\n- Set expectations for tone and format\n\n### âŒ Avoid:\n- Putting detailed instructions in system prompts (use user messages instead)\n- Making them too long (they count toward token limits)\n- Including task-specific details (those belong in user messages)\n\n## Advanced System Prompt Example",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Without system prompt\nresponse_no_system = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=200,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are list comprehensions in Python?\"}\n    ]\n)\n\nprint(\"ðŸ“ WITHOUT System Prompt:\")\nprint(response_no_system.content[0].text)\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# With system prompt\nresponse_with_system = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=200,\n    system=\"You are a helpful Python tutor who explains concepts in simple terms with practical examples. Always include a code example.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are list comprehensions in Python?\"}\n    ]\n)\n\nprint(\"ðŸŽ¯ WITH System Prompt:\")\nprint(response_with_system.content[0].text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# System Prompts\n\nSystem prompts are a powerful way to set the context, role, and behavior for Claude before the conversation begins. They're perfect for:\n- Defining Claude's role or persona\n- Setting guidelines and constraints\n- Providing background information\n- Establishing output format preferences\n\n## Why Use System Prompts?\n\nSystem prompts are processed differently than user messages:\n- They set the overall context for the entire conversation\n- They're more stable and consistent across responses\n- They help maintain character/role throughout long conversations\n- They're separate from the conversation history\n\n## Basic System Prompt Example\n\nLet's see how system prompts affect Claude's behavior:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Inspecting the message response\n",
    "Next, let's take a look at the shape of the response we get back from Claude. \n",
    "\n",
    "Let's ask Claude to do something simple and now let's inspect the contents of the `response` that we get back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01WapibPPHkyoxMZYvJAEwQ8', content=[TextBlock(text='Merci.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=20, output_tokens=7, service_tier='standard'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Translate Thank You to French. Respond with a single word\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a `Message` object that contains a handful of properties.  Here's an example:\n",
    "\n",
    "```\n",
    "Message(id='msg_01Mq5gDnUmDESukTgwPV8xtG', content=[TextBlock(text='Bonjour.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=19, output_tokens=8))\n",
    "```\n",
    "\n",
    " The most important piece of information is the `content` property: this contains the actual content the model generated for us.   This is a **list** of content blocks, each of which has a type that determines its shape.\n",
    "\n",
    " In order to access the actual text content of the model's response, we need to do the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci.\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `content`, the `Message` object contains some other pieces of information:\n",
    "\n",
    "* `id` - a unique object identifier\n",
    "* `type` - The object type, which will always be \"message\"\n",
    "* `role` - The conversational role of the generated message. This will always be \"assistant\".\n",
    "* `model` - The model that handled the request and generated the response\n",
    "* `stop_reason` - The reason the model stopped generating.  We'll learn more about this later.\n",
    "* `stop_sequence` - We'll learn more about this shortly.\n",
    "* `usage` - information on billing and rate-limit usage. Contains information on:\n",
    "    * `input_tokens` - The number of input tokens that were used.\n",
    "    * `output_tokens` - The number of output tokens that were used.\n",
    "\n",
    "It's important to know that we have access to these pieces of information, but if you only remember one thing, make it this: `content` contains the actual model-generated content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages list use cases\n",
    "\n",
    "The messages list is a powerful feature that allows you to build complex interactions with Claude. Here are some common use cases:\n",
    "\n",
    "### Putting words in Claude's mouth\n",
    "\n",
    "Another common strategy for getting very specific outputs is to \"put words in Claude's mouth\".  Instead of only providing `user` messages to Claude, we can also supply an `assistant` message that Claude will use when generating output.  \n",
    "\n",
    "When using Anthropicâ€™s API, you are not limited to just the `user` message. If you supply an `assistant` message, Claude will continue the conversation from the last `assistant` token.  Just remember that we must start with a `user` message.\n",
    "\n",
    "Suppose I want Claude to write me a haiku that starts with the first line, \"calming mountain air\".  I can provide the following conversation history: \n",
    "\n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Generate a beautiful haiku\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"calming mountain air\"}\n",
    "    ]\n",
    "```\n",
    "We tell Claude that we want it to generate a Haiku AND we put the first line of the Haiku in Claude's mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "petals fall like gentle tears\n",
      "nature's soothing dance\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Generate a beautiful haiku\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"calming mountain air\"}\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the entire haiku, starting with the line we provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calming mountain air\n",
      "petals fall like gentle tears\n",
      "nature's soothing dance\n"
     ]
    }
   ],
   "source": [
    "print(\"calming mountain air\" + response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "source": "models_to_compare = [\n    \"claude-3-5-haiku-20241022\",\n    \"claude-3-5-sonnet-20241022\",\n    \"claude-3-opus-20240229\"\n]\n\nprompt = \"Write a professional email thanking a client for their business.\"\n\nprint(\"ðŸ’µ Cost Comparison Across Models\")\nprint(\"=\" * 80 + \"\\n\")\n\nfor model in models_to_compare:\n    try:\n        response = client.messages.create(\n            model=model,\n            max_tokens=300,\n            temperature=0.7,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        cost, details = calculate_cost(\n            response.usage.input_tokens,\n            response.usage.output_tokens,\n            model\n        )\n        \n        model_name = model.split(\"-\")[1].capitalize() + \" \" + model.split(\"-\")[2].capitalize()\n        print(f\"ðŸ“Š {model_name}:\")\n        print(f\"   Tokens: {details['total_tokens']:,} | Cost: {details['total_cost']}\")\n        print()\n        \n        time.sleep(1)  # Rate limit protection\n        \n    except Exception as e:\n        print(f\"âŒ {model}: {str(e)}\\n\")\n\nprint(\"=\" * 80)\nprint(\"ðŸ’¡ Tip: Use Haiku for simple tasks, Sonnet for balanced needs, Opus for complex reasoning\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cost Comparison Across Models\n\nLet's compare the cost of the same task across different Claude models:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Make an API call\nmodel_to_use = \"claude-3-5-haiku-20241022\"\nresponse = client.messages.create(\n    model=model_to_use,\n    max_tokens=500,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Explain the concept of machine learning in 3 sentences.\"\n    }]\n)\n\n# Extract token usage from response\ninput_tokens = response.usage.input_tokens\noutput_tokens = response.usage.output_tokens\n\n# Calculate cost\ncost, details = calculate_cost(input_tokens, output_tokens, model_to_use)\n\nprint(\"ðŸ“ž API Call Results:\")\nprint(\"=\" * 80)\nprint(\"\\nðŸ¤– Claude's Response:\")\nprint(response.content[0].text)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nðŸ’° Cost Analysis:\")\nprint(f\"  Model: {model_to_use}\")\nprint(f\"  Input tokens:  {details['input_tokens']:,}\")\nprint(f\"  Output tokens: {details['output_tokens']:,}\")\nprint(f\"  Total tokens:  {details['total_tokens']:,}\")\nprint(f\"  TOTAL COST:    {details['total_cost']}\")\nprint(\"\\n\" + \"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Real API Call with Cost Tracking\n\nLet's make an actual API call and track its cost:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_cost(input_tokens, output_tokens, model_id):\n    \"\"\"\n    Calculate the cost of an API call based on token usage.\n    \n    Pricing as of 2025 (per million tokens):\n    \"\"\"\n    # Pricing table (dollars per 1M tokens)\n    pricing = {\n        \"claude-opus-4-20250514\": {\"input\": 15.00, \"output\": 75.00},\n        \"claude-sonnet-4-20250514\": {\"input\": 3.00, \"output\": 15.00},\n        \"claude-3-7-sonnet-20250219\": {\"input\": 3.00, \"output\": 15.00},\n        \"claude-3-5-sonnet-20241022\": {\"input\": 3.00, \"output\": 15.00},\n        \"claude-3-5-haiku-20241022\": {\"input\": 0.25, \"output\": 1.25},\n        \"claude-3-opus-20240229\": {\"input\": 15.00, \"output\": 75.00},\n        \"claude-3-sonnet-20240229\": {\"input\": 3.00, \"output\": 15.00},\n        \"claude-3-haiku-20240307\": {\"input\": 0.25, \"output\": 1.25},\n    }\n    \n    if model_id not in pricing:\n        return None, \"Model pricing not found\"\n    \n    # Calculate costs\n    input_cost = (input_tokens / 1_000_000) * pricing[model_id][\"input\"]\n    output_cost = (output_tokens / 1_000_000) * pricing[model_id][\"output\"]\n    total_cost = input_cost + output_cost\n    \n    return total_cost, {\n        \"input_tokens\": input_tokens,\n        \"output_tokens\": output_tokens,\n        \"total_tokens\": input_tokens + output_tokens,\n        \"input_cost\": f\"${input_cost:.6f}\",\n        \"output_cost\": f\"${output_cost:.6f}\",\n        \"total_cost\": f\"${total_cost:.6f}\"\n    }\n\n# Test the calculator\ncost, details = calculate_cost(1000, 500, \"claude-3-5-haiku-20241022\")\nprint(\"ðŸ’° Cost Calculator Example:\")\nprint(f\"  Input tokens:  {details['input_tokens']:,}\")\nprint(f\"  Output tokens: {details['output_tokens']:,}\")\nprint(f\"  Total tokens:  {details['total_tokens']:,}\")\nprint(f\"  Input cost:    {details['input_cost']}\")\nprint(f\"  Output cost:   {details['output_cost']}\")\nprint(f\"  TOTAL COST:    {details['total_cost']}\")\n\nprint(f\"\\nâœ… Cost calculator function ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Token Counting & Cost Management\n\nUnderstanding tokens and costs is essential for building production applications with Claude. Every API call costs money based on the number of tokens processed.\n\n## What are Tokens?\n\n- **Tokens** are the basic units of text that Claude processes\n- Roughly: 1 token â‰ˆ 4 characters or â‰ˆ 0.75 words in English\n- Both input (prompt) and output (response) consume tokens\n- Different models have different pricing per token\n\n## Cost Calculator Function\n\nLet's build a useful cost calculator:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Data extraction (use low temperature)\nprint(\"ðŸ“Š Data Extraction (Temperature = 0.0):\")\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=100,\n    temperature=0.0,\n    messages=[{\"role\": \"user\", \"content\": \"Extract the email from: Contact us at support@example.com for help\"}]\n)\nprint(response.content[0].text)\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Example 2: Creative writing (use high temperature)\nprint(\"âœ¨ Creative Writing (Temperature = 1.0):\")\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=100,\n    temperature=1.0,\n    messages=[{\"role\": \"user\", \"content\": \"Write an opening line for a sci-fi story\"}]\n)\nprint(response.content[0].text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Other Important Parameters\n\n### max_tokens\nControls the maximum length of the response.\n\n```python\n# Short response\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=50,  # Very short\n    messages=[{\"role\": \"user\", \"content\": \"Explain AI\"}]\n)\n```\n\n### top_p (Nucleus Sampling)\nAlternative to temperature for controlling randomness.\n- Range: 0.0 to 1.0\n- Considers only the most probable tokens whose cumulative probability exceeds p\n\n### top_k\nLimits the number of tokens to consider at each step.\n- Useful for controlling vocabulary size\n- Combine with temperature for fine-tuned control\n\n## Practical Temperature Examples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\n\nprompt = \"Write a creative tagline for an eco-friendly coffee shop\"\ntemperatures = [0.0, 0.5, 1.0]\n\nprint(\"ðŸŒ¡ï¸ Temperature Comparison\\n\" + \"=\"*80)\n\nfor temp in temperatures:\n    responses = []\n    print(f\"\\nðŸ”¹ Temperature: {temp}\")\n    print(\"-\" * 80)\n    \n    # Generate 3 responses to show variety\n    for i in range(3):\n        response = client.messages.create(\n            model=\"claude-3-5-haiku-20241022\",\n            max_tokens=50,\n            temperature=temp,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        tagline = response.content[0].text.strip()\n        print(f\"  {i+1}. {tagline}\")\n        time.sleep(0.5)  # Brief pause to avoid rate limits\n    \nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸ“Š Notice: Lower temps = more similar outputs, Higher temps = more variety\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Temperature & Model Parameters\n\nTemperature controls the randomness and creativity of Claude's responses. Understanding how to use it effectively is crucial for getting the right outputs.\n\n## What is Temperature?\n\n- **Range**: 0.0 to 1.0\n- **Low temperature (0.0-0.3)**: More focused, deterministic, consistent\n- **Medium temperature (0.4-0.7)**: Balanced creativity and consistency  \n- **High temperature (0.8-1.0)**: More creative, diverse, unpredictable\n\n## When to Use Different Temperatures\n\n| Temperature | Best For | Examples |\n|-------------|----------|----------|\n| **0.0-0.3** | Factual tasks, code generation, data extraction | API responses, translations, summaries |\n| **0.4-0.7** | Balanced tasks, Q&A, explanations | Customer support, tutorials, analysis |\n| **0.8-1.0** | Creative tasks, brainstorming, variety | Marketing copy, stories, ideation |\n\n## Temperature Comparison Example\n\nLet's see how temperature affects output for the same prompt:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "One of the most useful prompting strategies is called \"few-shot prompting\" which involves providing a model with a small number of **examples**.  These examples help guide Claude's generated output.  The messages conversation history is an easy way to provide examples to Claude.\n",
    "\n",
    "For example, suppose we want to use Claude to analyze the sentiment in tweets.  We could start by simply asking Claude to \"please analyze the sentiment in this tweet: \" and see what sort of output we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment in this tweet is positive. Here's a breakdown of the analysis:\n",
      "\n",
      "1. Positive language: The tweet uses phrases like \"doing a happy dance\" and \"pickleslove\" which convey a sense of excitement and enjoyment.\n",
      "\n",
      "2. Emojis: The use of the pepper and pickle emojis add a playful and enthusiastic tone to the tweet.\n",
      "\n",
      "3. Endorsement: The tweeter is positively endorsing the new spicy pickles from the company \"@PickleCo\", suggesting they enjoyed the product.\n",
      "\n",
      "4. Hashtags: The hashtags \"#pickleslove\" and \"#spicyfood\" reinforce the positive sentiment around the pickles and the tweeter's enjoyment of spicy food.\n",
      "\n",
      "Overall, the tweet expresses a very favorable sentiment towards the new spicy pickles from @PickleCo. The tweeter seems genuinely excited and pleased with the product, indicating a positive experience.\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the sentiment in this tweet: Just tried the new spicy pickles from @PickleCo, and my taste buds are doing a happy dance! ðŸŒ¶ï¸ðŸ¥’ #pickleslove #spicyfood\"},\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time I ran the above code, Claude generated this long response: \n",
    "```\n",
    "The sentiment in this tweet is overwhelmingly positive. The user expresses their enjoyment of the new spicy pickles from @PickleCo, using enthusiastic language and emojis to convey their delight.\n",
    "\n",
    "Positive indicators:\n",
    "1. \"My taste buds are doing a happy dance!\" - This phrase indicates that the user is extremely pleased with the taste of the pickles, to the point of eliciting a joyful physical response.\n",
    "\n",
    "2. Emojis - The use of the hot pepper ðŸŒ¶ï¸ and cucumber ðŸ¥’ emojis further emphasizes the user's excitement about the spicy pickles.\n",
    "\n",
    "3. Hashtags - The inclusion of #pickleslove and #spicyfood hashtags suggests that the user has a strong affinity for pickles and spicy food, and the new product aligns perfectly with their preferences.\n",
    "\n",
    "4. Exclamation mark - The exclamation mark at the end of the first sentence adds emphasis to the user's positive experience.\n",
    "\n",
    "Overall, the tweet conveys a strong sense of satisfaction, excitement, and enjoyment related to trying the new spicy pickles from @PickleCo.\n",
    "```\n",
    "\n",
    "This is a great response, but it's probably way more information than we need from Claude, especially if we're trying to automate the sentiment analysis of a large number of tweets.  \n",
    "\n",
    "We might prefer that Claude respond with a standardized output format like a single word (POSITIVE, NEUTRAL, NEGATIVE) or a numeric value (1, 0, -1).  For readability and simplicity, let's get Claude to respond with either \"POSITIVE\" or \"NEGATIVE\".  One way of doing this is through few-shot prompting.  We can provide Claude with a conversation history that shows exactly how we want it to respond: \n",
    "\n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Unpopular opinion: Pickles are disgusting. Don't @ me\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"I think my love for pickles might be getting out of hand. I just bought a pickle-shaped pool float\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"POSITIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Seriously why would anyone ever eat a pickle?  Those things are nasty!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Just tried the new spicy pickles from @PickleCo, and my taste buds are doing a happy dance! ðŸŒ¶ï¸ðŸ¥’ #pickleslove #spicyfood\"},\n",
    "    ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Unpopular opinion: Pickles are disgusting. Don't @ me\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"I think my love for pickles might be getting out of hand. I just bought a pickle-shaped pool float\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"POSITIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Seriously why would anyone ever eat a pickle?  Those things are nasty!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Just tried the new spicy pickles from @PickleCo, and my taste buds are doing a happy dance! ðŸŒ¶ï¸ðŸ¥’ #pickleslove #spicyfood\"},\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Error Handling & Retry Logic\n\nProduction applications need robust error handling to deal with API failures, rate limits, and network issues. Let's learn how to build resilient applications with Claude.\n\n## Common Error Types\n\nThe Anthropic API can return several types of errors:\n\n1. **APIError**: General API errors (500+ status codes)\n2. **RateLimitError**: Too many requests (429 status code)\n3. **APIConnectionError**: Network connectivity issues\n4. **AuthenticationError**: Invalid API key (401 status code)\n5. **BadRequestError**: Invalid request parameters (400 status code)\n\n## Basic Error Handling\n\nLet's start with basic try-except error handling:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from anthropic import APIError, RateLimitError, APIConnectionError, AuthenticationError, BadRequestError\n\ndef safe_api_call(prompt, model=\"claude-3-5-haiku-20241022\", max_tokens=500):\n    \"\"\"\n    Make an API call with basic error handling.\n    \"\"\"\n    try:\n        response = client.messages.create(\n            model=model,\n            max_tokens=max_tokens,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.content[0].text, None\n    \n    except RateLimitError as e:\n        error_msg = f\"âš ï¸ Rate limit exceeded. Please wait before making more requests.\"\n        print(error_msg)\n        return None, error_msg\n    \n    except AuthenticationError as e:\n        error_msg = f\"ðŸ”’ Authentication failed. Check your API key.\"\n        print(error_msg)\n        return None, error_msg\n    \n    except BadRequestError as e:\n        error_msg = f\"âŒ Bad request: {str(e)}\"\n        print(error_msg)\n        return None, error_msg\n    \n    except APIConnectionError as e:\n        error_msg = f\"ðŸŒ Connection error. Check your internet connection.\"\n        print(error_msg)\n        return None, error_msg\n    \n    except APIError as e:\n        error_msg = f\"âš¡ API error: {str(e)}\"\n        print(error_msg)\n        return None, error_msg\n    \n    except Exception as e:\n        error_msg = f\"â— Unexpected error: {str(e)}\"\n        print(error_msg)\n        return None, error_msg\n\n# Test the function\nresult, error = safe_api_call(\"What is the capital of France?\")\nif result:\n    print(f\"âœ… Success: {result}\")\nelse:\n    print(f\"Failed with error: {error}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Retry Logic with Exponential Backoff\n\nWhen rate limits or temporary errors occur, it's best to retry with exponential backoff. This means waiting progressively longer between retries.\n\n### Why Exponential Backoff?\n- **Prevents overwhelming the API** during high-traffic periods\n- **Gives time for rate limits to reset**\n- **Increases success rate** for transient errors\n- **Standard practice** recommended by Anthropic\n\nLet's implement a robust retry function:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\nfrom typing import Optional, Tuple\n\ndef call_claude_with_retry(\n    prompt: str,\n    model: str = \"claude-3-5-haiku-20241022\",\n    max_tokens: int = 500,\n    max_retries: int = 3,\n    initial_delay: float = 1.0\n) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Call Claude API with exponential backoff retry logic.\n    \n    Args:\n        prompt: The user prompt\n        model: Model to use\n        max_tokens: Maximum tokens in response\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay in seconds (doubles with each retry)\n    \n    Returns:\n        Tuple of (response_text, error_message)\n    \"\"\"\n    delay = initial_delay\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.messages.create(\n                model=model,\n                max_tokens=max_tokens,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            # Success!\n            if attempt > 0:\n                print(f\"âœ… Success after {attempt + 1} attempts\")\n            \n            return response.content[0].text, None\n        \n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                print(f\"âš ï¸ Rate limit hit. Waiting {delay:.1f}s before retry {attempt + 2}/{max_retries}...\")\n                time.sleep(delay)\n                delay *= 2  # Exponential backoff\n            else:\n                error_msg = f\"âŒ Rate limit exceeded after {max_retries} attempts\"\n                print(error_msg)\n                return None, error_msg\n        \n        except APIConnectionError as e:\n            if attempt < max_retries - 1:\n                print(f\"ðŸŒ Connection error. Retrying in {delay:.1f}s... ({attempt + 2}/{max_retries})\")\n                time.sleep(delay)\n                delay *= 2\n            else:\n                error_msg = f\"âŒ Connection failed after {max_retries} attempts\"\n                print(error_msg)\n                return None, error_msg\n        \n        except (AuthenticationError, BadRequestError) as e:\n            # Don't retry these - they won't succeed on retry\n            error_msg = f\"âŒ Non-retryable error: {str(e)}\"\n            print(error_msg)\n            return None, error_msg\n        \n        except APIError as e:\n            if attempt < max_retries - 1:\n                print(f\"âš¡ API error. Retrying in {delay:.1f}s... ({attempt + 2}/{max_retries})\")\n                time.sleep(delay)\n                delay *= 2\n            else:\n                error_msg = f\"âŒ API error after {max_retries} attempts: {str(e)}\"\n                print(error_msg)\n                return None, error_msg\n    \n    return None, \"Max retries exceeded\"\n\n# Test the retry function\nprint(\"Testing retry logic with a normal request:\\n\")\nresult, error = call_claude_with_retry(\"Explain recursion in one sentence.\")\n\nif result:\n    print(f\"\\nðŸ“ Response: {result}\")\nelse:\n    print(f\"\\nâŒ Failed: {error}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Practical Example: Batch Processing with Error Handling\n\nLet's combine error handling and retry logic for a real-world use case - processing multiple prompts:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def batch_process_prompts(prompts: list, delay_between_requests: float = 0.5):\n    \"\"\"\n    Process multiple prompts with error handling and rate limiting.\n    \n    Args:\n        prompts: List of prompt strings\n        delay_between_requests: Delay between successful requests (rate limiting)\n    \n    Returns:\n        List of results (each is a dict with prompt, response, and error)\n    \"\"\"\n    results = []\n    \n    print(f\"ðŸ“Š Processing {len(prompts)} prompts...\\n\")\n    print(\"=\" * 80)\n    \n    for i, prompt in enumerate(prompts, 1):\n        print(f\"\\n[{i}/{len(prompts)}] Processing: '{prompt[:50]}...'\")\n        \n        response, error = call_claude_with_retry(\n            prompt=prompt,\n            max_tokens=100,\n            max_retries=3\n        )\n        \n        results.append({\n            \"prompt\": prompt,\n            \"response\": response,\n            \"error\": error,\n            \"success\": response is not None\n        })\n        \n        # Rate limiting: wait between successful requests\n        if response and i < len(prompts):\n            time.sleep(delay_between_requests)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(f\"\\nâœ… Completed: {sum(1 for r in results if r['success'])}/{len(prompts)} successful\")\n    \n    return results\n\n# Example batch processing\nprompts = [\n    \"What is Python?\",\n    \"What is JavaScript?\",\n    \"What is Java?\",\n    \"What is C++?\"\n]\n\nresults = batch_process_prompts(prompts, delay_between_requests=0.3)\n\n# Display results\nprint(\"\\nðŸ“‹ Results Summary:\")\nprint(\"=\" * 80)\nfor i, result in enumerate(results, 1):\n    if result['success']:\n        print(f\"\\n{i}. âœ… {result['prompt']}\")\n        print(f\"   Response: {result['response'][:100]}...\")\n    else:\n        print(f\"\\n{i}. âŒ {result['prompt']}\")\n        print(f\"   Error: {result['error']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Multi-turn Conversations\n\nBuilding conversational applications requires managing conversation history effectively. Claude can maintain context across multiple turns by passing the full message history.\n\n## Why Multi-turn Conversations?\n\n- **Context preservation**: Claude remembers earlier parts of the conversation\n- **Natural dialogue**: Enables back-and-forth interactions\n- **Follow-up questions**: Users can ask clarifying questions\n- **Personalization**: Build rapport and adapt to user needs\n\n## Basic Multi-turn Example",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Start a conversation\nconversation_history = []\n\ndef chat(user_message, system_prompt=None):\n    \"\"\"\n    Send a message and maintain conversation history.\n    \"\"\"\n    # Add user message to history\n    conversation_history.append({\n        \"role\": \"user\",\n        \"content\": user_message\n    })\n    \n    # Make API call with full history\n    response = client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=500,\n        system=system_prompt if system_prompt else \"You are a helpful assistant.\",\n        messages=conversation_history\n    )\n    \n    # Add assistant response to history\n    assistant_message = response.content[0].text\n    conversation_history.append({\n        \"role\": \"assistant\",\n        \"content\": assistant_message\n    })\n    \n    return assistant_message\n\n# Example conversation\nprint(\"ðŸ—£ï¸ Multi-turn Conversation Example\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Turn 1\nprint(\"ðŸ‘¤ User: What's the capital of France?\")\nresponse1 = chat(\"What's the capital of France?\")\nprint(f\"ðŸ¤– Claude: {response1}\\n\")\n\n# Turn 2 - Claude remembers context\nprint(\"ðŸ‘¤ User: What's the population?\")\nresponse2 = chat(\"What's the population?\")\nprint(f\"ðŸ¤– Claude: {response2}\\n\")\n\n# Turn 3 - Still remembers\nprint(\"ðŸ‘¤ User: What are some famous landmarks there?\")\nresponse3 = chat(\"What are some famous landmarks there?\")\nprint(f\"ðŸ¤– Claude: {response3}\\n\")\n\nprint(\"=\" * 80)\nprint(f\"ðŸ“Š Conversation has {len(conversation_history)} messages\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Conversation Management Class\n\nFor production applications, it's useful to create a conversation manager:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ConversationManager:\n    \"\"\"\n    Manage multi-turn conversations with Claude.\n    \"\"\"\n    def __init__(self, system_prompt=None, model=\"claude-3-5-haiku-20241022\"):\n        self.messages = []\n        self.system_prompt = system_prompt\n        self.model = model\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n    \n    def send_message(self, user_message: str) -> str:\n        \"\"\"Send a message and get response.\"\"\"\n        # Add user message\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": user_message\n        })\n        \n        # Get response\n        response = client.messages.create(\n            model=self.model,\n            max_tokens=500,\n            system=self.system_prompt if self.system_prompt else \"You are a helpful assistant.\",\n            messages=self.messages\n        )\n        \n        # Track tokens\n        self.total_input_tokens += response.usage.input_tokens\n        self.total_output_tokens += response.usage.output_tokens\n        \n        # Add assistant response\n        assistant_message = response.content[0].text\n        self.messages.append({\n            \"role\": \"assistant\",\n            \"content\": assistant_message\n        })\n        \n        return assistant_message\n    \n    def get_history(self) -> list:\n        \"\"\"Get conversation history.\"\"\"\n        return self.messages\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get conversation statistics.\"\"\"\n        return {\n            \"turns\": len(self.messages) // 2,\n            \"total_messages\": len(self.messages),\n            \"input_tokens\": self.total_input_tokens,\n            \"output_tokens\": self.total_output_tokens,\n            \"total_tokens\": self.total_input_tokens + self.total_output_tokens\n        }\n    \n    def reset(self):\n        \"\"\"Reset conversation.\"\"\"\n        self.messages = []\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n\n# Example: Create a technical support chatbot\nsupport_bot = ConversationManager(\n    system_prompt=\"\"\"You are a friendly technical support agent for a software company.\n    Be helpful, patient, and provide clear step-by-step instructions.\"\"\"\n)\n\nprint(\"ðŸ¤– Technical Support Chatbot\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Simulate a support conversation\nqueries = [\n    \"My app keeps crashing when I try to export data\",\n    \"I'm using version 2.1.5\",\n    \"It happens right after I click the export button\"\n]\n\nfor query in queries:\n    print(f\"ðŸ‘¤ User: {query}\")\n    response = support_bot.send_message(query)\n    print(f\"ðŸ¤– Support: {response}\\n\")\n\n# Show statistics\nstats = support_bot.get_stats()\nprint(\"=\" * 80)\nprint(f\"ðŸ“Š Conversation Stats:\")\nprint(f\"   Turns: {stats['turns']}\")\nprint(f\"   Total messages: {stats['total_messages']}\")\nprint(f\"   Total tokens: {stats['total_tokens']:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Stop Sequences\n\nStop sequences tell Claude when to stop generating text. They're useful for:\n- Controlling output format\n- Implementing custom delimiters\n- Creating structured outputs\n- Preventing over-generation\n\n## How Stop Sequences Work\n\nWhen Claude encounters a stop sequence in its generation, it immediately stops and returns the text generated up to (but not including) that sequence.\n\n## Basic Stop Sequence Example",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Stop at a specific marker\nprint(\"ðŸ›‘ Stop Sequence Example 1: Custom Delimiter\")\nprint(\"=\" * 80 + \"\\n\")\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=500,\n    stop_sequences=[\"###\"],\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Write a short story about a robot. End with ###\"\n    }]\n)\n\nprint(f\"Response: {response.content[0].text}\")\nprint(f\"\\nStop reason: {response.stop_reason}\")\nprint(f\"Stop sequence: {response.stop_sequence}\")\n\nprint(\"\\n\" + \"=\" * 80 + \"\\n\")\n\n# Example 2: Multiple stop sequences\nprint(\"ðŸ›‘ Stop Sequence Example 2: Multiple Delimiters\")\nprint(\"=\" * 80 + \"\\n\")\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=500,\n    stop_sequences=[\"DONE\", \"END\", \"FINISHED\"],\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"List 3 programming languages, one per line. After the list, write DONE.\"\n    }]\n)\n\nprint(f\"Response: {response.content[0].text}\")\nprint(f\"\\nStop reason: {response.stop_reason}\")\nprint(f\"Stop sequence triggered: {response.stop_sequence}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Practical Use Cases for Stop Sequences\n\n### 1. Structured Data Extraction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract structured data with stop sequences\nprint(\"ðŸ“‹ Use Case: Structured Data Extraction\")\nprint(\"=\" * 80 + \"\\n\")\n\nproduct_text = \"\"\"\nExtract product information from this text and format as:\nProduct: [name]\nPrice: [price]\nRating: [rating]\nEND_EXTRACTION\n\nText: \"The UltraWidget 3000 is available for $299.99 and has a 4.5-star rating.\"\n\"\"\"\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=200,\n    stop_sequences=[\"END_EXTRACTION\"],\n    messages=[{\"role\": \"user\", \"content\": product_text}]\n)\n\nprint(response.content[0].text)\nprint(\"\\nâœ… Stopped cleanly at delimiter\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Dialogue Systems",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Use stop sequences for dialogue formatting\nprint(\"ðŸ’¬ Use Case: Dialogue System\")\nprint(\"=\" * 80 + \"\\n\")\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=300,\n    stop_sequences=[\"\\nUser:\", \"\\nHuman:\"],\n    system=\"You are an AI assistant in a chat interface. Always end your responses naturally.\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Tell me about Python programming.\"\n    }]\n)\n\nprint(f\"Assistant: {response.content[0].text}\")\nprint(\"\\nâœ… Prevents generating fake user messages\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Code Generation with Boundaries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate code with clear boundaries\nprint(\"ðŸ’» Use Case: Code Generation\")\nprint(\"=\" * 80 + \"\\n\")\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=400,\n    stop_sequences=[\"```\\n\\n\", \"# End of code\"],\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"\"\"Write a Python function to calculate factorial. \n        Format it in a code block and add a comment '# End of code' after the function.\"\"\"\n    }]\n)\n\nprint(response.content[0].text)\nprint(\"\\nâœ… Clean code extraction\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Streaming Responses\n\nStreaming allows you to receive Claude's response incrementally as it's generated, rather than waiting for the complete response. This dramatically improves perceived performance for users.\n\n## Why Use Streaming?\n\n- **Better UX**: Users see output immediately\n- **Lower perceived latency**: Feels faster even if total time is the same\n- **Progressive rendering**: Display content as it arrives\n- **Early cancellation**: Users can stop generation if not needed\n\n## Basic Streaming Example",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\n\n# Simple streaming example\nprint(\"ðŸŒŠ Streaming Response Example\")\nprint(\"=\" * 80 + \"\\n\")\n\nwith client.messages.stream(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=300,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Write a haiku about programming.\"\n    }]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n\nprint(\"\\n\\n\" + \"=\" * 80)\nprint(\"âœ… Response streamed in real-time!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced Streaming with Event Handling",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced streaming with full event access\nprint(\"ðŸ”„ Advanced Streaming with Events\")\nprint(\"=\" * 80 + \"\\n\")\n\nfull_response = \"\"\n\nwith client.messages.stream(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=500,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Explain the concept of streaming in 2 sentences.\"\n    }]\n) as stream:\n    # Access different event types\n    for event in stream:\n        if event.type == \"content_block_delta\":\n            # Text is being generated\n            if hasattr(event.delta, 'text'):\n                text_chunk = event.delta.text\n                full_response += text_chunk\n                print(text_chunk, end=\"\", flush=True)\n        \n        elif event.type == \"message_start\":\n            # Message generation started\n            pass\n        \n        elif event.type == \"message_stop\":\n            # Message generation completed\n            print(\"\\n\\nâœ… Stream completed\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"ðŸ“ Full response captured: {len(full_response)} characters\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Streaming with Token Tracking",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stream and collect usage statistics\nprint(\"ðŸ“Š Streaming with Token Tracking\")\nprint(\"=\" * 80 + \"\\n\")\n\nwith client.messages.stream(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=400,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"List 5 benefits of using AI in software development.\"\n    }]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n    \n    # Get the final message with usage statistics\n    final_message = stream.get_final_message()\n\nprint(\"\\n\\n\" + \"=\" * 80)\nprint(\"ðŸ’° Usage Statistics:\")\nprint(f\"   Input tokens:  {final_message.usage.input_tokens}\")\nprint(f\"   Output tokens: {final_message.usage.output_tokens}\")\nprint(f\"   Total tokens:  {final_message.usage.input_tokens + final_message.usage.output_tokens}\")\nprint(f\"   Stop reason:   {final_message.stop_reason}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Streaming Helper Function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def stream_response(prompt: str, model=\"claude-3-5-haiku-20241022\", max_tokens=1000, \n                   show_stats=True):\n    \"\"\"\n    Stream a response and optionally show statistics.\n    \n    Args:\n        prompt: User prompt\n        model: Model to use\n        max_tokens: Maximum tokens\n        show_stats: Whether to show usage statistics\n    \n    Returns:\n        Full response text\n    \"\"\"\n    full_response = \"\"\n    \n    with client.messages.stream(\n        model=model,\n        max_tokens=max_tokens,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    ) as stream:\n        for text in stream.text_stream:\n            full_response += text\n            print(text, end=\"\", flush=True)\n        \n        final_message = stream.get_final_message()\n    \n    if show_stats:\n        print(f\"\\n\\nðŸ“Š Tokens: {final_message.usage.input_tokens} in / \"\n              f\"{final_message.usage.output_tokens} out\")\n    \n    return full_response\n\n# Test the helper\nprint(\"ðŸŽ¯ Streaming Helper Function Test\")\nprint(\"=\" * 80 + \"\\n\")\n\nresponse = stream_response(\n    \"Write a motivational quote about learning.\",\n    max_tokens=100\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Prompt Templates & Reusable Patterns\n\nBuilding reusable prompt templates helps maintain consistency and makes your code more maintainable. This section covers practical patterns for production applications.\n\n## Why Use Templates?\n\n- **Consistency**: Ensure similar tasks use similar prompts\n- **Maintainability**: Update prompts in one place\n- **Testability**: Easy to A/B test different prompt versions\n- **Scalability**: Share prompts across your team\n\n## Basic Template Pattern",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simple string formatting template\nSUMMARIZATION_TEMPLATE = \"\"\"Please summarize the following text in {num_sentences} sentences.\nFocus on the {focus_area}.\n\nText to summarize:\n{text}\n\nSummary:\"\"\"\n\ndef summarize_text(text: str, num_sentences: int = 3, focus_area: str = \"main points\"):\n    \"\"\"Summarize text using a template.\"\"\"\n    prompt = SUMMARIZATION_TEMPLATE.format(\n        num_sentences=num_sentences,\n        focus_area=focus_area,\n        text=text\n    )\n    \n    response = client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=500,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response.content[0].text\n\n# Test the template\nsample_text = \"\"\"\nArtificial intelligence is transforming software development through automated code generation,\nintelligent debugging tools, and predictive analytics. Machine learning models can now suggest\ncode completions, identify potential bugs before they occur, and even write entire functions\nbased on natural language descriptions. This technology is making developers more productive\nand helping them focus on higher-level design decisions.\n\"\"\"\n\nprint(\"ðŸ“ Template-based Summarization\")\nprint(\"=\" * 80 + \"\\n\")\n\nsummary = summarize_text(sample_text, num_sentences=2, focus_area=\"practical benefits\")\nprint(f\"Summary: {summary}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Class-based Template System",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PromptTemplate:\n    \"\"\"Reusable prompt template with variable substitution.\"\"\"\n    \n    def __init__(self, template: str, system_prompt: str = None):\n        self.template = template\n        self.system_prompt = system_prompt\n    \n    def format(self, **kwargs) -> str:\n        \"\"\"Format template with provided variables.\"\"\"\n        return self.template.format(**kwargs)\n    \n    def execute(self, model=\"claude-3-5-haiku-20241022\", max_tokens=500, **kwargs):\n        \"\"\"Format and execute the template.\"\"\"\n        prompt = self.format(**kwargs)\n        \n        response = client.messages.create(\n            model=model,\n            max_tokens=max_tokens,\n            system=self.system_prompt,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        return response.content[0].text\n\n# Create templates for different tasks\ncode_review_template = PromptTemplate(\n    template=\"\"\"Review the following {language} code and provide feedback on:\n1. Code quality and best practices\n2. Potential bugs or issues\n3. Suggestions for improvement\n\nCode:\n```{language}\n{code}\n```\n\nReview:\"\"\",\n    system_prompt=\"You are an experienced software engineer conducting a code review.\"\n)\n\ntranslation_template = PromptTemplate(\n    template=\"Translate the following text from {source_lang} to {target_lang}:\\n\\n{text}\",\n    system_prompt=\"You are a professional translator. Provide accurate, natural-sounding translations.\"\n)\n\n# Test code review template\nprint(\"ðŸ” Code Review Template Example\")\nprint(\"=\" * 80 + \"\\n\")\n\ncode_sample = \"\"\"\ndef calculate_avg(numbers):\n    total = 0\n    for num in numbers:\n        total = total + num\n    return total / len(numbers)\n\"\"\"\n\nreview = code_review_template.execute(\n    language=\"Python\",\n    code=code_sample,\n    max_tokens=400\n)\n\nprint(review)\n\nprint(\"\\n\" + \"=\" * 80 + \"\\n\")\n\n# Test translation template\nprint(\"ðŸŒ Translation Template Example\")\nprint(\"=\" * 80 + \"\\n\")\n\ntranslation = translation_template.execute(\n    source_lang=\"English\",\n    target_lang=\"Spanish\",\n    text=\"Hello, how are you today?\",\n    max_tokens=100\n)\n\nprint(f\"Translation: {translation}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Common Prompt Patterns\n\nHere are some battle-tested prompt patterns you can adapt:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Collection of reusable prompt patterns\nPROMPT_PATTERNS = {\n    \"extraction\": \"\"\"Extract the following information from the text:\n{fields}\n\nText:\n{text}\n\nOutput format:\n{format_spec}\"\"\",\n    \n    \"classification\": \"\"\"Classify the following {item_type} into one of these categories:\n{categories}\n\n{item_type}: {content}\n\nCategory:\"\"\",\n    \n    \"analysis\": \"\"\"Analyze the following {subject} and provide insights on:\n{analysis_points}\n\n{subject}:\n{content}\n\nAnalysis:\"\"\",\n    \n    \"generation\": \"\"\"Generate {count} {item_type} that meet these requirements:\n{requirements}\n\nFormat: {format_spec}\n\nOutput:\"\"\",\n    \n    \"comparison\": \"\"\"Compare {item_a} and {item_b} across these dimensions:\n{dimensions}\n\n{item_a}:\n{content_a}\n\n{item_b}:\n{content_b}\n\nComparison:\"\"\"\n}\n\n# Example: Using the extraction pattern\ndef extract_contact_info(text: str):\n    \"\"\"Extract contact information using a template pattern.\"\"\"\n    prompt = PROMPT_PATTERNS[\"extraction\"].format(\n        fields=\"- Name\\n- Email\\n- Phone number\",\n        text=text,\n        format_spec=\"One item per line in 'Field: Value' format\"\n    )\n    \n    response = client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=200,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response.content[0].text\n\n# Test extraction\ncontact_text = \"Please contact John Smith at john.smith@email.com or call (555) 123-4567 for more information.\"\n\nprint(\"ðŸ“‹ Extraction Pattern Example\")\nprint(\"=\" * 80 + \"\\n\")\nprint(extract_contact_info(contact_text))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# JSON Mode & Structured Outputs\n\nGetting structured data from Claude is essential for programmatic use. This section covers techniques for reliable JSON generation and parsing.\n\n## Why Structured Outputs?\n\n- **Programmatic processing**: Easy to parse and use in code\n- **Type safety**: Define expected structure\n- **Integration**: Works with APIs and databases\n- **Consistency**: Same format every time\n\n## Basic JSON Generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Request JSON output explicitly\nprint(\"ðŸ“Š Basic JSON Generation\")\nprint(\"=\" * 80 + \"\\n\")\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=500,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"\"\"Extract information about this product and return ONLY valid JSON:\n\nProduct: \"iPhone 15 Pro - 256GB, Titanium Blue, with A17 Pro chip and 48MP camera\"\n\nFormat:\n{\n  \"name\": \"product name\",\n  \"storage\": \"storage capacity\",\n  \"color\": \"color\",\n  \"processor\": \"processor\",\n  \"camera\": \"camera specs\"\n}\"\"\"\n    }]\n)\n\njson_text = response.content[0].text.strip()\nprint(f\"Raw response:\\n{json_text}\\n\")\n\n# Parse JSON\ntry:\n    data = json.loads(json_text)\n    print(\"âœ… Successfully parsed JSON:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError as e:\n    print(f\"âŒ JSON parsing error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Structured Data Extraction Helper",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\n\ndef extract_json_from_response(response_text: str) -> dict:\n    \"\"\"\n    Extract and parse JSON from Claude's response.\n    Handles cases where JSON is wrapped in markdown code blocks.\n    \"\"\"\n    # Try direct parsing first\n    try:\n        return json.loads(response_text.strip())\n    except json.JSONDecodeError:\n        pass\n    \n    # Try extracting from code block\n    json_match = re.search(r'```(?:json)?\\n(.+?)\\n```', response_text, re.DOTALL)\n    if json_match:\n        try:\n            return json.loads(json_match.group(1))\n        except json.JSONDecodeError:\n            pass\n    \n    # Try finding JSON object\n    json_match = re.search(r'\\{.+\\}', response_text, re.DOTALL)\n    if json_match:\n        try:\n            return json.loads(json_match.group(0))\n        except json.JSONDecodeError:\n            pass\n    \n    raise ValueError(\"No valid JSON found in response\")\n\ndef get_structured_output(prompt: str, schema_description: str = None) -> dict:\n    \"\"\"\n    Get structured JSON output from Claude.\n    \n    Args:\n        prompt: The task description\n        schema_description: Optional JSON schema description\n    \n    Returns:\n        Parsed JSON dictionary\n    \"\"\"\n    full_prompt = f\"\"\"{prompt}\n\n{f'Required JSON schema:\\n{schema_description}\\n' if schema_description else ''}\nRespond with ONLY valid JSON, no additional text or markdown formatting.\"\"\"\n    \n    response = client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=1000,\n        messages=[{\"role\": \"user\", \"content\": full_prompt}]\n    )\n    \n    response_text = response.content[0].text\n    return extract_json_from_response(response_text)\n\n# Test the helper\nprint(\"ðŸ”§ Structured Output Helper Example\")\nprint(\"=\" * 80 + \"\\n\")\n\nschema = \"\"\"{\n  \"title\": \"string\",\n  \"author\": \"string\",\n  \"year\": number,\n  \"genre\": \"string\",\n  \"summary\": \"string\"\n}\"\"\"\n\nresult = get_structured_output(\n    prompt=\"Extract book information: 'To Kill a Mockingbird by Harper Lee (1960) is a classic American novel about racial injustice.'\",\n    schema_description=schema\n)\n\nprint(\"âœ… Extracted structured data:\")\nprint(json.dumps(result, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Complex Structured Outputs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate complex nested JSON\nprint(\"ðŸ—ï¸ Complex Structured Output Example\")\nprint(\"=\" * 80 + \"\\n\")\n\nschema = \"\"\"{\n  \"company\": {\n    \"name\": \"string\",\n    \"founded\": number,\n    \"employees\": number\n  },\n  \"products\": [\n    {\n      \"name\": \"string\",\n      \"category\": \"string\",\n      \"price\": number\n    }\n  ],\n  \"headquarters\": {\n    \"city\": \"string\",\n    \"country\": \"string\"\n  }\n}\"\"\"\n\nresponse = client.messages.create(\n    model=\"claude-3-5-haiku-20241022\",\n    max_tokens=800,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": f\"\"\"Create a fictional tech company profile with this schema:\n\n{schema}\n\nInclude 3 products. Return ONLY the JSON object.\"\"\"\n    }]\n)\n\ntry:\n    company_data = extract_json_from_response(response.content[0].text)\n    print(\"âœ… Generated complex structured data:\")\n    print(json.dumps(company_data, indent=2))\n    \n    # Validate structure\n    assert \"company\" in company_data\n    assert \"products\" in company_data\n    assert len(company_data[\"products\"]) == 3\n    print(\"\\nâœ… Structure validation passed\")\n    \nexcept (json.JSONDecodeError, AssertionError, ValueError) as e:\n    print(f\"âŒ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Advanced Use Cases\n\nThis section demonstrates real-world applications combining multiple techniques we've learned.\n\n## Use Case 1: Content Moderation System",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ContentModerator:\n    \"\"\"\n    Advanced content moderation system with confidence scores.\n    Combines: Templates, JSON outputs, error handling\n    \"\"\"\n    \n    MODERATION_PROMPT = \"\"\"Analyze this content for policy violations:\n\nContent: {content}\n\nCheck for:\n- Spam or promotional content\n- Hate speech or harassment\n- Explicit or inappropriate material\n- Misinformation\n- Violence or threats\n\nReturn JSON:\n{{\n  \"is_safe\": boolean,\n  \"confidence\": number (0-1),\n  \"violations\": [\"list of violations if any\"],\n  \"reason\": \"brief explanation\",\n  \"recommended_action\": \"approve|review|reject\"\n}}\"\"\"\n    \n    def __init__(self):\n        self.moderation_count = 0\n    \n    def moderate(self, content: str) -> dict:\n        \"\"\"Moderate content and return structured decision.\"\"\"\n        self.moderation_count += 1\n        \n        try:\n            prompt = self.MODERATION_PROMPT.format(content=content)\n            \n            response = client.messages.create(\n                model=\"claude-3-5-haiku-20241022\",\n                max_tokens=500,\n                temperature=0.0,  # Deterministic for consistency\n                system=\"You are a content moderator. Be consistent and fair in your assessments.\",\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            result = extract_json_from_response(response.content[0].text)\n            result[\"moderation_id\"] = self.moderation_count\n            return result\n            \n        except Exception as e:\n            return {\n                \"is_safe\": False,\n                \"confidence\": 0.0,\n                \"violations\": [],\n                \"reason\": f\"Error during moderation: {str(e)}\",\n                \"recommended_action\": \"review\",\n                \"moderation_id\": self.moderation_count\n            }\n\n# Test the moderator\nmoderator = ContentModerator()\n\ntest_contents = [\n    \"Check out my new blog post about Python programming!\",\n    \"This product is amazing! Buy now at scamsite.com for 90% off!!!\",\n    \"I really enjoyed this movie, the acting was superb.\"\n]\n\nprint(\"ðŸ›¡ï¸ Content Moderation System\")\nprint(\"=\" * 80 + \"\\n\")\n\nfor i, content in enumerate(test_contents, 1):\n    print(f\"[{i}] Moderating: \\\"{content[:50]}...\\\"\")\n    result = moderator.moderate(content)\n    print(f\"    Decision: {result['recommended_action'].upper()}\")\n    print(f\"    Safe: {result['is_safe']} (confidence: {result['confidence']})\")\n    print(f\"    Reason: {result['reason']}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Use Case 2: Intelligent Document Processor",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class DocumentProcessor:\n    \"\"\"\n    Process documents with multiple operations.\n    Combines: Multi-turn conversations, streaming, cost tracking\n    \"\"\"\n    \n    def __init__(self):\n        self.conversation = ConversationManager(\n            system_prompt=\"You are a document analysis assistant.\",\n            model=\"claude-3-5-haiku-20241022\"\n        )\n    \n    def process_document(self, document_text: str):\n        \"\"\"Process a document with multiple analysis steps.\"\"\"\n        print(\"ðŸ“„ Document Processing Pipeline\")\n        print(\"=\" * 80 + \"\\n\")\n        \n        # Step 1: Summarization\n        print(\"Step 1: Generating summary...\")\n        summary = self.conversation.send_message(\n            f\"Summarize this document in 2-3 sentences:\\n\\n{document_text}\"\n        )\n        print(f\"âœ… Summary: {summary}\\n\")\n        \n        # Step 2: Key points (Claude remembers the document)\n        print(\"Step 2: Extracting key points...\")\n        key_points = self.conversation.send_message(\n            \"Extract the 3 most important points from this document.\"\n        )\n        print(f\"âœ… Key Points:\\n{key_points}\\n\")\n        \n        # Step 3: Action items (still remembers context)\n        print(\"Step 3: Identifying action items...\")\n        actions = self.conversation.send_message(\n            \"Are there any action items or tasks mentioned?\"\n        )\n        print(f\"âœ… Actions:\\n{actions}\\n\")\n        \n        # Show stats\n        stats = self.conversation.get_stats()\n        print(\"=\" * 80)\n        print(f\"ðŸ“Š Processing Stats:\")\n        print(f\"   Conversation turns: {stats['turns']}\")\n        print(f\"   Total tokens: {stats['total_tokens']:,}\")\n        \n        return {\n            \"summary\": summary,\n            \"key_points\": key_points,\n            \"actions\": actions,\n            \"stats\": stats\n        }\n\n# Test document processor\nprocessor = DocumentProcessor()\n\nsample_document = \"\"\"\nProject Update: Q4 2024\n\nThe development team has completed the new user authentication system ahead of schedule.\nWe need to conduct security testing before the December 15th release. The QA team should \nprioritize testing the password reset flow and two-factor authentication. Marketing has \nrequested updated documentation by December 1st for the product launch.\n\nBudget: The project is 15% under budget, saving $50,000. Remaining funds could be allocated\nto the mobile app development starting in January.\n\"\"\"\n\nresult = processor.process_document(sample_document)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Use Case 3: Code Review Assistant",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def code_review_assistant(code: str, language: str = \"python\") -> dict:\n    \"\"\"\n    Comprehensive code review with structured feedback.\n    Combines: JSON outputs, templates, best practices\n    \"\"\"\n    \n    prompt = f\"\"\"Review this {language} code and provide structured feedback.\n\nCode:\n```{language}\n{code}\n```\n\nReturn JSON with:\n{{\n  \"overall_quality\": \"poor|fair|good|excellent\",\n  \"score\": number (1-10),\n  \"strengths\": [\"list of good aspects\"],\n  \"issues\": [\n    {{\n      \"severity\": \"low|medium|high\",\n      \"category\": \"bug|style|performance|security\",\n      \"description\": \"issue description\",\n      \"suggestion\": \"how to fix\"\n    }}\n  ],\n  \"recommendations\": [\"list of improvements\"]\n}}\"\"\"\n    \n    response = client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=1500,\n        temperature=0.2,\n        system=\"You are an experienced software engineer. Provide constructive, specific feedback.\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return extract_json_from_response(response.content[0].text)\n\n# Test code review\ntest_code = \"\"\"\ndef process_users(users):\n    results = []\n    for user in users:\n        if user['age'] > 18:\n            results.append(user['name'].upper())\n    return results\n\"\"\"\n\nprint(\"ðŸ‘¨â€ðŸ’» Code Review Assistant\")\nprint(\"=\" * 80 + \"\\n\")\n\nreview = code_review_assistant(test_code)\n\nprint(f\"ðŸ“Š Overall Quality: {review['overall_quality'].upper()} (Score: {review['score']}/10)\\n\")\n\nprint(\"âœ… Strengths:\")\nfor strength in review['strengths']:\n    print(f\"   â€¢ {strength}\")\n\nprint(f\"\\nâš ï¸ Issues Found: {len(review['issues'])}\")\nfor i, issue in enumerate(review['issues'], 1):\n    print(f\"\\n   [{i}] {issue['severity'].upper()} - {issue['category']}\")\n    print(f\"       {issue['description']}\")\n    print(f\"       ðŸ’¡ Suggestion: {issue['suggestion']}\")\n\nprint(f\"\\nðŸ“ Recommendations:\")\nfor rec in review['recommendations']:\n    print(f\"   â€¢ {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Use Case 4: Smart Customer Support Bot",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SupportBot:\n    \"\"\"\n    Intelligent customer support with conversation management and analytics.\n    Combines: Multi-turn conversations, classification, streaming\n    \"\"\"\n    \n    def __init__(self):\n        self.sessions = {}\n    \n    def create_session(self, customer_id: str):\n        \"\"\"Create a new support session.\"\"\"\n        self.sessions[customer_id] = {\n            \"conversation\": ConversationManager(\n                system_prompt=\"\"\"You are a friendly customer support agent.\n                - Be empathetic and patient\n                - Ask clarifying questions\n                - Provide step-by-step solutions\n                - Escalate complex issues when needed\"\"\",\n                model=\"claude-3-5-haiku-20241022\"\n            ),\n            \"issue_category\": None,\n            \"resolved\": False\n        }\n    \n    def classify_issue(self, message: str) -> str:\n        \"\"\"Classify the customer's issue.\"\"\"\n        response = client.messages.create(\n            model=\"claude-3-5-haiku-20241022\",\n            max_tokens=50,\n            temperature=0.0,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"\"\"Classify this support request into ONE category:\n                \nCategories: technical|billing|account|shipping|product\n\nMessage: \"{message}\"\n\nCategory:\"\"\"\n            }]\n        )\n        \n        return response.content[0].text.strip().lower()\n    \n    def handle_message(self, customer_id: str, message: str) -> dict:\n        \"\"\"Handle a customer message.\"\"\"\n        if customer_id not in self.sessions:\n            self.create_session(customer_id)\n        \n        session = self.sessions[customer_id]\n        \n        # Classify first message\n        if session[\"conversation\"].get_stats()[\"turns\"] == 0:\n            session[\"issue_category\"] = self.classify_issue(message)\n        \n        # Get response\n        response = session[\"conversation\"].send_message(message)\n        \n        return {\n            \"response\": response,\n            \"category\": session[\"issue_category\"],\n            \"turn\": session[\"conversation\"].get_stats()[\"turns\"]\n        }\n\n# Test the support bot\nbot = SupportBot()\ncustomer_id = \"CUST001\"\n\nprint(\"ðŸŽ§ Smart Customer Support Bot\")\nprint(\"=\" * 80 + \"\\n\")\n\nmessages = [\n    \"Hi, I can't log into my account. It says my password is incorrect.\",\n    \"Yes, I tried resetting it but I didn't receive the email.\",\n    \"I'll check my spam folder. Thanks!\"\n]\n\nfor msg in messages:\n    print(f\"ðŸ‘¤ Customer: {msg}\")\n    result = bot.handle_message(customer_id, msg)\n    print(f\"ðŸ¤– Support [{result['category']}]: {result['response']}\\n\")\n\nstats = bot.sessions[customer_id][\"conversation\"].get_stats()\nprint(\"=\" * 80)\nprint(f\"ðŸ“Š Session Stats: {stats['turns']} turns, {stats['total_tokens']} tokens\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key Takeaways from Advanced Use Cases\n\n### 1. **Content Moderation System**\n- **Techniques**: Templates, JSON outputs, error handling, deterministic temperature\n- **Key Features**: Confidence scores, structured decisions, fallback handling\n- **Production Tips**: Use temperature=0.0 for consistency, always handle exceptions\n\n### 2. **Document Processor**\n- **Techniques**: Multi-turn conversations, context preservation, token tracking\n- **Key Features**: Sequential analysis, conversation memory, statistics\n- **Production Tips**: Break complex tasks into steps, track cumulative costs\n\n### 3. **Code Review Assistant**\n- **Techniques**: Structured outputs, severity categorization, actionable feedback\n- **Key Features**: Multi-level analysis, specific suggestions, quality scores\n- **Production Tips**: Clear schema definitions, specific feedback categories\n\n### 4. **Customer Support Bot**\n- **Techniques**: Session management, classification, multi-turn dialogue\n- **Key Features**: Issue categorization, conversation tracking, per-customer sessions\n- **Production Tips**: Classify early, maintain conversation context, track metrics\n\n## Common Patterns Across Use Cases\n\nâœ… **Error Handling**: Every system includes robust try-except blocks  \nâœ… **Structured Outputs**: JSON for programmatic processing  \nâœ… **Cost Tracking**: Monitor token usage for budgeting  \nâœ… **Temperature Control**: Use 0.0-0.3 for consistent, deterministic tasks  \nâœ… **System Prompts**: Define clear roles and expectations  \nâœ… **Conversation Management**: Maintain context for multi-turn interactions\n\n## Building Your Own Applications\n\nWhen building production applications with Claude:\n\n1. **Start Simple**: Begin with basic API calls, add complexity gradually\n2. **Test Extensively**: Edge cases, error conditions, various inputs\n3. **Monitor Performance**: Track latency, costs, success rates\n4. **Iterate**: Refine prompts based on real-world usage\n5. **Handle Failures**: Always have fallback logic\n6. **Track Costs**: Monitor token usage to stay within budget\n7. **Optimize**: Use Haiku for simple tasks, upgrade when needed\n\n---\n\n**Congratulations!** ðŸŽ‰ You now have a comprehensive understanding of the Anthropic Claude API and production-ready patterns for building AI applications.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Best Practices for Structured Outputs\n\n### âœ… DO:\n- **Be explicit**: \"Respond with ONLY valid JSON\"\n- **Provide schemas**: Show the exact structure you want\n- **Give examples**: Include sample JSON in your prompt\n- **Validate outputs**: Always parse and validate the JSON\n- **Handle errors gracefully**: Use try-except for parsing\n- **Use stop sequences**: Can help ensure clean JSON generation\n\n### âŒ DON'T:\n- **Assume perfect JSON**: Always validate\n- **Use complex nested structures**: Keep it simple when possible\n- **Forget to handle code blocks**: Claude sometimes wraps JSON in markdown\n- **Skip schema definition**: Be explicit about structure\n\n## Tips for Reliable JSON\n\n1. **Explicit Instructions**\n```python\nprompt = \"\"\"Return your response as valid JSON only.\nNo additional text, no markdown, just the JSON object.\"\"\"\n```\n\n2. **Few-shot Examples**\n```python\nprompt = \"\"\"Example format:\n{\"name\": \"John\", \"age\": 30}\n\nNow extract: \"Jane is 25 years old\"\n\"\"\"\n```\n\n3. **Use Stop Sequences**\n```python\nstop_sequences=[\"}\\n\\n\", \"```\"]  # Stop after JSON ends\n```\n\n4. **Validate with Schemas** (using libraries like `pydantic` or `jsonschema`)\n\n---\n\n**Summary**: Structured outputs enable programmatic use of Claude's responses. Use clear schemas, validate outputs, and handle parsing errors gracefully.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Template Best Practices\n\n### âœ… DO:\n- **Use clear variable names** (`{user_name}` not `{x}`)\n- **Include examples in templates** when format matters\n- **Version your templates** for tracking changes\n- **Test templates with edge cases**\n- **Document template variables** and their expected formats\n- **Use XML tags for structure**: `<context>`, `<task>`, `<format>`\n\n### âŒ DON'T:\n- **Hardcode values** that might change\n- **Make templates too rigid** (allow flexibility)\n- **Forget to sanitize user input** in templates\n- **Create overly complex templates** (split into smaller ones)\n\n## Template Organization\n\n```python\n# Organize templates by domain\nclass EmailTemplates:\n    CUSTOMER_SUPPORT = PromptTemplate(...)\n    MARKETING = PromptTemplate(...)\n    INTERNAL = PromptTemplate(...)\n\nclass ContentTemplates:\n    BLOG_POST = PromptTemplate(...)\n    SOCIAL_MEDIA = PromptTemplate(...)\n    DOCUMENTATION = PromptTemplate(...)\n```\n\n---\n\n**Summary**: Templates make your prompts reusable, testable, and maintainable. Invest time in building a good template library for your common use cases.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## When to Use Streaming vs Non-Streaming\n\n| Use Streaming | Use Non-Streaming |\n|---------------|-------------------|\n| Interactive chat interfaces | Batch processing |\n| Long-form content generation | Short responses |\n| Better user experience needed | Processing responses programmatically |\n| Real-time feedback desired | Results needed all at once |\n| Web/mobile applications | Background jobs |\n\n## Streaming Best Practices\n\n### âœ… DO:\n- **Use streaming for user-facing applications** (chat, content generation)\n- **Handle stream interruptions gracefully** (network issues, cancellations)\n- **Display loading indicators** before stream starts\n- **Buffer text for smooth rendering** (avoid flickering)\n- **Track tokens with `get_final_message()`**\n\n### âŒ DON'T:\n- **Use streaming for simple batch processing** (adds complexity)\n- **Forget error handling** (streams can fail mid-generation)\n- **Block the UI thread** during streaming (use async if needed)\n- **Parse incomplete JSON/structured data** during streaming\n\n## Streaming Event Types\n\n```python\n# Available event types:\n- message_start: Message generation begins\n- content_block_start: Content block starts\n- content_block_delta: New content chunk (the actual text)\n- content_block_stop: Content block ends\n- message_delta: Message metadata update\n- message_stop: Message complete\n```\n\n## Error Handling in Streaming\n\n```python\ntry:\n    with client.messages.stream(...) as stream:\n        for text in stream.text_stream:\n            print(text, end=\"\", flush=True)\nexcept APIError as e:\n    print(f\"\\nâŒ Stream error: {e}\")\nexcept KeyboardInterrupt:\n    print(\"\\nâš ï¸ Stream cancelled by user\")\n```\n\n---\n\n**Summary**: Streaming provides a superior user experience for interactive applications, allowing users to see results immediately rather than waiting for complete responses.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Stop Sequence Best Practices\n\n### âœ… DO:\n- **Use distinctive markers** (e.g., \"###\", \"END_OUTPUT\", \"---\")\n- **Combine with prompts** that mention the stop sequence\n- **Use multiple stop sequences** for flexibility\n- **Test your stop sequences** to ensure they work as expected\n- **Use them for structured outputs** (JSON, CSV, etc.)\n\n### âŒ DON'T:\n- **Use common words** as stop sequences (they might appear naturally)\n- **Make them too complex** (simple markers work best)\n- **Rely solely on stop sequences** for control (combine with good prompts)\n- **Forget to handle** cases where stop sequence isn't triggered\n\n## Stop Reasons\n\nClaude's `stop_reason` field tells you why generation stopped:\n- **`end_turn`**: Natural completion\n- **`max_tokens`**: Hit the max_tokens limit\n- **`stop_sequence`**: Hit one of your stop sequences\n\n```python\nif response.stop_reason == \"stop_sequence\":\n    print(f\"Stopped at: {response.stop_sequence}\")\nelif response.stop_reason == \"max_tokens\":\n    print(\"Warning: Output truncated due to max_tokens\")\n```\n\n---\n\n**Summary**: Stop sequences give you precise control over Claude's output boundaries, making them invaluable for structured data extraction and preventing unwanted generation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Managing Long Conversations\n\nAs conversations grow, token usage increases. Here are strategies for managing long conversations:\n\n### 1. **Token Limits**\n- Claude models have 200K token context windows\n- Input + output must fit within this limit\n- Monitor conversation length\n\n### 2. **Summarization Strategy**\n```python\ndef summarize_conversation(conversation):\n    \"\"\"Summarize old messages to save tokens.\"\"\"\n    # Keep recent messages, summarize old ones\n    if len(conversation) > 10:\n        old_messages = conversation[:-6]  # Keep last 6 messages\n        summary_prompt = \"Summarize this conversation: \" + str(old_messages)\n        summary = client.messages.create(...)\n        # Replace old messages with summary\n```\n\n### 3. **Sliding Window**\n```python\ndef maintain_window(messages, max_messages=20):\n    \"\"\"Keep only recent messages.\"\"\"\n    if len(messages) > max_messages:\n        return messages[-max_messages:]\n    return messages\n```\n\n### 4. **Context Pruning**\n- Remove less important messages\n- Keep system prompt and recent context\n- Preserve key information\n\n---\n\n**Best Practices for Multi-turn Conversations:**\n- âœ… Always pass the full conversation history\n- âœ… Track token usage to avoid limits\n- âœ… Implement conversation reset functionality\n- âœ… Use system prompts for consistent behavior\n- âœ… Consider summarization for very long conversations\n- âŒ Don't exceed context window limits\n- âŒ Don't lose important context from early in the conversation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Prompt Engineering Best Practices Checklist\n\nUse this comprehensive checklist when building applications with Claude to ensure you're following best practices.\n\n## ðŸŽ¯ 1. Model Selection\n\n- [ ] **Choose the right model for your task**\n  - Use **Haiku** for simple, high-volume tasks (fastest, cheapest)\n  - Use **Sonnet** for balanced performance (general purpose)\n  - Use **Opus** for complex reasoning and analysis (most capable)\n\n- [ ] **Consider context window requirements**\n  - All models support 200K tokens\n  - Plan for input + output tokens within limits\n\n- [ ] **Test with cost-effective models first**\n  - Start with Haiku for prototyping\n  - Upgrade only if quality is insufficient\n\n## ðŸ“ 2. Prompt Design\n\n- [ ] **Write clear, specific prompts**\n  - Be explicit about what you want\n  - Provide context and constraints\n  - Use concrete examples when possible\n\n- [ ] **Use system prompts for role/behavior**\n  - Define Claude's role or persona\n  - Set high-level guidelines\n  - Keep them concise (they count toward tokens)\n\n- [ ] **Leverage few-shot learning**\n  - Provide 2-5 examples for consistent formatting\n  - Show Claude exactly how you want outputs\n  - Use message history to demonstrate patterns\n\n- [ ] **Structure complex prompts**\n  - Break tasks into steps\n  - Use XML tags for clarity: `<context>`, `<task>`, `<format>`\n  - Number steps if order matters\n\n## âš™ï¸ 3. Parameter Configuration\n\n- [ ] **Set appropriate temperature**\n  - **0.0-0.3**: Factual, deterministic tasks\n  - **0.4-0.7**: Balanced tasks\n  - **0.8-1.0**: Creative tasks\n\n- [ ] **Configure max_tokens**\n  - Estimate based on expected output length\n  - Add buffer for safety (20-30% extra)\n  - Monitor actual usage to optimize\n\n- [ ] **Consider top_p and top_k**\n  - Use top_p for controlled randomness\n  - Combine with temperature for fine-tuning\n\n## ðŸ›¡ï¸ 4. Error Handling & Reliability\n\n- [ ] **Implement try-except blocks**\n  - Catch specific error types\n  - Handle each error appropriately\n  - Log errors with context\n\n- [ ] **Add retry logic with exponential backoff**\n  - Start with 1s delay, double each time\n  - Retry 3-5 times for transient errors\n  - Don't retry auth/bad request errors\n\n- [ ] **Implement rate limiting**\n  - Add delays between batch requests (0.3-1s)\n  - Track usage with response.usage\n  - Monitor for rate limit errors\n\n- [ ] **Handle timeouts gracefully**\n  - Set reasonable timeout values\n  - Provide fallback responses\n  - Inform users of delays\n\n## ðŸ’° 5. Cost Management\n\n- [ ] **Track token usage**\n  - Monitor input and output tokens\n  - Use calculate_cost() for estimates\n  - Log usage for analysis\n\n- [ ] **Optimize prompts for cost**\n  - Remove unnecessary verbosity\n  - Reuse system prompts\n  - Batch related requests\n\n- [ ] **Use appropriate models**\n  - Don't use Opus for simple tasks\n  - Haiku is 12x cheaper than Opus\n  - Cost scales with tokens and model tier\n\n## ðŸ”’ 6. Security & Privacy\n\n- [ ] **Protect API keys**\n  - Use environment variables\n  - Never commit keys to version control\n  - Rotate keys periodically\n\n- [ ] **Sanitize user inputs**\n  - Validate before sending to API\n  - Remove sensitive information\n  - Implement input length limits\n\n- [ ] **Handle sensitive data appropriately**\n  - Review Anthropic's data handling policies\n  - Don't send PII unnecessarily\n  - Consider data retention policies\n\n## ðŸ§ª 7. Testing & Validation\n\n- [ ] **Test with diverse inputs**\n  - Edge cases and unusual inputs\n  - Different languages if applicable\n  - Various prompt phrasings\n\n- [ ] **Validate outputs**\n  - Check response format\n  - Verify content accuracy\n  - Test error scenarios\n\n- [ ] **Monitor in production**\n  - Track success rates\n  - Monitor latency\n  - Analyze token usage patterns\n\n## ðŸ“Š 8. Production Readiness\n\n- [ ] **Implement logging**\n  - Log all API calls with metadata\n  - Include timestamps and user context\n  - Track errors and retries\n\n- [ ] **Add monitoring**\n  - Set up alerts for failures\n  - Monitor rate limits\n  - Track cost trends\n\n- [ ] **Plan for scaling**\n  - Implement request queuing\n  - Consider caching strategies\n  - Load test before launch\n\n- [ ] **Document your implementation**\n  - Document prompt templates\n  - Explain model selection rationale\n  - Maintain error handling guides\n\n## ðŸŽ¨ 9. User Experience\n\n- [ ] **Provide loading indicators**\n  - Show progress for long requests\n  - Set user expectations\n  - Handle interruptions gracefully\n\n- [ ] **Stream responses when possible**\n  - For better perceived performance\n  - Allow users to see progress\n  - Enable early cancellation\n\n- [ ] **Handle errors user-friendly**\n  - Show helpful error messages\n  - Suggest actions to resolve issues\n  - Provide fallback options\n\n## ðŸ”„ 10. Maintenance & Iteration\n\n- [ ] **Version your prompts**\n  - Track prompt changes\n  - A/B test improvements\n  - Maintain prompt history\n\n- [ ] **Stay updated on model changes**\n  - Review Anthropic's changelogs\n  - Test new models when released\n  - Update model IDs as needed\n\n- [ ] **Gather feedback**\n  - Collect user feedback\n  - Monitor output quality\n  - Iterate on prompts\n\n---\n\n## Quick Reference Summary\n\n| Aspect | Recommendation |\n|--------|----------------|\n| **Model** | Haiku for simple, Sonnet for balanced, Opus for complex |\n| **Temperature** | 0.0-0.3 factual, 0.4-0.7 balanced, 0.8-1.0 creative |\n| **Max Tokens** | Estimate + 20-30% buffer |\n| **Retries** | 3-5 attempts with exponential backoff |\n| **Rate Limiting** | 0.3-1s delay between requests |\n| **Cost** | Haiku: $0.25/$1.25, Sonnet: $3/$15, Opus: $15/$75 per 1M tokens |\n\nâœ… **Remember**: Start simple, test thoroughly, optimize iteratively!\n\n---\n\n## Additional Resources\n\n- **Anthropic Documentation**: https://docs.anthropic.com/\n- **Prompt Engineering Guide**: https://docs.anthropic.com/claude/docs/prompt-engineering\n- **API Reference**: https://docs.anthropic.com/claude/reference/\n- **Pricing**: https://www.anthropic.com/api",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Best Practices for Error Handling\n\n### âœ… DO:\n- **Always wrap API calls in try-except blocks**\n- **Use exponential backoff for rate limits** (start with 1s, double each time)\n- **Implement retry logic for transient errors** (rate limits, connection issues, 5xx errors)\n- **Log errors for debugging** (include timestamps and error details)\n- **Set reasonable timeouts** to prevent hanging requests\n- **Add delays between batch requests** to avoid rate limits\n\n### âŒ DON'T:\n- **Don't retry authentication errors** (401) - fix your API key instead\n- **Don't retry bad request errors** (400) - fix your request parameters\n- **Don't retry indefinitely** - set a max retry limit (3-5 is typical)\n- **Don't ignore errors silently** - always log or handle them\n- **Don't spam retries immediately** - use exponential backoff\n\n## Rate Limit Guidelines\n\nAnthropic's rate limits vary by tier and model:\n- **Free tier**: Lower limits, suitable for development\n- **Build tier**: Higher limits for production apps\n- **Scale tier**: Custom limits for enterprise\n\n### Rate Limiting Tips:\n1. **Track your usage** with the `usage` field in responses\n2. **Implement request queuing** for high-volume applications\n3. **Use Haiku models** for higher throughput (they're faster and cheaper)\n4. **Distribute load** across time if possible\n5. **Monitor the `retry-after` header** when you hit rate limits\n\n---\n\n### Summary\n\nRobust error handling is essential for production applications:\n- Use try-except blocks to catch specific error types\n- Implement exponential backoff for retries\n- Don't retry non-transient errors (auth, bad requests)\n- Add delays between batch requests\n- Log errors for debugging\n\nWith proper error handling, your application will be resilient and provide a better user experience!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}