{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb13b85",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Amazon Bedrock\n",
    "\n",
    "Based on the AWS Samples: Amazon Bedrock Module 1 - Text Generation, we want to introduce how prompt engineering are done in Amazon Bedrock.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Welcome to this introduction to building conversational AI with Amazon Bedrock's Converse API! The primary goal of this chapter is to provide a comprehensive introduction to Amazon Bedrock APIs for generating text. While we'll explore various use cases like summarization and code generation, our focus is on understanding the API patterns.\n",
    "\n",
    "In this notebook, you will:\n",
    "* Learn the basics of the Amazon Bedrock Invoke API\n",
    "* Explore the more powerful Converse API and it's features like multi-turn conversation, streaming, or  function calling\n",
    "* Apply these APIs across various foundation models\n",
    "* Compare results across different state-of-the-art models\n",
    "\n",
    "Pre-requisites\n",
    "* This notebook requires permissions to access Amazon Bedrock.\n",
    "* Please ensure that you have added the libraries: Boto3 and AWS-CLI SDK and added the IAM credentials to access Amazon Bedrock with your AWS account. \n",
    "* Please make sure to enable the following models in your Amazon Bedrock Console (Go to Model Access and select the models you want to use from Amazon Bedrock) before running this notebook:\n",
    "\n",
    "Anthropic Claude 3.7 Sonnet\n",
    "Anthropic Claude 3.5 Sonnet\n",
    "Anthropic Claude 3.5 Haiku\n",
    "Amazon Nova Pro\n",
    "Amazon Nova Micro\n",
    "DeepSeek-R1\n",
    "Meta LLama 3.1 70B Instruct\n",
    "\n",
    "This notebook uses these models to demonstrate various capabilities of the Amazon Bedrock APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc95eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import display, Markdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2468ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock client\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e851b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model IDs that will be used in this module\n",
    "MODELS = {\n",
    "    \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",\n",
    "    \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",\n",
    "    \"DeepSeek-R1\": \"us.deepseek.r1-v1:0\",\n",
    "    \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdad1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to display model responses in a more readable format\n",
    "def display_response(response, model_name=None):\n",
    "    if model_name:\n",
    "        display(Markdown(f\"### Response from {model_name}\"))\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8845fb2",
   "metadata": {},
   "source": [
    "# Text Summarization with Foundation Models\n",
    "\n",
    "Let's start by exploring how to leverage Amazon Bedrock APIs for text summarization. We'll first use the basic Invoke API, then introduce the more powerful Converse API.\n",
    "\n",
    "As an example, let's take a paragraph about Amazon Bedrock from an AWS blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06eb5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize = \"\"\"\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "for text and images—including Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
    "today—through a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
    "customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
    "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6240300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt for summarization\n",
    "prompt = f\"\"\"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
    "<text>\n",
    "{text_to_summarize}\n",
    "</text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3217ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create request body for Claude 3.7 Sonnet\n",
    "claude_body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        }\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34c8b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Invoke Model API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Summary\n",
       "\n",
       "Amazon announced Amazon Bedrock, a new service that provides API access to foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon. Bedrock aims to democratize access to generative AI by offering an easy way for customers to build and scale applications. The service provides access to various text and image models, including Amazon's new Titan large language models (LLMs). As a serverless AWS managed service, Bedrock allows customers to find appropriate models, customize them with private data, and integrate them into applications using familiar AWS tools without managing infrastructure. It includes integrations with Amazon SageMaker features like Experiments and Pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send request to Claude 3.7 Sonnet\n",
    "try:\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        body=claude_body,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    \n",
    "    # Extract and display the response text\n",
    "    claude_summary = response_body[\"content\"][0][\"text\"]\n",
    "    display_response(claude_summary, \"Claude 3.7 Sonnet (Invoke Model API)\")\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "            \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "            \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "            \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa4164",
   "metadata": {},
   "source": [
    "# Text Summarization using the Converse API (Recommended Approach)\n",
    "While the Invoke Model API allows direct access to foundation models, it has several limitations:\n",
    "* it uses different request/response formats for each model family;\n",
    "there is no built-in support for multi-turn conversations;\n",
    "it requires custom handling for different model capabilities.\n",
    "The Converse API addresses these limitations by providing a unified interface. Let's explore it on our text summarization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2757ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a converse request with our summarization task\n",
    "converse_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.1,\n",
    "        \"topP\": 0.95,\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c996243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Converse API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service providing API access to foundation models (FMs) from various AI companies, including Amazon's own Titan LLMs. The service democratizes generative AI by offering a serverless experience where customers can easily find, customize, and deploy text and image models without managing infrastructure. Bedrock integrates with existing AWS tools, allowing users to test different models and manage FMs at scale."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call Claude 3.7 Sonnet with Converse API\n",
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=converse_request[\"messages\"],\n",
    "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response\n",
    "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
    "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd49ef",
   "metadata": {},
   "source": [
    "# Overview of the Converse API\n",
    "\n",
    "Now, that we have used the Converse API, let's take some time to take a closer look. To use the Converse API, you use the Converse or ConverseStream (for streaming responses) operations to send messages to a model. While, it is possible to use the existing base inference operations (```InvokeModel``` or ```InvokeModelWithResponseStream```) for conversation applications as well, we recommend using the Converse API as it provides consistent API, that works with all Amazon Bedrock models that support messages. This means you can write code once and use it with different models. \n",
    "\n",
    "Should a model have unique inference parameters, the Converse API also allows you to pass those unique parameters in a model specific structure. You can use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an Amazon Bedrock model. For example, you can create a chat bot that maintains a conversation over many turns and uses a persona or tone customization that is unique to your needs, such as a helpful technical support assistant. The Converse API also supports other Bedrock capabilites, like tool use and guardrails.\n",
    "\n",
    "Let's break down its key components (you can also review the documentation for a full list of parameters):\n",
    "\n",
    "```\n",
    "{\n",
    "  \"modelId\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"Your prompt or message here\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": \"You are a helpful AI assistant.\"\n",
    "    }\n",
    "  ],\n",
    "  \"inferenceConfig\": {\n",
    "    \"temperature\": 0.7,\n",
    "    \"topP\": 0.9,\n",
    "    \"maxTokens\": 2000,\n",
    "    \"stopSequences\": []\n",
    "  },\n",
    "  \"toolConfig\": {\n",
    "    \"tools\": [],\n",
    "    \"toolChoice\": {\n",
    "      \"auto\": {}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bef1e1",
   "metadata": {},
   "source": [
    "# Easily switch between models\n",
    "\n",
    "One of the biggest advantages of the Converse API is the ability to easily switch between models using the exact same request format. Let's compare summaries across different foundation models by looping over the model dictionary we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0d32a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully called Claude 3.7 Sonnet (took 2.94 seconds)\n",
      "✅ Successfully called Claude 3.5 Sonnet (took 3.01 seconds)\n",
      "✅ Successfully called Claude 3.5 Haiku (took 2.46 seconds)\n",
      "✅ Successfully called Amazon Nova Pro (took 1.64 seconds)\n",
      "✅ Successfully called Amazon Nova Micro (took 0.74 seconds)\n",
      "✅ Successfully called DeepSeek-R1 (took 2.55 seconds)\n",
      "✅ Successfully called Meta Llama 3.1 70B Instruct (took 4.15 seconds)\n"
     ]
    }
   ],
   "source": [
    "import time  # Add this import at the top\n",
    "\n",
    "# Define model IDs that will be used in this module\n",
    "MODELS = {\n",
    "    \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",\n",
    "    \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",\n",
    "    \"DeepSeek-R1\": \"us.deepseek.r1-v1:0\",\n",
    "    \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "}\n",
    "\n",
    "# call different models with the same converse request\n",
    "results = {}    \n",
    "for model_name, model_id in MODELS.items(): # looping over all models defined above\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=converse_request[\"messages\"],\n",
    "                inferenceConfig=converse_request[\"inferenceConfig\"] if \"inferenceConfig\" in converse_request else None\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Extract the model's response using the correct structure\n",
    "            model_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            response_time = round(end_time - start_time, 2)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                \"response\": model_response,\n",
    "                \"time\": response_time\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Successfully called {model_name} (took {response_time} seconds)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calling {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"time\": None\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7960bd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Claude 3.7 Sonnet (took 2.94 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service providing API access to foundation models (FMs) from various AI companies, including Amazon's own Titan LLMs. The service enables customers to easily build and scale generative AI applications by finding suitable models, customizing them with private data, and integrating them using familiar AWS tools without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude 3.5 Sonnet (took 3.01 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon has announced Bedrock, a new service that provides API access to foundation models (FMs) from various AI companies and Amazon itself. Bedrock aims to democratize access to generative AI by offering a user-friendly, serverless platform for building and scaling AI applications. The service allows customers to easily find, customize, and deploy FMs for text and image tasks using familiar AWS tools and infrastructure, without the need to manage complex systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude 3.5 Haiku (took 2.46 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to foundation models from various AI companies through an API, enabling customers to build and scale generative AI applications. Bedrock offers a serverless experience that allows users to access, customize, and deploy foundation models privately and securely without managing infrastructure. The service includes models from AI21 Labs, Anthropic, Stability AI, and Amazon, including two new Amazon Titan large language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Amazon Nova Pro (took 1.64 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to Foundation Models (FMs) from various providers like AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling customers to build and scale generative AI applications. Bedrock offers a range of powerful text and image FMs, including Amazon's new Titan FMs, through a secure, scalable AWS managed service, allowing users to customize, integrate, and deploy models seamlessly using familiar AWS tools without infrastructure management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Amazon Nova Micro (took 0.74 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to generative AI models from AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling developers to quickly build and scale AI-based applications without managing infrastructure. Bedrock offers scalable, secure access to a range of powerful models for text and images, including Amazon's new Titan models, and integrates seamlessly with AWS tools like SageMaker."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### DeepSeek-R1 (took 2.55 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Amazon Bedrock is a new AWS service providing API access to powerful foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon, including Amazon's new Titan text models. It enables developers to build and scale generative AI applications effortlessly with a serverless, no-infrastructure platform, allowing customization using private data and integration with familiar AWS tools like SageMaker for testing and deployment. Bedrock democratizes access to advanced AI by combining scalable, secure managed services with streamlined model selection and customization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Meta Llama 3.1 70B Instruct (took 4.15 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here is a 3-sentence summary of the text:\n",
       "\n",
       "Amazon Web Services (AWS) has launched Amazon Bedrock, a new service that provides access to foundation models (FMs) from leading AI labs via an API. Bedrock makes it easy for customers to build and scale generative AI-based applications using FMs, with a scalable, reliable, and secure managed service. The service offers a serverless experience, allowing customers to easily find, customize, and deploy FMs into their applications without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display results in a formatted way\n",
    "for model_name, result in results.items():\n",
    "    if \"Error\" not in result[\"response\"]:\n",
    "        display(Markdown(f\"### {model_name} (took {result['time']} seconds)\"))\n",
    "        display(Markdown(result[\"response\"]))\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72572a",
   "metadata": {},
   "source": [
    "# Cross-Regional Inference in Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock offers Cross-Regional Inference which automatically selects the optimal AWS Region within your geography to process your inference requests. Cross-Regional Inference offers higher throughput limits (up to 2x allocated quotas) and seamlessly manages traffic bursts by dynamically routing requests across multiple AWS regions, enhancing application resilience during peak demand periods without additional routing or data transfer costs. Customers can control where their inference data flows by selecting from a pre-defined set of regions, helping them comply with applicable data residency requirements and sovereignty laws. Moreover, this capability prioritizes the connected Bedrock API source region when possible, helping to minimize latency and improve responsiveness. As a result, customers can enhance their applications' reliability, performance, and efficiency. Please review the list of supported regions and models for inference profiles.\n",
    "\n",
    "To use Cross-Regional Inference, you simply need to specify a cross-region inference profile as the modelId when making a request. Cross-region inference profiles are identified by including a region prefix (e.g., us. or eu.) before the model name.\n",
    "\n",
    "For example:\n",
    "```\n",
    "{\n",
    "    \"Amazon Nova Pro\": \"amazon.nova-pro-v1:0\",  # Regular model ID\n",
    "    \"Amazon Nova Pro (CRIS)\": \"us.amazon.nova-pro-v1:0\"  # Cross-regional model ID\n",
    "}\n",
    "```\n",
    "Let's see how easy it is to use Cross Region Inference by invoking the Claude 3.5 Sonnet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd180b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard response: Amazon has announced Bedrock, a new service providing API access to foundation models (FMs) from various AI companies and Amazon itself. Bedrock aims to democratize access to generative AI by offering a user-friendly, serverless platform for building and scaling AI applications. It allows customers to easily select, customize, and deploy FMs for text and image tasks using familiar AWS tools and infrastructure.\n",
      "Cross-region response: Amazon has announced Bedrock, a new service that provides easy access to various foundation models (FMs) from different AI companies through an API. This service aims to democratize generative AI by allowing developers to build and scale applications using these models, including Amazon's own newly announced Titan FMs. Bedrock offers a serverless experience, enabling customers to select, customize, and integrate FMs into their applications using familiar AWS tools and without managing infrastructure.\n"
     ]
    }
   ],
   "source": [
    "# Regular model invocation (standard region)\n",
    "standard_response = bedrock.converse(\n",
    "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Standard model ID\n",
    "    messages=converse_request[\"messages\"]\n",
    ")\n",
    "\n",
    "# Cross-region inference (note the \"us.\" prefix)\n",
    "cris_response = bedrock.converse(\n",
    "    modelId=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Cross-region model ID with regional prefix\n",
    "    messages=converse_request[\"messages\"]\n",
    ")\n",
    "\n",
    "# Print responses\n",
    "print(\"Standard response:\", standard_response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "print(\"Cross-region response:\", cris_response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250aa41c",
   "metadata": {},
   "source": [
    "# Multi-turn Conversations\n",
    "\n",
    "The Converse API makes multi-turn conversations simple. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61367049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Multi-turn conversation)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is AWS's new serverless service that provides API access to foundation models from multiple providers, allowing customers to easily build, customize, and deploy generative AI applications without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a multi-turn conversation with Converse API\n",
    "multi_turn_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": results[\"Claude 3.7 Sonnet\"][\"response\"]}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=multi_turn_messages,\n",
    "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response using the correct structure\n",
    "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(follow_up_response, \"Claude 3.7 Sonnet (Multi-turn conversation)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bd300",
   "metadata": {},
   "source": [
    "# Streaming Responses with ConverseStream API\n",
    "\n",
    "For longer generations, you might want to receive the content as it's being generated. The ConverseStream API supports streaming, which allows you to process the response incrementally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b383e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of streaming with Converse API\n",
    "def stream_converse(model_id, messages, inference_config=None):\n",
    "    if inference_config is None:\n",
    "        inference_config = {}\n",
    "    \n",
    "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        response_stream = response.get('stream')\n",
    "        if response_stream:\n",
    "            for event in response_stream:\n",
    "\n",
    "                if 'messageStart' in event:\n",
    "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "                if 'messageStop' in event:\n",
    "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "                if 'metadata' in event:\n",
    "                    metadata = event['metadata']\n",
    "                    if 'usage' in metadata:\n",
    "                        print(\"\\nToken usage\")\n",
    "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                        print(\n",
    "                            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                    if 'metrics' in event['metadata']:\n",
    "                        print(\n",
    "                            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "                \n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "        return full_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3c9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try streaming a longer summary\n",
    "streaming_request = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
    "                \n",
    "                {text_to_summarize}\n",
    "                \n",
    "                Make your summary comprehensive but clear.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177b5d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response (chunks will appear as they are received):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Role: assistant\n",
      "# Summary: Amazon Bedrock - Democratizing Access to Foundation Models\n",
      "\n",
      "## Key Points\n",
      "\n",
      "Amazon has launched Amazon Bedrock, a new service that provides API access to Foundation Models (FMs) from multiple providers including AI21 Labs, Anthropic, Stability AI, and Amazon itself. This launch responds directly to customer feedback requesting easier access to these powerful AI models.\n",
      "\n",
      "The service offers:\n",
      "\n",
      "1. **Diverse Model Access**: Users can access various powerful foundation models for text and image generation, including Amazon's newly announced Titan Large Language Models (LLMs).\n",
      "\n",
      "2. **Serverless Architecture**: Bedrock provides a fully managed, serverless experience, eliminating the need for customers to manage infrastructure.\n",
      "\n",
      "3. **Customization Capabilities**: Users can privately customize foundation models with their own data to better suit specific use cases.\n",
      "\n",
      "4. **Integration with AWS Ecosystem**: The service integrates with existing AWS tools like SageMaker ML features, including Experiments (for testing different models) and Pipelines (for managing FMs at scale).\n",
      "\n",
      "5. **Simplified Selection**: Bedrock helps customers identify the right model for their specific needs.\n",
      "\n",
      "## Implications\n",
      "\n",
      "1. **Democratization of AI**: By making powerful AI models accessible through APIs, AWS is lowering the barrier to entry for organizations wanting to implement generative AI.\n",
      "\n",
      "2. **Competitive Positioning**: AWS is positioning itself as an AI model aggregator, offering multiple providers through a single service rather than just promoting its own models.\n",
      "\n",
      "3. **Enterprise Adoption**: The integration with existing AWS services makes it easier for current AWS customers to adopt and scale AI capabilities.\n",
      "\n",
      "4. **Multi-model Strategy**: Rather than forcing customers to choose a single model, AWS is enabling comparison and selection of the best model for specific use cases.\n",
      "\n",
      "5. **Infrastructure Simplification**: The serverless approach removes a significant technical barrier to AI implementation, potentially accelerating enterprise adoption of generative AI technologies.\n",
      "\n",
      "This service represents AWS's strategic response to the rapidly evolving generative AI landscape, offering a flexible, scalable approach that leverages their cloud infrastructure strengths.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 273\n",
      ":Output tokens: 455\n",
      ":Total tokens: 728\n",
      "Latency: 10862 milliseconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Only run this when you're ready to see streaming output\n",
    "streamed_response = stream_converse(\n",
    "    MODELS[\"Claude 3.7 Sonnet\"], \n",
    "    streaming_request, \n",
    "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
