{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Agentic AI Flow: Namecard -> Deep Research -> CRM (Google Sheets)\n\n## Why this notebook exists\nThis lab demonstrates a practical agentic workflow from **unstructured input (a namecard image)** to **structured business output (a CRM-ready row)**.\n\nBy the end of this session, students should be able to explain:\n1. Why agentic systems are split into stages.\n2. How typed schemas reduce errors between stages.\n3. Where human-in-the-loop checks are necessary.\n4. How to connect AI outputs to real operational systems (Google Sheets as CRM).\n\n## End-to-end flow\n1. **Stage 1 (Perception):** OCR + field extraction from a namecard.\n2. **Stage 2 (Reasoning + Tool Use):** Research planner, search execution, dossier synthesis.\n3. **Stage 3 (Action):** Convert to CRM schema and write to Google Sheets.\n\n## Teaching framing\nAs you run each section, keep asking:\n- What is the input contract?\n- What is the output contract?\n- What can fail, and how do we recover safely?\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section A (10 min): Setup\n\n## What students learn here\n- How environment configuration affects agent reliability.\n- Why we front-load dependencies and secrets before coding logic.\n\n## What the next cells do\n1. Install required Python packages.\n2. Import libraries for OCR, research, schema validation, widgets, and Google Sheets.\n3. Initialize a shared `workflow_state` dictionary so each stage is inspectable.\n4. Configure resilience settings (retry counts, fallback mode, confidence thresholds).\n\n## Instructor guidance\n- Run the install cell first.\n- If imports fail, re-run the import cell after kernel restart.\n- Confirm these env vars before proceeding:\n  - `OPENAI_API_KEY`\n  - `GOOGLE_SERVICE_ACCOUNT_JSON`\n  - `GOOGLE_SHEET_ID`\n  - optional: `GOOGLE_WORKSHEET_NAME`\n\n## Expected output before moving on\nYou should see: `OpenAI client ready: True` (or `False` if key missing).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "%pip install -q openai pydantic pandas gspread google-auth ipywidgets python-dotenv"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: This cell imports libraries, installs missing packages, and sets global config/state.\n\nimport importlib\nimport subprocess\nimport sys\n\n\ndef ensure_package(import_name: str, pip_name: str | None = None):\n    \"\"\"Install a package at runtime if it is missing in the current kernel.\"\"\"\n    try:\n        importlib.import_module(import_name)\n    except ModuleNotFoundError:\n        pkg = pip_name or import_name\n        print(f\"Installing missing package: {pkg}\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n\n\nensure_package(\"pandas\")\nensure_package(\"ipywidgets\")\nensure_package(\"pydantic\")\nensure_package(\"gspread\")\nensure_package(\"google.oauth2\", \"google-auth\")\nensure_package(\"dotenv\", \"python-dotenv\")\nensure_package(\"openai\")\n\nimport base64\nimport concurrent.futures as futures\nimport json\nimport os\nimport re\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\nfrom urllib.parse import urlparse\n\nimport pandas as pd\nfrom IPython.display import Markdown, display\nfrom dotenv import load_dotenv\nimport ipywidgets as widgets\nfrom pydantic import BaseModel, Field\n\nimport gspread\nfrom google.oauth2.service_account import Credentials\nfrom openai import OpenAI\n\nload_dotenv(override=True)\n\n# Shared in-memory state so students can inspect each stage output.\nworkflow_state: Dict[str, Any] = {\n    \"namecard_extraction\": None,\n    \"research_dossier\": None,\n    \"crm_record\": None,\n    \"sheet_write_result\": None,\n    \"errors\": [],\n}\n\n# When True, classroom demo continues even if external APIs fail.\nALLOW_MOCK_FALLBACK = True\nEXTRACTION_CONFIDENCE_THRESHOLD = 0.72\nDEFAULT_WORKSHEET_NAME = os.getenv(\"GOOGLE_WORKSHEET_NAME\", \"Leads\")\nRETRY_ATTEMPTS = 3\nRETRY_BACKOFF_SECONDS = 1.5\n\nTRUSTED_DOMAINS = {\n    \"linkedin.com\",\n    \"bloomberg.com\",\n    \"crunchbase.com\",\n    \"reuters.com\",\n    \"forbes.com\",\n    \"wikipedia.org\",\n}\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) if os.getenv(\"OPENAI_API_KEY\") else None\nprint(\"OpenAI client ready:\", bool(client))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section B (20 min): Stage 1 - Namecard Extraction\n\n## Pedagogical objective\nTeach students how to convert unstructured multimodal input into a **typed data object** with confidence signals.\n\n## What this section's code does\n1. Defines strict schemas (`NamecardExtraction` and shared models).\n2. Adds utility helpers for retry logic, confidence averaging, and upload parsing.\n3. Sends uploaded image to a multimodal model for OCR + entity extraction.\n4. Sanitizes model output before Pydantic validation.\n5. Applies manual field overrides when OCR is uncertain.\n6. Triggers low-confidence warning for human review.\n\n## Why this matters\n- Raw LLM output is not trusted by default.\n- Schema validation catches malformed values early.\n- Confidence thresholds decide when humans should intervene.\n\n## Student checkpoint\nAfter this stage, inspect:\n- Extracted `full_name`, `company_name`, `email`\n- `field_confidence` values\n- Whether manual correction was needed\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: These Pydantic models define the data shape passed between each stage.\n\nclass NamecardExtraction(BaseModel):\n    full_name: str\n    job_title: Optional[str] = None\n    company_name: Optional[str] = None\n    email: Optional[str] = None\n    phone: Optional[str] = None\n    linkedin_url: Optional[str] = None\n    website: Optional[str] = None\n    raw_text: str\n    field_confidence: Dict[str, float] = Field(default_factory=dict)\n\n\nclass CitationItem(BaseModel):\n    url: str\n    title: str\n    snippet: str\n    retrieved_at: str\n\n\nclass ResearchDossier(BaseModel):\n    person_summary: str\n    person_current_role: Optional[str] = None\n    person_background: List[str] = Field(default_factory=list)\n    company_summary: str\n    company_industry: Optional[str] = None\n    company_size_signal: Optional[str] = None\n    engagement_angle: str\n    citations: List[CitationItem] = Field(default_factory=list)\n    confidence: Dict[str, float] = Field(default_factory=dict)\n\n\nclass CRMRecord(BaseModel):\n    lead_name: str\n    title: Optional[str] = None\n    company: Optional[str] = None\n    email: Optional[str] = None\n    phone: Optional[str] = None\n    linkedin: Optional[str] = None\n    company_website: Optional[str] = None\n    person_notes: str\n    company_notes: str\n    engagement_angle: str\n    source_count: int\n    overall_confidence: float\n    created_at_utc: str\n\n\nclass SheetWriteResult(BaseModel):\n    success: bool\n    sheet_id: str\n    worksheet_name: str\n    row_number: Optional[int] = None\n    error_message: Optional[str] = None\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: Utility helpers used across stages (retry, confidence scoring, upload parsing).\n\ndef now_utc_iso() -> str:\n    return datetime.now(timezone.utc).isoformat()\n\n\ndef retry_call(fn, *args, **kwargs):\n    \"\"\"Retry a function call with linear backoff to handle temporary failures.\"\"\"\n    last_err = None\n    for i in range(1, RETRY_ATTEMPTS + 1):\n        try:\n            return fn(*args, **kwargs)\n        except Exception as e:\n            last_err = e\n            if i < RETRY_ATTEMPTS:\n                time.sleep(RETRY_BACKOFF_SECONDS * i)\n    raise last_err\n\n\ndef mean_confidence(values: Dict[str, float]) -> float:\n    if not values:\n        return 0.0\n    nums = [max(0.0, min(1.0, float(v))) for v in values.values()]\n    return round(sum(nums) / len(nums), 4)\n\n\ndef normalize_domain(url: Optional[str]) -> Optional[str]:\n    if not url:\n        return None\n    try:\n        host = urlparse(url).netloc.lower().replace(\"www.\", \"\")\n        return host or None\n    except Exception:\n        return None\n\n\ndef select_top_sources(citations: List[CitationItem], official_domain: Optional[str], limit: int = 8) -> List[CitationItem]:\n    scored = []\n    for item in citations:\n        domain = normalize_domain(item.url)\n        trusted = domain in TRUSTED_DOMAINS if domain else False\n        official = domain == official_domain if official_domain else False\n        score = (2 if official else 0) + (1 if trusted else 0)\n        scored.append((score, item))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    return [item for _, item in scored[:limit]]\n\n\ndef widget_file_to_bytes(upload_value) -> tuple[bytes, str]:\n    if not upload_value:\n        raise ValueError(\"No file uploaded. Re-upload the card after running the UI cell.\")\n\n    def normalize_item(item, default_name: str = \"uploaded_image\"):\n        if isinstance(item, dict):\n            content = item.get(\"content\", b\"\")\n            name = item.get(\"name\", default_name)\n        else:\n            # Supports UploadedFile-like objects from some ipywidgets versions\n            content = getattr(item, \"content\", b\"\")\n            name = getattr(item, \"name\", default_name)\n\n        if isinstance(content, memoryview):\n            content = content.tobytes()\n        elif isinstance(content, bytearray):\n            content = bytes(content)\n        elif not isinstance(content, (bytes,)):  # best-effort fallback\n            try:\n                content = bytes(content)\n            except Exception:\n                content = b\"\"\n\n        return content, name\n\n    # ipywidgets can return dict, tuple, or list depending on version\n    if isinstance(upload_value, dict):\n        if len(upload_value) == 0:\n            raise ValueError(\"Upload payload is empty.\")\n        first_key = next(iter(upload_value.keys()))\n        item = upload_value[first_key]\n        content, name = normalize_item(item, default_name=first_key)\n    elif isinstance(upload_value, (list, tuple)):\n        if len(upload_value) == 0:\n            raise ValueError(\"Upload payload is empty.\")\n        content, name = normalize_item(upload_value[0])\n    else:\n        # Some versions expose a single UploadedFile-like object\n        content, name = normalize_item(upload_value)\n\n    if not content:\n        raise ValueError(\n            \"Uploaded file has no readable bytes. Try re-uploading the image (JPG/PNG) and run again.\"\n        )\n\n    return content, name\n\n\n\ndef heuristic_extract_raw_text(raw_text: str) -> Dict[str, Optional[str]]:\n    email = None\n    phone = None\n    website = None\n\n    email_match = re.search(r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\", raw_text, re.IGNORECASE)\n    if email_match:\n        email = email_match.group(0)\n\n    phone_match = re.search(r\"(\\+?\\d[\\d\\s\\-()]{7,}\\d)\", raw_text)\n    if phone_match:\n        phone = phone_match.group(0)\n\n    web_match = re.search(r\"(https?://[^\\s]+|www\\.[^\\s]+)\", raw_text, re.IGNORECASE)\n    if web_match:\n        website = web_match.group(0)\n        if website.startswith(\"www.\"):\n            website = \"https://\" + website\n\n    return {\"email\": email, \"phone\": phone, \"website\": website}\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: Stage 1 functions for OCR extraction, output cleanup, and manual override handling.\n\n\n\ndef sanitize_namecard_payload(data: Dict[str, Any]) -> Dict[str, Any]:\n    required_fields = [\n        \"full_name\",\n        \"job_title\",\n        \"company_name\",\n        \"email\",\n        \"phone\",\n        \"linkedin_url\",\n        \"website\",\n        \"raw_text\",\n    ]\n\n    for field in required_fields:\n        if field not in data:\n            data[field] = None\n\n    if not data.get(\"full_name\"):\n        data[\"full_name\"] = \"Unknown Name\"\n\n    if data.get(\"raw_text\") is None:\n        data[\"raw_text\"] = \"\"\n\n    fc = data.get(\"field_confidence\")\n    if not isinstance(fc, dict):\n        fc = {}\n\n    normalized_fc: Dict[str, float] = {}\n    for key, val in fc.items():\n        try:\n            normalized_fc[key] = float(val) if val is not None else 0.0\n        except Exception:\n            normalized_fc[key] = 0.0\n\n    for key in required_fields:\n        normalized_fc.setdefault(key, 0.0)\n\n    data[\"field_confidence\"] = normalized_fc\n    return data\n\n\ndef extract_namecard_with_openai(image_bytes: bytes, filename: str) -> NamecardExtraction:\n    if not client:\n        raise RuntimeError(\"OPENAI_API_KEY is missing.\")\n\n    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n    image_ext = filename.split(\".\")[-1].lower() if \".\" in filename else \"png\"\n    mime_type = \"image/png\" if image_ext not in {\"jpg\", \"jpeg\"} else \"image/jpeg\"\n\n    prompt = (\n        \"Extract fields from this business card image and return strict JSON only. \"\n        \"Required keys: full_name, job_title, company_name, email, phone, linkedin_url, website, raw_text, field_confidence. \"\n        \"field_confidence must be an object mapping each field to a float between 0 and 1. \"\n        \"Use null if uncertain.\"\n    )\n\n    # Call OpenAI with retries so temporary API issues do not stop class flow.\n    response = retry_call(\n        client.responses.create,\n        model=\"gpt-4o-mini\",\n        input=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"input_text\", \"text\": prompt},\n                    {\"type\": \"input_image\", \"image_url\": f\"data:{mime_type};base64,{b64}\"},\n                ],\n            }\n        ],\n        temperature=0,\n    )\n\n    # Parse model text output into JSON, then sanitize before strict validation.\n    text = response.output_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    data = json.loads(text)\n    data = sanitize_namecard_payload(data)\n    return NamecardExtraction.model_validate(data)\n\n\ndef build_deterministic_extraction_fallback(manual_overrides: Dict[str, str], filename: str) -> NamecardExtraction:\n    raw_text = \"\\n\".join([f\"{k}: {v}\" for k, v in manual_overrides.items() if v]) or f\"Uploaded card: {filename}\"\n    heuristics = heuristic_extract_raw_text(raw_text)\n\n    return NamecardExtraction(\n        full_name=manual_overrides.get(\"full_name\") or \"Unknown Name\",\n        job_title=manual_overrides.get(\"job_title\") or None,\n        company_name=manual_overrides.get(\"company_name\") or None,\n        email=manual_overrides.get(\"email\") or heuristics[\"email\"],\n        phone=manual_overrides.get(\"phone\") or heuristics[\"phone\"],\n        linkedin_url=manual_overrides.get(\"linkedin_url\") or None,\n        website=manual_overrides.get(\"website\") or heuristics[\"website\"],\n        raw_text=raw_text,\n        field_confidence={\n            \"full_name\": 0.55,\n            \"job_title\": 0.45,\n            \"company_name\": 0.45,\n            \"email\": 0.60 if (manual_overrides.get(\"email\") or heuristics[\"email\"]) else 0.30,\n            \"phone\": 0.60 if (manual_overrides.get(\"phone\") or heuristics[\"phone\"]) else 0.30,\n            \"linkedin_url\": 0.40,\n            \"website\": 0.50 if (manual_overrides.get(\"website\") or heuristics[\"website\"]) else 0.30,\n        },\n    )\n\n\ndef apply_manual_overrides(extraction: NamecardExtraction, manual_overrides: Dict[str, str]) -> NamecardExtraction:\n    payload = extraction.model_dump()\n    for key, value in manual_overrides.items():\n        if value:\n            payload[key] = value\n            payload[\"field_confidence\"][key] = max(float(payload[\"field_confidence\"].get(key, 0.0)), 0.95)\n    return NamecardExtraction.model_validate(payload)\n\n\ndef run_stage1_extraction(image_bytes: bytes, filename: str, manual_overrides: Dict[str, str]) -> NamecardExtraction:\n    # First try live OCR extraction from the uploaded card image.\n    try:\n        extraction = extract_namecard_with_openai(image_bytes, filename)\n    except Exception as e:\n        # If OCR fails, optionally continue with deterministic fallback for teaching continuity.\n        if not ALLOW_MOCK_FALLBACK:\n            raise\n        print(f\"[Stage1] OpenAI extraction failed, using deterministic fallback: {e}\")\n        extraction = build_deterministic_extraction_fallback(manual_overrides, filename)\n\n    # Apply manual corrections entered by user and compute average confidence.\n    extraction = apply_manual_overrides(extraction, manual_overrides)\n    avg_conf = mean_confidence(extraction.field_confidence)\n    if avg_conf < EXTRACTION_CONFIDENCE_THRESHOLD:\n        print(f\"[Stage1] Low extraction confidence ({avg_conf:.2f}) -> manual review recommended.\")\n\n    return extraction\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section C (30 min): Stage 2 - Deep Research (Planner -> Search -> Writer)\n\n## Pedagogical objective\nShow students how a complex task is decomposed into specialized agentic sub-steps.\n\n## What this section's code does\n1. **Planner:** Generates focused web search queries based on extracted contact context.\n2. **Search executor:** Runs searches (parallelized) and collects citation candidates.\n3. **Source filtering:** Prioritizes official and trusted domains.\n4. **Writer/synthesizer:** Produces a structured `ResearchDossier` with confidence and citations.\n5. **Fallback behavior:** If live research fails, returns a deterministic dossier scaffold.\n\n## Why this architecture is teachable\n- It separates planning, evidence gathering, and synthesis.\n- It makes failure isolation easier (you can debug each stage independently).\n- It encourages citation-grounded outputs instead of free-form hallucinations.\n\n## Student checkpoint\nVerify that dossier fields are evidence-backed:\n- `person_summary`\n- `company_summary`\n- `engagement_angle`\n- non-empty `citations`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: Stage 2 functions for planning searches, collecting citations, and synthesizing research.\n\nclass WebSearchItem(BaseModel):\n    reason: str\n    query: str\n\n\nclass WebSearchPlan(BaseModel):\n    searches: List[WebSearchItem]\n\n\ndef plan_searches(extraction: NamecardExtraction, n_searches: int = 6) -> WebSearchPlan:\n    if not client:\n        raise RuntimeError(\"OPENAI_API_KEY is missing.\")\n\n    prompt = f\"\"\"\nYou are a research planning assistant.\nGiven this contact:\n- full_name: {extraction.full_name}\n- job_title: {extraction.job_title}\n- company_name: {extraction.company_name}\n- website: {extraction.website}\n- linkedin_url: {extraction.linkedin_url}\n\nReturn strict JSON with key 'searches': a list of {n_searches} objects with keys:\n- reason\n- query\n\nRules:\n- Disambiguate common names using company and title.\n- Include searches for both person and company.\n- Keep each query under 15 words.\n\"\"\".strip()\n\n    # Call OpenAI with retries so temporary API issues do not stop class flow.\n    # Ask model to generate a focused search plan from extracted contact context.\n    response = retry_call(\n        client.responses.create,\n        model=\"gpt-4o-mini\",\n        input=prompt,\n        temperature=0,\n    )\n\n    text = response.output_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    data = json.loads(text)\n    return WebSearchPlan.model_validate(data)\n\n\ndef run_web_search(query: str) -> List[CitationItem]:\n    if not client:\n        raise RuntimeError(\"OPENAI_API_KEY is missing.\")\n\n    prompt = (\n        f\"Search the web for: {query}. \"\n        \"Return strict JSON with key 'citations'. \"\n        \"Each citation must have url, title, snippet, retrieved_at (UTC ISO). Limit to 5 citations.\"\n    )\n\n    # Call OpenAI with retries so temporary API issues do not stop class flow.\n    response = retry_call(\n        client.responses.create,\n        model=\"gpt-4o-mini\",\n        tools=[{\"type\": \"web_search_preview\"}],\n        input=prompt,\n        temperature=0,\n    )\n\n    text = response.output_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    data = json.loads(text)\n    items = []\n    for item in data.get(\"citations\", []):\n        if not item.get(\"retrieved_at\"):\n            item[\"retrieved_at\"] = now_utc_iso()\n        items.append(CitationItem.model_validate(item))\n    return items\n\n\ndef perform_searches(plan: WebSearchPlan) -> List[CitationItem]:\n    all_citations: List[CitationItem] = []\n    # Run multiple searches in parallel to reduce total waiting time in class.\n    max_workers = min(4, max(1, len(plan.searches)))\n    with futures.ThreadPoolExecutor(max_workers=max_workers) as pool:\n        task_map = {pool.submit(run_web_search, item.query): item for item in plan.searches}\n        for task in futures.as_completed(task_map):\n            item = task_map[task]\n            try:\n                result = task.result()\n                all_citations.extend(result)\n                print(f\"[Search] Done: {item.query} -> {len(result)} citations\")\n            except Exception as e:\n                print(f\"[Search] Failed: {item.query} -> {e}\")\n    return all_citations\n\n\ndef synthesize_dossier(extraction: NamecardExtraction, citations: List[CitationItem]) -> ResearchDossier:\n    if not client:\n        raise RuntimeError(\"OPENAI_API_KEY is missing.\")\n\n    # Prioritize official/trusted sources before synthesis to reduce noisy evidence.\n    official_domain = normalize_domain(extraction.website)\n    top_sources = select_top_sources(citations, official_domain=official_domain, limit=10)\n\n    prompt = f\"\"\"\nYou are a senior research analyst.\nCreate a structured dossier for CRM usage.\n\nContact:\n{json.dumps(extraction.model_dump(), ensure_ascii=True)}\n\nSources:\n{json.dumps([c.model_dump() for c in top_sources], ensure_ascii=True)}\n\nReturn strict JSON with keys:\n- person_summary (string)\n- person_current_role (string|null)\n- person_background (array of strings)\n- company_summary (string)\n- company_industry (string|null)\n- company_size_signal (string|null)\n- engagement_angle (string)\n- citations (array of citations)\n- confidence (object field->float 0..1)\n\nRules:\n- Only include claims grounded in provided citations.\n- If uncertain, set unknown and lower confidence.\n\"\"\".strip()\n\n    # Call OpenAI with retries so temporary API issues do not stop class flow.\n    # Ask model to generate a focused search plan from extracted contact context.\n    response = retry_call(\n        client.responses.create,\n        model=\"gpt-4o-mini\",\n        input=prompt,\n        temperature=0,\n    )\n\n    text = response.output_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    data = json.loads(text)\n    return ResearchDossier.model_validate(data)\n\n\ndef build_deterministic_dossier_fallback(extraction: NamecardExtraction) -> ResearchDossier:\n    person = extraction.full_name\n    company = extraction.company_name or \"Unknown company\"\n    role = extraction.job_title or \"Unknown role\"\n    website = extraction.website or \"\"\n\n    fallback_citations = []\n    if website:\n        fallback_citations.append(\n            CitationItem(\n                url=website,\n                title=f\"{company} official website\",\n                snippet=f\"Company website referenced for {company}.\",\n                retrieved_at=now_utc_iso(),\n            )\n        )\n\n    return ResearchDossier(\n        person_summary=f\"{person} is listed on the uploaded card as {role} at {company}.\",\n        person_current_role=role,\n        person_background=[\"Background unavailable from fallback mode; verify via live research.\"],\n        company_summary=f\"{company} appears as the company on the uploaded card.\",\n        company_industry=None,\n        company_size_signal=None,\n        engagement_angle=\"Start with discovery questions and confirm current business priorities.\",\n        citations=fallback_citations,\n        confidence={\n            \"person_summary\": 0.55,\n            \"person_current_role\": 0.55,\n            \"company_summary\": 0.55,\n            \"engagement_angle\": 0.50,\n        },\n    )\n\n\ndef run_stage2_research(extraction: NamecardExtraction) -> ResearchDossier:\n    # Full Stage 2 flow: plan -> search -> synthesize.\n    try:\n        plan = plan_searches(extraction)\n        print(f\"[Stage2] Planned {len(plan.searches)} searches\")\n        citations = perform_searches(plan)\n        print(f\"[Stage2] Total citations collected: {len(citations)}\")\n        dossier = synthesize_dossier(extraction, citations)\n        if not dossier.citations:\n            dossier.citations = citations[:5]\n        return dossier\n    except Exception as e:\n        # If OCR fails, optionally continue with deterministic fallback for teaching continuity.\n        if not ALLOW_MOCK_FALLBACK:\n            raise\n        print(f\"[Stage2] Live research failed, using deterministic fallback: {e}\")\n        return build_deterministic_dossier_fallback(extraction)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section D (20 min): Stage 3 - CRM Mapping + Google Sheets Write\n\n## Pedagogical objective\nTeach operationalization: converting AI findings into system-ready business records.\n\n## What this section's code does\n1. Defines canonical CRM column order (`CRM_COLUMNS`).\n2. Maps extraction + dossier into a typed `CRMRecord`.\n3. Loads Google service account credentials safely.\n4. Validates worksheet headers before writing.\n5. Applies idempotency guard (`email + company`) to avoid duplicates.\n6. Appends row and returns structured `SheetWriteResult`.\n\n## Why this matters in production\n- Schema drift is common and must be detected early.\n- Typed write results make error handling deterministic.\n- Idempotency protects downstream systems from duplicate writes.\n\n## Student checkpoint\nConfirm:\n- Header validation passes.\n- Write response includes row number.\n- Duplicate run reuses existing row when applicable.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: Stage 3 functions to map AI output into CRM format and write to Google Sheets.\n\n# Canonical sheet column order. Keep this in sync with row 1 in Google Sheets.\nCRM_COLUMNS = [\n    \"lead_name\",\n    \"title\",\n    \"company\",\n    \"email\",\n    \"phone\",\n    \"linkedin\",\n    \"company_website\",\n    \"person_notes\",\n    \"company_notes\",\n    \"engagement_angle\",\n    \"source_count\",\n    \"overall_confidence\",\n    \"created_at_utc\",\n]\n\n\ndef map_to_crm_record(extraction: NamecardExtraction, dossier: ResearchDossier) -> CRMRecord:\n    # Combine extraction and research confidence into one overall CRM confidence signal.\n    overall_conf = round((mean_confidence(extraction.field_confidence) + mean_confidence(dossier.confidence)) / 2, 4)\n\n    person_notes = dossier.person_summary\n    if dossier.person_background:\n        person_notes += \"\\nBackground: \" + \"; \".join(dossier.person_background)\n\n    company_notes = dossier.company_summary\n    if dossier.company_industry:\n        company_notes += f\"\\nIndustry: {dossier.company_industry}\"\n    if dossier.company_size_signal:\n        company_notes += f\"\\nSize signal: {dossier.company_size_signal}\"\n\n    return CRMRecord(\n        lead_name=extraction.full_name,\n        title=extraction.job_title or dossier.person_current_role,\n        company=extraction.company_name,\n        email=extraction.email,\n        phone=extraction.phone,\n        linkedin=extraction.linkedin_url,\n        company_website=extraction.website,\n        person_notes=person_notes,\n        company_notes=company_notes,\n        engagement_angle=dossier.engagement_angle,\n        source_count=len(dossier.citations),\n        overall_confidence=overall_conf,\n        created_at_utc=now_utc_iso(),\n    )\n\n\ndef load_google_credentials() -> Credentials:\n    raw = os.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\", \"\").strip()\n    if not raw:\n        raise RuntimeError(\"GOOGLE_SERVICE_ACCOUNT_JSON is missing.\")\n\n    scopes = [\n        \"https://www.googleapis.com/auth/spreadsheets\",\n        \"https://www.googleapis.com/auth/drive\",\n    ]\n\n    if raw.startswith(\"{\"):\n        return Credentials.from_service_account_info(json.loads(raw), scopes=scopes)\n\n    if os.path.exists(raw):\n        return Credentials.from_service_account_file(raw, scopes=scopes)\n\n    raise RuntimeError(\"GOOGLE_SERVICE_ACCOUNT_JSON must be a valid file path or JSON string.\")\n\n\ndef open_worksheet(sheet_id: str, worksheet_name: str):\n    \"\"\"Open target Google Sheet worksheet using authenticated client.\"\"\"\n    creds = load_google_credentials()\n    gc = gspread.authorize(creds)\n    sh = gc.open_by_key(sheet_id)\n    return sh.worksheet(worksheet_name)\n\n\ndef validate_sheet_headers(ws, expected_headers: List[str]) -> None:\n    found = ws.row_values(1)\n    if found != expected_headers:\n        raise RuntimeError(f\"Sheet header mismatch. Expected: {expected_headers} | Found: {found}\")\n\n\ndef crm_to_row(record: CRMRecord) -> List[Any]:\n    payload = record.model_dump()\n    return [payload.get(col) for col in CRM_COLUMNS]\n\n\ndef find_duplicate_row(ws, email: Optional[str], company: Optional[str]) -> Optional[int]:\n    if not email and not company:\n        return None\n\n    rows = ws.get_all_records(expected_headers=CRM_COLUMNS)\n    target_email = (email or \"\").strip().lower()\n    target_company = (company or \"\").strip().lower()\n\n    for row_idx, row in enumerate(rows, start=2):\n        row_email = str(row.get(\"email\", \"\")).strip().lower()\n        row_company = str(row.get(\"company\", \"\")).strip().lower()\n        if target_email and target_company and row_email == target_email and row_company == target_company:\n            return row_idx\n\n    return None\n\n\ndef write_record_to_sheet(record: CRMRecord, enable_idempotency_guard: bool = True) -> SheetWriteResult:\n    sheet_id = os.getenv(\"GOOGLE_SHEET_ID\", \"\").strip()\n    worksheet_name = os.getenv(\"GOOGLE_WORKSHEET_NAME\", DEFAULT_WORKSHEET_NAME)\n\n    # Fail early with clear message if required sheet config is missing.\n    if not sheet_id:\n        return SheetWriteResult(\n            success=False,\n            sheet_id=\"\",\n            worksheet_name=worksheet_name,\n            row_number=None,\n            error_message=\"GOOGLE_SHEET_ID is missing.\",\n        )\n\n    try:\n        # Open sheet and verify header contract before any write happens.\n        ws = open_worksheet(sheet_id, worksheet_name)\n        validate_sheet_headers(ws, CRM_COLUMNS)\n\n        # Optional de-duplication check: reuse existing row if same (email, company).\n        if enable_idempotency_guard:\n            existing = find_duplicate_row(ws, record.email, record.company)\n            if existing:\n                return SheetWriteResult(\n                    success=True,\n                    sheet_id=sheet_id,\n                    worksheet_name=worksheet_name,\n                    row_number=existing,\n                    error_message=\"Duplicate detected by (email, company); existing row reused.\",\n                )\n\n        # Append new row when validation and duplicate checks pass.\n        ws.append_row(crm_to_row(record), value_input_option=\"USER_ENTERED\")\n        new_row = len(ws.col_values(1))\n        return SheetWriteResult(\n            success=True,\n            sheet_id=sheet_id,\n            worksheet_name=worksheet_name,\n            row_number=new_row,\n            error_message=None,\n        )\n\n    except Exception as e:\n        return SheetWriteResult(\n            success=False,\n            sheet_id=sheet_id,\n            worksheet_name=worksheet_name,\n            row_number=None,\n            error_message=str(e),\n        )\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section E (10 min): End-to-End Pipeline and UI\n\n## Pedagogical objective\nGive students a complete runnable interface that mirrors real operator workflows.\n\n## What this section's code does\n1. Builds notebook UI widgets for upload and manual corrections.\n2. Displays uploaded filename and byte size for input traceability.\n3. Executes full pipeline (`Stage 1 -> Stage 2 -> Stage 3`) on button click.\n4. Prints progress logs and renders both summary markdown and CRM payload table.\n5. Preserves payload even when Google Sheets write fails.\n\n## How students should run it\n1. Upload one JPG/PNG business card.\n2. Optionally correct fields manually.\n3. Click `Run Pipeline`.\n4. Review each stage output before discussing final CRM write result.\n\n## Instructor demo tip\nRun one good sample and one noisy sample to compare confidence behavior.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: End-to-end pipeline orchestrator and result formatter.\n\ndef run_pipeline(image_bytes: bytes, filename: str, manual_overrides: Dict[str, str]) -> Dict[str, Any]:\n    # Reset errors and run each stage in sequence.\n    workflow_state[\"errors\"] = []\n\n    extraction = run_stage1_extraction(image_bytes, filename, manual_overrides)\n    workflow_state[\"namecard_extraction\"] = extraction.model_dump()\n\n    dossier = run_stage2_research(extraction)\n    workflow_state[\"research_dossier\"] = dossier.model_dump()\n\n    crm_record = map_to_crm_record(extraction, dossier)\n    workflow_state[\"crm_record\"] = crm_record.model_dump()\n\n    sheet_result = write_record_to_sheet(crm_record, enable_idempotency_guard=True)\n    workflow_state[\"sheet_write_result\"] = sheet_result.model_dump()\n\n    return {\n        \"extraction\": extraction,\n        \"dossier\": dossier,\n        \"crm_record\": crm_record,\n        \"sheet_result\": sheet_result,\n    }\n\n\ndef format_result_markdown(result: Dict[str, Any]) -> str:\n    extraction = result[\"extraction\"]\n    dossier = result[\"dossier\"]\n    crm_record = result[\"crm_record\"]\n    sheet_result = result[\"sheet_result\"]\n\n    lines = []\n    lines.append(\"## Pipeline Result\")\n    lines.append(\"### Stage 1: Extraction\")\n    lines.append(f\"- Name: **{extraction.full_name}**\")\n    lines.append(f\"- Title: {extraction.job_title}\")\n    lines.append(f\"- Company: {extraction.company_name}\")\n    lines.append(f\"- Email: {extraction.email}\")\n    lines.append(f\"- Avg confidence: **{mean_confidence(extraction.field_confidence):.2f}**\")\n\n    lines.append(\"### Stage 2: Research Dossier\")\n    lines.append(f\"- Person current role: {dossier.person_current_role}\")\n    lines.append(f\"- Company industry: {dossier.company_industry}\")\n    lines.append(f\"- Source count: **{len(dossier.citations)}**\")\n    lines.append(f\"- Engagement angle: {dossier.engagement_angle}\")\n\n    lines.append(\"### Stage 3: CRM + Google Sheets\")\n    lines.append(f\"- Write success: **{sheet_result.success}**\")\n    lines.append(f\"- Worksheet: `{sheet_result.worksheet_name}`\")\n    lines.append(f\"- Row number: {sheet_result.row_number}\")\n    lines.append(f\"- Error: {sheet_result.error_message}\")\n\n    lines.append(\"### CRM Payload Preview\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(crm_record.model_dump(), indent=2, ensure_ascii=True))\n    lines.append(\"```\")\n\n    return \"\\n\".join(lines)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: Notebook UI widgets and button handlers for interactive demo usage.\n\nuploader = widgets.FileUpload(accept=\".jpg,.jpeg,.png\", multiple=False, description=\"Upload Namecard\")\nupload_status = widgets.HTML(value=\"<i>No file uploaded yet.</i>\")\n\nmanual_full_name = widgets.Text(description=\"Name\", placeholder=\"Optional override\")\nmanual_job_title = widgets.Text(description=\"Title\", placeholder=\"Optional override\")\nmanual_company = widgets.Text(description=\"Company\", placeholder=\"Optional override\")\nmanual_email = widgets.Text(description=\"Email\", placeholder=\"Optional override\")\nmanual_phone = widgets.Text(description=\"Phone\", placeholder=\"Optional override\")\nmanual_linkedin = widgets.Text(description=\"LinkedIn\", placeholder=\"Optional override\")\nmanual_website = widgets.Text(description=\"Website\", placeholder=\"Optional override\")\n\nrun_button = widgets.Button(description=\"Run Pipeline\", button_style=\"primary\")\noutput = widgets.Output()\n\n\ndef on_upload_change(change):\n    \"\"\"Update UI label immediately when a file is uploaded.\"\"\"\n    if change.get(\"name\") != \"value\":\n        return\n    try:\n        content, filename = widget_file_to_bytes(uploader.value)\n        upload_status.value = f\"<b>Uploaded:</b> {filename} ({len(content)} bytes)\"\n    except Exception:\n        upload_status.value = \"<i>No file uploaded yet.</i>\"\n\n\nuploader.observe(on_upload_change, names=\"value\")\n\n\ndef on_run_clicked(_):\n    \"\"\"Handle Run button click and execute the full workflow.\"\"\"\n    with output:\n        # Clear previous run logs so students only see the latest execution trace.\n        output.clear_output()\n        try:\n            upload_count = len(uploader.value) if hasattr(uploader.value, \"__len__\") else 1\n            print(f\"[Upload] Files detected: {upload_count}\")\n            image_bytes, filename = widget_file_to_bytes(uploader.value)\n            print(f\"[Upload] Reading: {filename} ({len(image_bytes)} bytes)\")\n            manual_overrides = {\n                \"full_name\": manual_full_name.value.strip(),\n                \"job_title\": manual_job_title.value.strip(),\n                \"company_name\": manual_company.value.strip(),\n                \"email\": manual_email.value.strip(),\n                \"phone\": manual_phone.value.strip(),\n                \"linkedin_url\": manual_linkedin.value.strip(),\n                \"website\": manual_website.value.strip(),\n            }\n\n            print(\"[Pipeline] Running end-to-end workflow...\")\n            result = run_pipeline(image_bytes, filename, manual_overrides)\n            display(Markdown(format_result_markdown(result)))\n            display(pd.DataFrame([result[\"crm_record\"].model_dump()]))\n\n            if result[\"sheet_result\"].success:\n                print(\"[Pipeline] Completed successfully.\")\n            else:\n                print(\"[Pipeline] Completed with sheet write failure; payload preserved for retry.\")\n\n        except Exception as e:\n            print(\"[Pipeline] Failed:\", e)\n\n\nrun_button.on_click(on_run_clicked)\n\nform = widgets.VBox([\n    widgets.HTML(\"<h3>1) Upload Namecard</h3>\"),\n    uploader,\n    upload_status,\n    widgets.HTML(\"<h3>2) Optional Manual Corrections</h3>\"),\n    manual_full_name,\n    manual_job_title,\n    manual_company,\n    manual_email,\n    manual_phone,\n    manual_linkedin,\n    manual_website,\n    widgets.HTML(\"<h3>3) Execute</h3>\"),\n    run_button,\n    output,\n])\n\ndisplay(form)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Validation Scenarios (Class Drills)\n\n## Pedagogical objective\nMove from \"it runs\" to \"it is trustworthy.\" This section teaches students to validate AI pipelines with explicit checks.\n\n## What this validation cell checks\n1. Citation quality (missing URLs).\n2. Low-confidence extraction guardrail.\n3. Google Sheets write success/failure state.\n4. CRM schema completeness.\n\n## Suggested classroom drill sequence\n1. Run a clean card and observe expected PASS outcomes.\n2. Introduce a low-quality card image to trigger confidence warning.\n3. Break a sheet header intentionally to show fail-fast schema behavior.\n4. Re-run and discuss why these checks matter for deployment readiness.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Beginner note: Quick validation checks to verify data quality and pipeline health.\n\ndef run_validation_checks():\n    \"\"\"Run post-execution checks for quality, schema, and write status.\"\"\"\n    print(\"Validation checks:\")\n\n    dossier_payload = workflow_state.get(\"research_dossier\")\n    if dossier_payload:\n        missing_url = [c for c in dossier_payload.get(\"citations\", []) if not c.get(\"url\")]\n        print(\"- Citation quality:\", \"PASS\" if not missing_url else f\"FAIL ({len(missing_url)} missing URLs)\")\n    else:\n        print(\"- Citation quality: SKIP (no dossier)\")\n\n    extraction_payload = workflow_state.get(\"namecard_extraction\")\n    if extraction_payload:\n        avg = mean_confidence(extraction_payload.get(\"field_confidence\", {}))\n        print(\"- Low-confidence guard:\", \"PASS\" if avg >= EXTRACTION_CONFIDENCE_THRESHOLD else f\"REVIEW NEEDED ({avg:.2f})\")\n    else:\n        print(\"- Low-confidence guard: SKIP (no extraction)\")\n\n    sheet_payload = workflow_state.get(\"sheet_write_result\")\n    if sheet_payload:\n        print(\"- Sheets write:\", \"PASS\" if sheet_payload.get(\"success\") else f\"FAIL ({sheet_payload.get('error_message')})\")\n    else:\n        print(\"- Sheets write: SKIP (not run)\")\n\n    crm_payload = workflow_state.get(\"crm_record\")\n    if crm_payload:\n        missing_cols = [c for c in CRM_COLUMNS if c not in crm_payload]\n        print(\"- CRM schema:\", \"PASS\" if not missing_cols else f\"FAIL missing {missing_cols}\")\n    else:\n        print(\"- CRM schema: SKIP (no record)\")\n\n\nrun_validation_checks()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Teaching Flow (90 Minutes)\n\n## Suggested facilitation plan\n1. **0-10 min:** Explain architecture, contracts, and failure boundaries.\n2. **10-30 min:** Stage 1 live run (clear card + blurry card comparison).\n3. **30-60 min:** Stage 2 decomposition (planner/search/writer responsibilities).\n4. **60-80 min:** Stage 3 CRM integration and idempotency behavior.\n5. **80-90 min:** Validation drills + discussion of production hardening.\n\n## Discussion prompts\n- Where should human review be mandatory?\n- Which confidence thresholds are acceptable for your domain?\n- What additional observability would you add in production?\n- How would you batch this pipeline for many cards?\n\n## Stretch extensions (post-class)\n1. Add batch ingestion for multiple cards.\n2. Add per-claim citation linking in UI.\n3. Route low-confidence leads into manual queue instead of auto-write.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}