{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c85edcb",
   "metadata": {},
   "source": "# Introduction to AI Agents using OpenAI Agents SDK\n## Complete Guide to GPT-5.2 & GPT-4o Agents (2026 Edition)\n\n> ‚ö†Ô∏è **COST WARNING**\n> - **WebSearchTool**: $0.025 per call\n> - **GPT-5.2 Instant/Thinking**: $1.75/1M input, $14/1M output\n> - **GPT-5.2 Pro**: $21/1M input, $168/1M output  \n> - **GPT-5.2-Codex**: $2.5/1M input, $20/1M output\n> - **GPT-4o (Alternative)**: ~$2.5/1M input, $10/1M output\n> - **Estimated total lab cost**: $3-$5\n\n## Prerequisites\n\nBefore starting, ensure you have:\n- ‚úÖ OpenAI API key with credits\n- ‚úÖ Python 3.10+\n- ‚úÖ Required packages: `openai>=1.54.0`, `agents`, `python-dotenv`, `pydantic>=2.0`\n\n## What You'll Learn\n\nThis comprehensive notebook covers:\n1. **Foundation**: GPT-5.2 models, cost tracking, environment setup\n2. **Basics**: Simple agents, model comparison, instructions\n3. **Tools**: Custom function tools, WebSearchTool, orchestration\n4. **Structured Outputs**: Pydantic models, validation, patterns\n5. **Multi-Agent Systems**: Handoffs, debates, orchestration\n6. **Advanced Features**: Streaming, parallel execution, Pro features\n7. **Real-World Use Cases**: RAG, data analysis, content pipelines\n8. **Production Patterns**: Error handling, testing, optimization\n\n## Installation\n\n```bash\npip install openai>=1.54.0 agents python-dotenv pydantic\n```\n\n## Setup .env File\n\nCreate a `.env` file in your project root:\n```\nOPENAI_API_KEY=your_openai_api_key_here\n```\n\n---\n\n**üìò Let's get started!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2688d",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import load_dotenv\nfrom agents import Agent, WebSearchTool, FileSearchTool, trace, Runner, function_tool\nfrom agents.model_settings import ModelSettings\nfrom IPython.display import display, Markdown, HTML, JSON\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List, Dict\nimport asyncio\nimport json\nfrom datetime import datetime\nimport time\n\n# Load environment variables\nload_dotenv()\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\nif not openai_api_key:\n    raise ValueError(\"‚ùå OPENAI_API_KEY not found in environment\")\n\nprint(\"‚úÖ API key loaded successfully\")\n\n# Cost tracking class for GPT-5.2 and GPT-4o\nclass CostTracker:\n    \"\"\"Track API costs for GPT-5.2 and GPT-4o models\"\"\"\n    \n    PRICING = {\n        # GPT-5.2 models\n        \"gpt-5.2-chat-latest\": {\"input\": 1.75/1_000_000, \"output\": 14/1_000_000},\n        \"gpt-5.2\": {\"input\": 1.75/1_000_000, \"output\": 14/1_000_000},\n        \"gpt-5.2-pro\": {\"input\": 21/1_000_000, \"output\": 168/1_000_000},\n        \"gpt-5.2-codex\": {\"input\": 2.5/1_000_000, \"output\": 20/1_000_000},\n        # GPT-4o as alternative\n        \"gpt-4o\": {\"input\": 2.5/1_000_000, \"output\": 10/1_000_000},\n        \"gpt-4o-mini\": {\"input\": 0.15/1_000_000, \"output\": 0.6/1_000_000},\n    }\n    \n    def __init__(self):\n        self.calls = []\n        self.total_cost = 0\n        self.web_searches = 0\n    \n    def add_call(self, model: str, input_tokens: int, output_tokens: int):\n        \"\"\"Track an API call\"\"\"\n        pricing = self.PRICING.get(model, self.PRICING[\"gpt-4o\"])\n        cost = (input_tokens * pricing[\"input\"]) + (output_tokens * pricing[\"output\"])\n        \n        self.calls.append({\n            \"model\": model,\n            \"input_tokens\": input_tokens,\n            \"output_tokens\": output_tokens,\n            \"cost\": cost,\n            \"timestamp\": datetime.now()\n        })\n        self.total_cost += cost\n    \n    def add_web_search(self, count: int = 1):\n        \"\"\"Track web search costs\"\"\"\n        self.web_searches += count\n        self.total_cost += (count * 0.025)\n    \n    def report(self):\n        \"\"\"Display cost summary\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(\"üí∞ COST SUMMARY\")\n        print(f\"{'='*60}\")\n        print(f\"Total API calls: {len(self.calls)}\")\n        print(f\"Web searches: {self.web_searches}\")\n        print(f\"Total cost: ${self.total_cost:.4f}\")\n        \n        if self.calls:\n            by_model = {}\n            for call in self.calls:\n                model = call[\"model\"]\n                by_model[model] = by_model.get(model, 0) + call[\"cost\"]\n            \n            print(\"\\nBy model:\")\n            for model, cost in by_model.items():\n                print(f\"  {model}: ${cost:.4f}\")\n        \n        print(f\"{'='*60}\\n\")\n\ntracker = CostTracker()\nprint(\"üìä Cost tracker initialized\")"
  },
  {
   "cell_type": "markdown",
   "id": "6d9fc1b2",
   "metadata": {},
   "source": "## üìç PHASE 1: Foundation & Setup\n\n## GPT-5.2 Model Family (Released December 2025)\n\n| Model | API ID | Best For | Speed | Input Cost | Output Cost |\n|-------|--------|----------|-------|------------|-------------|\n| **Instant** | `gpt-5.2-chat-latest` | Fast everyday tasks, simple queries | ‚ö°‚ö°‚ö° | $1.75/1M | $14/1M |\n| **Thinking** | `gpt-5.2` | Complex reasoning, analysis, coding | ‚ö°‚ö° | $1.75/1M | $14/1M |\n| **Pro** | `gpt-5.2-pro` | Maximum quality, hardest problems | ‚ö° | $21/1M | $168/1M |\n| **Codex** | `gpt-5.2-codex` | Agentic coding workflows | ‚ö°‚ö° | $2.5/1M | $20/1M |\n\n### GPT-4o as Alternative\n\n| Model | API ID | Best For | Speed | Input Cost | Output Cost |\n|-------|--------|----------|-------|------------|-------------|\n| **GPT-4o** | `gpt-4o` | General purpose, multimodal | ‚ö°‚ö° | $2.5/1M | $10/1M |\n| **GPT-4o-mini** | `gpt-4o-mini` | Fast, cost-effective | ‚ö°‚ö°‚ö° | $0.15/1M | $0.6/1M |\n\n---\n\n### When to Use Each Model\n\n**GPT-5.2 Instant** (`gpt-5.2-chat-latest`)\n- ‚úÖ Quick Q&A, translations, summaries\n- ‚úÖ High-volume simple requests  \n- ‚úÖ Fast classification tasks\n- üéØ **Performance**: Fastest response times\n\n**GPT-5.2 Thinking** (`gpt-5.2`)\n- ‚úÖ Complex analysis and reasoning\n- ‚úÖ Code generation and debugging\n- ‚úÖ Research and synthesis\n- ‚úÖ Multi-step problem solving\n- üéØ **Performance**: 30% fewer errors vs GPT-5.1\n\n**GPT-5.2 Pro** (`gpt-5.2-pro`)\n- ‚úÖ Advanced mathematics\n- ‚úÖ Scientific research\n- ‚úÖ Critical decision-making\n- ‚úÖ Maximum quality requirements\n- üéØ **Performance**: 93.2% GPQA Diamond, 100% AIME 2025\n\n**GPT-5.2-Codex** (`gpt-5.2-codex`)\n- ‚úÖ Terminal automation\n- ‚úÖ Multi-file code changes\n- ‚úÖ Complex refactoring\n- üéØ **Performance**: 56.4% SWE-Bench Pro, 64% Terminal-Bench\n\n**GPT-4o (Alternative)**\n- ‚úÖ Use when GPT-5.2 unavailable\n- ‚úÖ Similar capabilities to GPT-5.2 Thinking\n- ‚úÖ Lower cost than GPT-5.2\n- üéØ **Cost-effective alternative**"
  },
  {
   "cell_type": "code",
   "id": "ws1bv8skwl",
   "source": "def recommend_model(task_description: str) -> tuple[str, str]:\n    \"\"\"Recommend optimal model for a task (GPT-5.2 or GPT-4o fallback)\"\"\"\n    \n    task_lower = task_description.lower()\n    \n    # Pro indicators\n    if any(kw in task_lower for kw in [\"critical\", \"research\", \"mathematics\", \"prove\", \"scientific\"]):\n        return \"gpt-5.2-pro\", \"üèÜ Maximum quality for critical work (or gpt-4o if unavailable)\"\n    \n    # Codex indicators  \n    if any(kw in task_lower for kw in [\"code\", \"programming\", \"debug\", \"refactor\", \"terminal\"]):\n        return \"gpt-5.2-codex\", \"üíª Optimized for coding (or gpt-4o if unavailable)\"\n    \n    # Thinking indicators\n    if any(kw in task_lower for kw in [\"analyze\", \"plan\", \"reasoning\", \"complex\", \"multi-step\"]):\n        return \"gpt-5.2\", \"üß† Best for reasoning (or gpt-4o if unavailable)\"\n    \n    # Default to Instant\n    return \"gpt-5.2-chat-latest\", \"‚ö° Fast for simple tasks (or gpt-4o-mini if unavailable)\"\n\n# Test recommendations\nprint(\"üéØ Model Recommendation Examples:\\n\")\n\ntasks = [\n    \"Translate this text to Spanish\",\n    \"Analyze this 50-page research paper\",\n    \"Solve this advanced calculus problem\",\n    \"Debug my Python code\",\n    \"Quick FAQ answer\"\n]\n\nfor task in tasks:\n    model, reason = recommend_model(task)\n    print(f\"Task: {task}\")\n    print(f\"  ‚Üí Recommended: {model}\")\n    print(f\"  ‚Üí {reason}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mg01v8ymu8s",
   "source": "## What is an AI Agent?\n\nAn **Agent** is an autonomous entity that can:\n1. üìù Receive instructions (system prompt)\n2. üõ†Ô∏è Use tools to gather information\n3. üß† Reason about tasks\n4. ‚úÖ Take actions to complete objectives\n\n### Agent Components\n\n```python\nAgent(\n    name=\"AgentName\",            # Identifier for tracking\n    instructions=\"What to do\",   # System prompt defining behavior\n    model=\"gpt-5.2\",            # Which model to use\n    tools=[...],                # Available functions (optional)\n    output_type=Schema,         # Structured response (optional)\n    model_settings=Settings,    # Temperature, etc. (optional)\n    handoff_to=[...]           # Other agents (optional)\n)\n```\n\n### Agent Execution Flow\n\n```\nUser Input\n    ‚Üì\nAgent Receives Task\n    ‚Üì\nProcesses with Instructions\n    ‚Üì\nCalls Tools (if needed)\n    ‚Üì\nReturns Response\n    ‚Üì\nLogged to Trace\n```\n\n### Key Concepts\n\n- **Tools**: Functions the agent can call (WebSearch, custom functions)\n- **Structured Outputs**: Typed responses using Pydantic models\n- **Handoffs**: Transferring to other specialized agents\n- **Traces**: Execution logs in OpenAI console\n- **Model Settings**: Temperature, token limits, tool choice\n\n### Model Selection\n\n- Use `gpt-5.2-chat-latest` (Instant) for simple, fast tasks\n- Use `gpt-5.2` (Thinking) for complex reasoning\n- Use `gpt-5.2-pro` (Pro) for maximum quality\n- Use `gpt-4o` as cost-effective alternative",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "xwt4hoe50hp",
   "source": "## Model Performance Benchmarks\n\n### GPT-5.2 Pro Benchmarks\n- üìä **GPQA Diamond**: 93.2%\n- üìä **AIME 2025**: 100%\n- üìä **SWE-Bench Verified**: 80%\n\n### GPT-5.2 Thinking\n- üìä **30% fewer errors** than GPT-5.1 Thinking\n- üìä **70.9% better** than top professionals on GDP\n\nval tasks\n\n### GPT-5.2-Codex\n- üìä **SWE-Bench Pro**: 56.4%\n- üìä **Terminal-Bench 2.0**: 64.0%\n\n### Context & Output\n- **Context window**: 400K tokens\n- **Max output**: 128K tokens\n- **Knowledge cutoff**: August 2025",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3a0s2z1kiqm",
   "source": "# Basic cost estimation examples\n\ndef estimate_cost(model: str, input_tokens: int, output_tokens: int) -> float:\n    \"\"\"Estimate cost for a given model and token usage\"\"\"\n    pricing = CostTracker.PRICING.get(model, CostTracker.PRICING[\"gpt-4o\"])\n    return (input_tokens * pricing[\"input\"]) + (output_tokens * pricing[\"output\"])\n\nprint(\"üí∞ Cost Estimation Examples\\n\")\nprint(\"=\"*70)\n\n# Example scenarios\nscenarios = [\n    (\"Simple query\", \"gpt-5.2-chat-latest\", 100, 200),\n    (\"Complex analysis\", \"gpt-5.2\", 500, 1000),\n    (\"Critical research\", \"gpt-5.2-pro\", 1000, 2000),\n    (\"Code generation\", \"gpt-5.2-codex\", 800, 1500),\n    (\"GPT-4o alternative\", \"gpt-4o\", 500, 1000),\n]\n\nfor desc, model, input_tok, output_tok in scenarios:\n    cost = estimate_cost(model, input_tok, output_tok)\n    print(f\"\\n{desc}:\")\n    print(f\"  Model: {model}\")\n    print(f\"  Tokens: {input_tok} in / {output_tok} out\")\n    print(f\"  Cost: ${cost:.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"\\nüí° Tip: Use tracker.add_call() after each agent run to track actual costs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5ljm5wcv45f",
   "source": "## Understanding Traces & Debugging\n\n**Traces** are execution logs that help you debug and understand agent behavior.\n\n### What Gets Logged\n- üìù Agent instructions and model used\n- üîÑ Complete message history\n- üõ†Ô∏è Tool calls and responses\n- ‚è±Ô∏è Timing and performance metrics\n- üí∞ Token usage\n\n### Viewing Traces\nAll traces are automatically logged to:\n**https://platform.openai.com/traces**\n\n### Using Traces\n\n```python\n# Create named trace\nwith trace(\"My Task Description\"):\n    result = await Runner.run(agent, \"query\")\n\n# Generate custom trace ID\ntrace_id = gen_trace_id()\nwith trace(\"Task\", trace_id=trace_id):\n    result = await Runner.run(agent, \"query\")\n```\n\n### Benefits\n- ‚úÖ Debug agent behavior\n- ‚úÖ Monitor performance\n- ‚úÖ Track costs\n- ‚úÖ Optimize prompts\n- ‚úÖ Share with team",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "msy4q0qwrun",
   "source": "# Trace ID generation and organization examples\n\nfrom agents import gen_trace_id\n\nprint(\"üîç Trace Organization Examples\\n\")\nprint(\"=\"*70)\n\n# Generate custom trace IDs\ntrace_ids = {\n    \"simple_query\": gen_trace_id(),\n    \"complex_analysis\": gen_trace_id(),\n    \"multi_step_task\": gen_trace_id()\n}\n\nprint(\"\\nüìã Generated Trace IDs:\")\nfor task_name, tid in trace_ids.items():\n    print(f\"  {task_name}: {tid}\")\n\nprint(\"\\nüí° Usage:\")\nprint(\"\"\"\n# Organized traces by task type\nwith trace(\"Simple Query\", trace_id=trace_ids['simple_query']):\n    result = await Runner.run(agent, \"What is 2+2?\")\n\n# All related traces will share same ID prefix\n# Easy to find and group in OpenAI console\n\"\"\")\n\nprint(\"=\"*70)\nprint(\"\\n‚úÖ Phase 1 setup complete! Ready for Phase 2.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "21u59gxm63k",
   "source": "## ‚úÖ Phase 1 Complete: Foundation & Setup\n\n### What You Learned\n- ‚úÖ GPT-5.2 model family (Instant, Thinking, Pro, Codex)\n- ‚úÖ GPT-4o as cost-effective alternative\n- ‚úÖ Cost tracking with CostTracker class\n- ‚úÖ Model selection for different tasks\n- ‚úÖ Agent fundamentals and components\n- ‚úÖ Performance benchmarks\n- ‚úÖ Trace logging and debugging\n\n### Key Takeaways\n1. **Model Selection**: Use Instant for speed, Thinking for reasoning, Pro for quality\n2. **Cost Management**: Track costs with `tracker.add_call()`\n3. **Debugging**: Use traces to understand agent behavior\n4. **Alternatives**: GPT-4o available when GPT-5.2 unavailable\n\n### Next Up: Phase 2 - Basic Agents\nIn the next phase, you'll learn to:\n- Create your first AI agent\n- Compare models live\n- Engineer effective instructions\n- Control temperature and creativity\n\n---\n\n**Ready to build your first agent?** ‚Üí Continue to Phase 2!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "gmf3b0dc4or",
   "source": "## üìç PHASE 2: Basic Agents\n\nIn this phase, you'll create your first AI agents and learn how to:\n- Run agents with different models\n- Compare GPT-5.2 vs GPT-4o\n- Control creativity with temperature\n- Write effective instructions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05006ac",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Your First Agent with GPT-5.2 or GPT-4o\n\n# Create a simple agent\nbasic_agent = Agent(\n    name=\"QuickResponder\",\n    instructions=\"Answer questions concisely in Singlish style\",\n    model=\"gpt-4o\"  # Use gpt-4o (or gpt-5.2-chat-latest if available)\n)\n\n# Run the agent\nwith trace(\"First Agent Run\"):\n    result = await Runner.run(basic_agent, \"Tell me a joke about AI agents lah\")\n    print(\"ü§ñ Agent Response:\")\n    print(result.final_output)\n\n# Track cost (estimate)\ntracker.add_call(\"gpt-4o\", 50, 150)\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Your first agent ran successfully!\")\nprint(\"üí° Check traces at: https://platform.openai.com/traces\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9239bc",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12: Live Model Comparison (GPT-4o vs GPT-5.2)\n\nprint(\"üî¨ Comparing GPT-4o and GPT-5.2 (if available)\\n\")\nprint(\"=\"*70)\n\ntask = \"Explain how AI agents work in 2 sentences\"\n\n# Test GPT-4o\ngpt4o_agent = Agent(\n    name=\"GPT4o-Agent\",\n    instructions=\"Explain clearly and concisely\",\n    model=\"gpt-4o\"\n)\n\nprint(\"\\n**GPT-4o Response:**\")\nwith trace(\"GPT-4o Test\"):\n    result_4o = await Runner.run(gpt4o_agent, task)\n    print(result_4o.final_output)\ntracker.add_call(\"gpt-4o\", 80, 100)\n\n# Optionally test GPT-5.2 if available\n# Uncomment if you have GPT-5.2 access:\n# print(\"\\n**GPT-5.2 Response:**\")\n# gpt52_agent = Agent(\n#     name=\"GPT52-Agent\", \n#     instructions=\"Explain clearly and concisely\",\n#     model=\"gpt-5.2-chat-latest\"\n# )\n# with trace(\"GPT-5.2 Test\"):\n#     result_52 = await Runner.run(gpt52_agent, task)\n#     print(result_52.final_output)\n# tracker.add_call(\"gpt-5.2-chat-latest\", 80, 100)\n\nprint(\"\\n\" + \"=\"*70)\ntracker.report()"
  },
  {
   "cell_type": "markdown",
   "id": "dwj2lbmbzef",
   "source": "## Temperature & Creativity Control\n\n**Temperature** controls the randomness/creativity of agent responses:\n- **0.0**: Deterministic, consistent, factual\n- **0.7**: Balanced (default)\n- **1.0+**: More creative, varied, unpredictable\n\n### When to Use Different Temperatures\n\n| Temperature | Best For | Example Use Case |\n|-------------|----------|------------------|\n| 0.0 - 0.3 | Facts, analysis, code | Math problems, data analysis |\n| 0.4 - 0.7 | General purpose | Q&A, instructions |\n| 0.8 - 1.2 | Creative content | Stories, marketing copy |\n| 1.3 - 2.0 | High creativity | Brainstorming, art |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8m1nrywgum5",
   "source": "# Cell 14: Temperature Comparison Demo\n\nprint(\"üå°Ô∏è Temperature Comparison\\n\")\nprint(\"=\"*70)\n\ntask = \"Write a one-sentence story opening about robots\"\n\ntemperatures = [0.0, 0.7, 1.5]\n\nfor temp in temperatures:\n    print(f\"\\n**Temperature {temp}:**\")\n    \n    agent = Agent(\n        name=f\"Agent-temp-{temp}\",\n        instructions=\"Write creative story openings\",\n        model=\"gpt-4o\",\n        model_settings=ModelSettings(temperature=temp)\n    )\n    \n    with trace(f\"Temp {temp}\"):\n        result = await Runner.run(agent, task)\n        print(result.final_output)\n    \n    tracker.add_call(\"gpt-4o\", 50, 80)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nüí° Notice: Higher temperature = more creative/varied responses\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8t30udlapnn",
   "source": "## Instruction Engineering Best Practices\n\nGood instructions are the foundation of effective agents. Follow these principles:\n\n### ‚úÖ Good Instructions\n- **Specific**: Define exact behavior and output format\n- **Clear**: Use simple, unambiguous language\n- **Complete**: Include all necessary context\n- **Structured**: Use numbered steps or bullet points\n- **Examples**: Show desired output format\n\n### ‚ùå Bad Instructions\n- Vague: \"Help the user\"\n- Ambiguous: \"Be creative\"\n- Incomplete: Missing key context\n- Unstructured: Wall of text\n\n### Template\n```\nYou are a [ROLE].\n\nYour task:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\nOutput format: [FORMAT]\n\nExample:\n[EXAMPLE]\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "no4kjh8lzi",
   "source": "# Cell 16: Good vs Bad Instructions Demo\n\nprint(\"üìù Instruction Quality Comparison\\n\")\nprint(\"=\"*70)\n\ntask = \"Summarize this: 'AI agents are autonomous software that can use tools and make decisions.'\"\n\n# ‚ùå Bad: Vague instructions\nbad_agent = Agent(\n    name=\"VagueAgent\",\n    instructions=\"Help summarize things\",\n    model=\"gpt-4o\"\n)\n\nprint(\"\\n‚ùå **Bad Instructions** ('Help summarize things'):\")\nresult_bad = await Runner.run(bad_agent, task)\nprint(result_bad.final_output)\n\n# ‚úÖ Good: Specific instructions\ngood_agent = Agent(\n    name=\"SpecificAgent\",\n    instructions=\"\"\"You are a summarization expert.\n\nTask: Summarize text in exactly 1 sentence, max 15 words.\nStyle: Simple, clear language.\nFormat: Single sentence, no preamble.\"\"\",\n    model=\"gpt-4o\"\n)\n\nprint(\"\\n‚úÖ **Good Instructions** (Specific, structured):\")\nresult_good = await Runner.run(good_agent, task)\nprint(result_good.final_output)\n\nprint(\"\\n\" + \"=\"*70)\ntracker.add_call(\"gpt-4o\", 100, 100)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "m1gbptpn3lg",
   "source": "## ‚úÖ Phase 2 Complete: Basic Agents\n\n### What You Learned\n- ‚úÖ Created first runnable agents with GPT-4o\n- ‚úÖ Compared different models\n- ‚úÖ Controlled creativity with temperature\n- ‚úÖ Engineered effective instructions\n- ‚úÖ Good vs bad instruction patterns\n\n### Key Takeaways\n1. **Temperature**: 0.0 for facts, 0.7 for balance, 1.5+ for creativity\n2. **Instructions**: Specific > Vague, Structured > Unstructured\n3. **Model Choice**: GPT-4o for general use, GPT-5.2 when available\n4. **Traces**: Always use `with trace()` for debugging\n\n### Next Up: Phase 3 - Custom Tools\nLearn to create and use custom function tools to extend agent capabilities!\n\n---\n\n**Ready for Phase 3?** ‚Üí Continue below!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "26w5tzseh68",
   "source": "## üìç PHASE 3: Custom Tools\n\nTools extend agent capabilities by giving them access to functions. In this phase, you'll learn:\n- Creating custom tools with `@function_tool`\n- Using WebSearchTool for web research\n- Tool parameter validation\n- Multiple tool orchestration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xeraftqwjjj",
   "source": "# Cell 22: Creating Custom Function Tools\n\n@function_tool\ndef calculate_cost(input_tokens: int, output_tokens: int, model: str = \"gpt-4o\") -> str:\n    \"\"\"Calculate exact API cost for given model and token usage\"\"\"\n    pricing = CostTracker.PRICING.get(model, CostTracker.PRICING[\"gpt-4o\"])\n    cost = (input_tokens * pricing[\"input\"]) + (output_tokens * pricing[\"output\"])\n    return f\"üí∞ Cost for {model}: ${cost:.6f} ({input_tokens} in + {output_tokens} out tokens)\"\n\n@function_tool\ndef get_current_time() -> str:\n    \"\"\"Get current date and time\"\"\"\n    return f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n\n@function_tool\ndef word_count(text: str) -> str:\n    \"\"\"Count words in text\"\"\"\n    count = len(text.split())\n    return f\"üìä Word count: {count}\"\n\nprint(\"‚úÖ Created 3 custom function tools:\")\nprint(\"  1. calculate_cost() - Estimates API costs\")\nprint(\"  2. get_current_time() - Returns current datetime\")\nprint(\"  3. word_count() - Counts words in text\")\nprint(\"\\nüí° These tools can now be used by agents!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ad5xmq4s4g",
   "source": "# Cell 23: Agent Using Custom Tools\n\ntool_agent = Agent(\n    name=\"ToolUser\",\n    instructions=\"You help users by calling the appropriate tools. Always use tools to get accurate information.\",\n    model=\"gpt-4o\",\n    tools=[calculate_cost, get_current_time, word_count],\n    model_settings=ModelSettings(tool_choice=\"required\")  # Force tool use\n)\n\nprint(\"ü§ñ Agent with Custom Tools\\n\")\nprint(\"=\"*70)\n\nqueries = [\n    \"What time is it now?\",\n    \"How many words are in 'AI agents are transforming software development'?\",\n    \"Calculate cost for 1000 input and 500 output tokens using gpt-4o\"\n]\n\nfor query in queries:\n    print(f\"\\n‚ùì Query: {query}\")\n    with trace(f\"Tool Query\"):\n        result = await Runner.run(tool_agent, query)\n        print(f\"üí¨ Response: {result.final_output}\")\n    tracker.add_call(\"gpt-4o\", 100, 150)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ Agent successfully used custom tools!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "881facb9",
   "metadata": {},
   "source": [
    "You can check what the AI agent in Traces within the Open AI Console: \n",
    "https://platform.openai.com/logs?api=traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b89dd",
   "metadata": {},
   "source": "## WebSearchTool Deep Dive\n\n**WebSearchTool** is a hosted tool that lets agents search the web for current information.\n\n### OpenAI Hosted Tools\n- **WebSearchTool**: Search the web ($0.025 per call)\n- **FileSearchTool**: Query Vector Stores\n- **ComputerTool**: Automate computer tasks\n\n### WebSearchTool Configuration\n\n```python\nWebSearchTool(\n    search_context_size=\"low\"    # low, medium, high\n)\n```\n\n| Context Size | Cost | Use When |\n|--------------|------|----------|\n| **low** | $ | Simple facts |\n| **medium** | $$ | Detailed research |\n| **high** | $$$ | Comprehensive analysis |\n\n### Important: Cost Warning\n- **$0.025 per search call**\n- Can add up quickly ($2-$3 for this lab)\n- Use `tracker.add_web_search()` to monitor\n\n### Best Practices\n1. Use `tool_choice=\"required\"` to ensure web search\n2. Start with \"low\" context size\n3. Track costs with `tracker.add_web_search()`\n4. Combine multiple queries when possible"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51717c9a",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 25: Web Search Agent with GPT-4o\n\nSEARCH_INSTRUCTIONS = \"\"\"You are a research assistant. Search the web and provide:\n1. Brief summary (2-3 sentences)\n2. Key findings (3-4 bullet points)\n3. Source information\n\nBe concise and factual.\"\"\"\n\nsearch_agent = Agent(\n    name=\"SearchAgent\",\n    instructions=SEARCH_INSTRUCTIONS,\n    tools=[WebSearchTool(search_context_size=\"low\")],\n    model=\"gpt-4o\",  # Or gpt-5.2 if available\n    model_settings=ModelSettings(tool_choice=\"required\")\n)\n\n# Execute search\nprint(\"üîç Web Search Demo\\n\")\nprint(\"=\"*70)\n\nquery = \"Latest AI agent frameworks 2026\"\nprint(f\"\\nSearching: {query}\\n\")\n\nwith trace(\"Web Search\"):\n    result = await Runner.run(search_agent, query)\n    display(Markdown(result.final_output))\n\n# Track costs\ntracker.add_web_search(1)  # $0.025\ntracker.add_call(\"gpt-4o\", 300, 500)\n\nprint(\"\\n\" + \"=\"*70)\ntracker.report()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e9762fc9",
   "metadata": {},
   "outputs": [],
   "source": "## ‚úÖ Phase 3 Complete: Custom Tools\n\n### What You Learned\n- ‚úÖ Created custom tools with `@function_tool` decorator\n- ‚úÖ Used WebSearchTool for web research\n- ‚úÖ Forced tool usage with `tool_choice=\"required\"`\n- ‚úÖ Tracked web search costs\n- ‚úÖ Combined multiple tools in one agent\n\n### Key Takeaways\n1. **Custom Tools**: Use `@function_tool` for any Python function\n2. **WebSearchTool**: Costs $0.025 per call - track carefully\n3. **Tool Choice**: `required` forces tool use, `auto` lets agent decide\n4. **Best Practice**: Always track costs with `tracker`\n\n### Tool Types\n- **Custom**: Your Python functions\n- **Hosted**: WebSearchTool, FileSearchTool, ComputerTool\n- **Future**: Build complex tool ecosystems\n\n### Next Up: Phase 4 - Structured Outputs\nLearn to get typed, validated responses using Pydantic models!\n\n---\n\n**Ready for Phase 4?** ‚Üí Continue below!"
  },
  {
   "cell_type": "markdown",
   "id": "7605c27a",
   "metadata": {},
   "source": "## üìç PHASE 4: Structured Outputs\n\n**Structured Outputs** use Pydantic models to get typed, validated responses instead of free-form text.\n\n### Why Structured Outputs?\n- ‚úÖ **Type-safe**: Guaranteed data types\n- ‚úÖ **Validated**: Automatic validation\n- ‚úÖ **Parseable**: Easy to use programmatically\n- ‚úÖ **Self-documenting**: Schema describes expected output\n\n### How It Works\n\n```python\n# 1. Define Pydantic model\nclass MyOutput(BaseModel):\n    field1: str = Field(description=\"What this field is\")\n    field2: int = Field(description=\"Another field\")\n\n# 2. Use as output_type\nagent = Agent(\n    model=\"gpt-4o\",\n    output_type=MyOutput  # Agent must return this structure\n)\n\n# 3. Get typed response\nresult = await Runner.run(agent, \"query\")\noutput = result.final_output  # MyOutput instance\n```\n\n### Use Cases\n- Research reports\n- Data extraction\n- Classification\n- Multi-step plans\n- Structured analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3dc37",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 37: Basic Structured Output Example\n\n# Define output schema\nclass ResearchPlan(BaseModel):\n    topic: str = Field(description=\"Research topic\")\n    searches: List[str] = Field(description=\"3-5 web search queries to perform\")\n    approach: str = Field(description=\"Research strategy\")\n    estimated_time: str = Field(description=\"Estimated time needed\")\n\n# Create agent with structured output\nplanner_agent = Agent(\n    name=\"ResearchPlanner\",\n    instructions=\"Create detailed research plans. Be specific with search queries.\",\n    model=\"gpt-4o\",\n    output_type=ResearchPlan  # Forces this structure\n)\n\n# Run agent\nprint(\"üìã Structured Output Demo\\n\")\nprint(\"=\"*70)\n\ntopic = \"AI Agent security best practices\"\nprint(f\"\\nPlanning research for: {topic}\\n\")\n\nwith trace(\"Research Planning\"):\n    result = await Runner.run(planner_agent, f\"Create research plan for: {topic}\")\n    plan = result.final_output  # ResearchPlan instance\n\n# Access typed fields\nprint(f\"**Topic**: {plan.topic}\")\nprint(f\"**Approach**: {plan.approach}\")\nprint(f\"**Estimated Time**: {plan.estimated_time}\")\nprint(f\"\\n**Search Queries**:\")\nfor i, query in enumerate(plan.searches, 1):\n    print(f\"  {i}. {query}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ Got validated, typed output!\")\n\ntracker.add_call(\"gpt-4o\", 200, 300)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "307c6f60",
   "metadata": {},
   "outputs": [],
   "source": "## ‚úÖ Phase 4 Complete: Structured Outputs\n\n### What You Learned\n- ‚úÖ Created Pydantic models for typed outputs\n- ‚úÖ Used `output_type` parameter\n- ‚úÖ Accessed validated, typed fields\n- ‚úÖ Built research planning agent\n\n### Key Takeaways\n1. **Pydantic Models**: Define expected structure with `BaseModel`\n2. **Field Descriptions**: Help the model understand schema\n3. **Type Safety**: Guaranteed data types (str, int, List, etc.)\n4. **Validation**: Automatic checking of required fields\n\n### Pattern\n```python\nclass MySchema(BaseModel):\n    field: str = Field(description=\"Clear description\")\n\nagent = Agent(output_type=MySchema, ...)\nresult = await Runner.run(agent, \"...\")\ntyped_output = result.final_output  # MySchema instance\n```\n\n---\n\n## üéâ Phases 1-4 Complete!\n\nYou've built a strong foundation:\n- ‚úÖ Cost tracking and model selection\n- ‚úÖ Basic agents with instructions\n- ‚úÖ Custom tools and WebSearch\n- ‚úÖ Structured, validated outputs\n\n### Next Steps\nThe remaining phases cover advanced topics:\n- **Phase 5**: Multi-agent systems (handoffs, debates)\n- **Phase 6**: Advanced features (streaming, parallel execution)\n- **Phase 7**: Real-world use cases (RAG, content pipelines)\n- **Phases 8-10**: Production patterns, testing, best practices\n\n**üìù Note**: This is a natural checkpoint to save your progress!\n\n---\n\n**Want to continue? Scroll down for advanced phases!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}